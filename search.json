[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods in evolutionary ecology WS25/26",
    "section": "",
    "text": "About\nThis script covers the computational and bioinformatics parts of the module “Methods in Evolutionary Ecology”. We will introduce you to R and BASH, two of the most widely used scripting languages, and make you familiar with navigating in a UNIX environment. These skills are important for any biologist, irrespective of the field you may want to specialise in in the future. Building upon your new knowledge, we will learn how to reconstruct phylogenies from sequencing data, how to work with genomic data, and how to characterise microbiomes. At the end of three weeks computational work, you will tackle a small computational group project, putting your new skills into practise.\nThe script is designed to cover the entire course content. While we will go you through all of the material together in detail during the course, the script should also enable you to work through the content on your own, e.g., to recap after the course has finished and as a reference and starting point for future computational endeavours.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Methods in evolutionary ecology WS25/26",
    "section": "Quarto",
    "text": "Quarto\nThe text is formatted using Quarto, which comes with a number of benefits. It allows us to provide explanations as structured and nicely formatted regular text, and to include code blocks for all computational steps. When compiling Quarto documents, all of the code is run, which means that you not only see the code, but also the outputs it creates.\nHere is an example:\nThis little block of R code generates 100 random coordinates and plots them. The code is shown below, together with the output the code has produced (in this case, a plot).\n\nx &lt;- runif(100)\ny &lt;- runif(100)\nplot(x, y)\n\n\n\n\n\n\n\n\nThe code can conveniently be copied from the block into your own scripts.\nQuarto supports many formats, we here provide the script as a webpage and a printable pdf. Writing Quarto documents is very simple and can be done using RStudio as an editor. The entire script is available for you on github – feel free to download it and modify it with your own comments, notes, and code. We will provide a short introduction to github and Quarto in the course.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#how-to-find-your-way-around",
    "href": "index.html#how-to-find-your-way-around",
    "title": "Methods in evolutionary ecology WS25/26",
    "section": "How to find your way around",
    "text": "How to find your way around\nSimply use the navigation on the left to quickly access the different topics, or flip through the individual pages using the buttons at the bottom of the page. You may wish to download the pdf version of the script (click the pdf icon in the top left) which is ideal for printing. The script is organized by topics, rather than course days, because we will adapt the tempo according to your needs.\n\nPlease note: The script will very likely only be complete at the end of the course. We will still be modifying and correcting it throughtout the three weeks you are with us. So make sure to check out the final version at the end of the course.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "Rintro1.html",
    "href": "Rintro1.html",
    "title": "1  First steps",
    "section": "",
    "text": "1.1 Operators and functions\nR is a statistical programming environment that has become a standard tool in the data and life sciences and many other fields. You may have used R already to run some statistics in a course you took in your studies, and this will be a likely use case for your remaining degree. However, R is much more: it can be used to analyse massive the datasets of the “omics”- age, build webpages, blogs, and interactive apps, and even for art!\nBefore taking full advantage of what the various R packages have to offer, we need to become familiar with its basic structure and commands. It pays off to invest a little effort in practicing the basics, because all R packages use the same syntax – a solid familiarity with base R thus allows you to explore the entire R universe independently.\nR can be used just like an arithmetic calculator. You are familiar with all of the basic syntax already, if you know how to use a calculator!\nSome examples:\n3 + 4  \n\n[1] 7\n\n3 - 4  \n\n[1] -1\n\n3 * 4  \n\n[1] 12\n\n3 / 4  \n\n[1] 0.75\n\n3 ^ 4  # power of\n\n[1] 81\nAs with a regular calculator, there is operator precedence: power &gt; multiplicative operations &gt; additive operations:\n(1 + 2) * 3\n\n[1] 9\n\n2^3 * 3\n\n[1] 24\n\n2^(3 * 3)\n\n[1] 512\nSquare roots, exponentials, and logarithms also work just as with a calculator:\nsqrt(9) \n\n[1] 3\n\nexp(3) \n\n[1] 20.08554\n\nlog(3) \n\n[1] 1.098612\n\nlog(exp(3)) # natural logarithm\n\n[1] 3\n\nlog10(100)  # logarithm to base 10\n\n[1] 2\nIn order to “save” a value for use later on, you have to assign it to a variable! &lt;- is the assignment operator you need to use for this (handy shortcut in RStudio is ALT + -).\nx &lt;- 3 + 4\nCalling the variable will then print the result to the R console, and can be used in other calculations.\nx\n\n[1] 7\n\nx + 10\n\n[1] 17\nYou can call your variables whatever you want, but be careful: R will overwrite any variable if you tell it to, without a warning! You should also avoid giving your variables names that are already assigned to functions.\nmy_favourite_variable &lt;- 100\nmy_favourite_variable\n\n[1] 100\nAll variables (among other things) are visible in the environment panel in RStudio (default: top right part of the screen).",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Rintro1.html#operators-and-functions",
    "href": "Rintro1.html#operators-and-functions",
    "title": "1  First steps",
    "section": "",
    "text": "“=” can also be used to assign variables but is discouraged, because the direction of the assignment is not immediately obvious. It is best practise to always start with the variable, followed by the assignment operator",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Rintro1.html#data-types",
    "href": "Rintro1.html#data-types",
    "title": "1  First steps",
    "section": "1.2 Data types",
    "text": "1.2 Data types\nYou need to be familiar with at least three important data types in R: logical, numeric, and character. Data being stored in a different data type than required is one of the most frequent error messages you will encounter as an R beginner.\nlogical simply means true or false. R also understands the abbreviations T and F. To determine which types you data is in, you can use mode or class.\n\nvar1 &lt;- FALSE \nmode(var1)\n\n[1] \"logical\"\n\n\nnumeric means numbers\n\nvar2 &lt;- 10\nclass(10)\n\n[1] \"numeric\"\n\n\nA character is any form of text, a so called “string”. It must always be surrounded by quotation marks!\n\nvar3 &lt;- \"A so called string\"\nmode(var3)\n\n[1] \"character\"\n\n\nIf in doubt, R will often convert or read in data as characters. Watch out for some common errors!\n\nvar4 &lt;- \"5\"\nvar4\n\n[1] \"5\"\n\nis.numeric(var4)\n\n[1] FALSE\n\nvar5 &lt;- \"TRUE\"\nvar5\n\n[1] \"TRUE\"\n\nis.logical(var5)\n\n[1] FALSE\n\n\nYou can convert between types easily!\n\nvar6 &lt;- as.numeric(var4)\nvar6\n\n[1] 5\n\nclass(var6)\n\n[1] \"numeric\"",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Rintro1.html#exercises",
    "href": "Rintro1.html#exercises",
    "title": "1  First steps",
    "section": "1.3 Exercises",
    "text": "1.3 Exercises\n\nSum the values of 1 to 5\n\n\nsum(1:5)\n\n[1] 15\n\n1 + 2 + 3 + 4 + 5\n\n[1] 15\n\n\n\nCreate a variable v1 and assign it a character value\n\n\nv1 &lt;- \"text\"\n\n\nCopy variable v1 to v2\n\n\nv2 &lt;- v1\n\n\nCompare the value of v1 against v2\n\n\nv1 == v2\n\n[1] TRUE\n\nidentical(v1, v2)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nTipTip\n\n\n\nCompare values and variables using the following operators\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nequals\n\n\n!=\nnot equal\n\n\n\nPlease note, = and == do very different things! Don’t mix them up.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Rintro2.html",
    "href": "Rintro2.html",
    "title": "2  Data structures",
    "section": "",
    "text": "2.1 Vectors\nSo far we’ve only looked at simple variables consisting of a single value or character. Typically, your data will be more complex. In R, there are three structures relevant for the data you will be working with.\nA vector is a number of elements of the same data type (logical, numeric, character). It can be generated by concatenating the elements using the function c.\nvec1 &lt;- c(T, F, T, F)\nvec1\n\n[1]  TRUE FALSE  TRUE FALSE\n\nmode(vec1)\n\n[1] \"logical\"\n\nvec2 &lt;- c(1, 2, 3, 4, 5)\nvec2\n\n[1] 1 2 3 4 5\n\nmode(vec2)\n\n[1] \"numeric\"\n\nvec3 &lt;- c(\"Spring\", \"Summer\", \"Autumn\", \"Winter\")\nvec3\n\n[1] \"Spring\" \"Summer\" \"Autumn\" \"Winter\"\n\nmode(vec3)\n\n[1] \"character\"\nOther ways to generate vectors are rep and seq. rep is used to repeat any number of elements any number of times.\nrep(5, 10)\n\n [1] 5 5 5 5 5 5 5 5 5 5\n\nrep(vec3, 5)\n\n [1] \"Spring\" \"Summer\" \"Autumn\" \"Winter\" \"Spring\" \"Summer\" \"Autumn\" \"Winter\"\n [9] \"Spring\" \"Summer\" \"Autumn\" \"Winter\" \"Spring\" \"Summer\" \"Autumn\" \"Winter\"\n[17] \"Spring\" \"Summer\" \"Autumn\" \"Winter\"\nseq can be used to create numerical sequences.\nseq(from = 0, to = 100, by = 5)\n\n [1]   0   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90\n[20]  95 100\nThe command above is easy to read and understand for humans, which is good. R will also understand if you specify it as\nseq(0, 100, 5)\n\n [1]   0   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90\n[20]  95 100\nAs a shortcut for a common sequences, you can use\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\nAs mentioned above,, vectors can only combine elements of a single data type. Combining multiple different data types may result in some unwanted behaviour.\nvec_mix1 &lt;- c(5, TRUE, 65)\nmode(vec_mix1)\n\n[1] \"numeric\"\n\nvec_mix2 &lt;- c(\"blue\", TRUE, \"red\")\nmode(vec_mix2)\n\n[1] \"character\"\nIn many cases you may wish to access a single element of a vector. You can do so using square brackets.\nz &lt;- c(\"order\", \"family\", \"genus\", \"species\")\nz[2]\n\n[1] \"family\"\nSimilarly, you can access any combination of elements from the vector.\nz[1:2]\n\n[1] \"order\"  \"family\"\n\ni &lt;- c(1, 3)\nz[i]\n\n[1] \"order\" \"genus\"\n\nz[c(1, 1, 1, 4)]\n\n[1] \"order\"   \"order\"   \"order\"   \"species\"\n\nz[-1]\n\n[1] \"family\"  \"genus\"   \"species\"\nThe square brackets are also used if you need to change elements of the vector. Changes are made using the assignment operator which you already know.\nx &lt;- 1:5\nx\n\n[1] 1 2 3 4 5\n\nx[c(1, 4)] &lt;- 10\nx\n\n[1] 10  2  3 10  5\nWhich elements of a vector have certain characteristics? This is important for filtering/selecting in your dataset. You can combine different queries using logical operators.\nx &gt;= 5\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\nx[x &gt;= 5]\n\n[1] 10 10  5\n\nwhich(x &gt;= 5)\n\n[1] 1 4 5\n\nz\n\n[1] \"order\"   \"family\"  \"genus\"   \"species\"\n\nwhich(z == \"genus\")\n\n[1] 3\n\nz[z== \"genus\"]\n\n[1] \"genus\"\n\nz[z != \"genus\"]\n\n[1] \"order\"   \"family\"  \"species\"\n\nwhich(z== \"genus\" | z == \"order\")\n\n[1] 1 3\nLogical operators in R\nConveniently, the elements of a vector can be named and accessed using the names. Let’s first create a vector…\ndmel &lt;- c(\"Hexapoda\", \"Diptera\", \"Drosophilidae\", \"Drosophila\", \"Drosophila melanogaster\")\ndmel\n\n[1] \"Hexapoda\"                \"Diptera\"                \n[3] \"Drosophilidae\"           \"Drosophila\"             \n[5] \"Drosophila melanogaster\"\n… and then add names for each element\nnames(dmel) &lt;- c(\"Class\", \"Order\", \"Family\", \"Genus\", \"Species\")\ndmel\n\n                    Class                     Order                    Family \n               \"Hexapoda\"                 \"Diptera\"           \"Drosophilidae\" \n                    Genus                   Species \n             \"Drosophila\" \"Drosophila melanogaster\" \n\nstr(dmel)\n\n Named chr [1:5] \"Hexapoda\" \"Diptera\" \"Drosophilidae\" \"Drosophila\" ...\n - attr(*, \"names\")= chr [1:5] \"Class\" \"Order\" \"Family\" \"Genus\" ...\nNow we can use the names to access the values\ndmel[c(\"Class\", \"Species\")]\n\n                    Class                   Species \n               \"Hexapoda\" \"Drosophila melanogaster\" \n\ndmel[names(dmel) == \"Order\"]\n\n    Order \n\"Diptera\"",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Rintro2.html#vectors",
    "href": "Rintro2.html#vectors",
    "title": "2  Data structures",
    "section": "",
    "text": "|\nOR\n\n\n&\nAND\n\n\n!\nNOT\n\n\n\n\n\n\n\n\n\n\n2.1.1 Exercises\n\nCreate a vector consecutively numbering all days of the year 2026. Assign the correct weekday names for all elements of the vector.\nUse the vector to determine how many days in 2026 are weekend days.\n\n\n\n\n\n\n\nTipTip\n\n\n\nIf you struggle to assign the correct names, have a look at the help for rep.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Rintro2.html#matrices",
    "href": "Rintro2.html#matrices",
    "title": "2  Data structures",
    "section": "2.2 Matrices",
    "text": "2.2 Matrices\nA matrix in R can be thought of as a two-dimensional vector. All elements must be of the same data type. There are various ways to create a matrix. For example, one can use the matrix function like this.\n\nmat1 &lt;- matrix(data = 1:12, nrow = 3, ncol = 4, byrow=T) \nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\nAlternatively, a vector can be transformed into a matrix\n\nmat2 &lt;- 1:12\ndim(mat2) &lt;- c(3, 4)\nmat2\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nOften you will want to combine multiple vectors into a matrix\n\ndmel &lt;- c(\"Hexapoda\", \"Diptera\", \"Drosophilidae\", \"Drosophila\", \"Drosophila melanogaster\")\ndhyd &lt;- c(\"Hexapoda\", \"Diptera\", \"Drosophilidae\", \"Drosophila\", \"Drosophila hydei\")\nmat3 &lt;- cbind(dmel, dhyd)\nmat3\n\n     dmel                      dhyd              \n[1,] \"Hexapoda\"                \"Hexapoda\"        \n[2,] \"Diptera\"                 \"Diptera\"         \n[3,] \"Drosophilidae\"           \"Drosophilidae\"   \n[4,] \"Drosophila\"              \"Drosophila\"      \n[5,] \"Drosophila melanogaster\" \"Drosophila hydei\"\n\nmat4 &lt;- rbind(dmel, dhyd)\nmat4\n\n     [,1]       [,2]      [,3]            [,4]        \ndmel \"Hexapoda\" \"Diptera\" \"Drosophilidae\" \"Drosophila\"\ndhyd \"Hexapoda\" \"Diptera\" \"Drosophilidae\" \"Drosophila\"\n     [,5]                     \ndmel \"Drosophila melanogaster\"\ndhyd \"Drosophila hydei\"       \n\n\nJust like vectors, matrix elements can have names\n\nmat3\n\n     dmel                      dhyd              \n[1,] \"Hexapoda\"                \"Hexapoda\"        \n[2,] \"Diptera\"                 \"Diptera\"         \n[3,] \"Drosophilidae\"           \"Drosophilidae\"   \n[4,] \"Drosophila\"              \"Drosophila\"      \n[5,] \"Drosophila melanogaster\" \"Drosophila hydei\"\n\ncolnames(mat3)\n\n[1] \"dmel\" \"dhyd\"\n\nrownames(mat3) &lt;- c(\"Class\", \"Order\", \"Family\", \"Genus\", \"Species\")\nmat3\n\n        dmel                      dhyd              \nClass   \"Hexapoda\"                \"Hexapoda\"        \nOrder   \"Diptera\"                 \"Diptera\"         \nFamily  \"Drosophilidae\"           \"Drosophilidae\"   \nGenus   \"Drosophila\"              \"Drosophila\"      \nSpecies \"Drosophila melanogaster\" \"Drosophila hydei\"\n\n\nAnd just like with vectors, we can use square brackets to access and replace values. Because there are 2 dimensions, we need to provide 2 values (one for rows, one for columns, separated by ,).\n\nmat3\n\n        dmel                      dhyd              \nClass   \"Hexapoda\"                \"Hexapoda\"        \nOrder   \"Diptera\"                 \"Diptera\"         \nFamily  \"Drosophilidae\"           \"Drosophilidae\"   \nGenus   \"Drosophila\"              \"Drosophila\"      \nSpecies \"Drosophila melanogaster\" \"Drosophila hydei\"\n\nmat3[1:3, 2]\n\n          Class           Order          Family \n     \"Hexapoda\"       \"Diptera\" \"Drosophilidae\" \n\nmat3[1:3, ]\n\n       dmel            dhyd           \nClass  \"Hexapoda\"      \"Hexapoda\"     \nOrder  \"Diptera\"       \"Diptera\"      \nFamily \"Drosophilidae\" \"Drosophilidae\"\n\nmat3[c(\"Class\", \"Species\"), ]\n\n        dmel                      dhyd              \nClass   \"Hexapoda\"                \"Hexapoda\"        \nSpecies \"Drosophila melanogaster\" \"Drosophila hydei\"\n\n\n\n2.2.1 Exercise\n\ncreate a matrix using with 20 rows & 5 columns, using 100 randomly generated numbers between 0 and 1000.\n\n\nrandom_numbers &lt;- runif(100, 0, 1000)\n\n\nmatrix(data = runif(100, 0, 1000), nrow = 20)\n\n           [,1]      [,2]       [,3]      [,4]      [,5]\n [1,] 406.52737  94.65615  79.593348 785.83244 859.54488\n [2,] 872.86638 742.13181 370.145682 512.53231 971.45221\n [3,] 906.12953 682.17085 689.011138 585.62626 165.75229\n [4,] 314.58933 967.04543 713.425011 617.97192 276.34891\n [5,] 981.77449  67.33415 844.791563 996.66288 294.28394\n [6,] 346.90053 373.50757  89.432794 308.53522 100.73181\n [7,] 273.29147 829.59223 700.459525 891.01782 177.57526\n [8,] 573.09280 679.31572 699.271689 332.92091 335.86642\n [9,] 577.58001 271.39734 680.343705 539.60915 345.52217\n[10,] 550.88474  85.49117 662.483973 791.38405 665.75735\n[11,]  56.26581 470.69044 743.986116 811.72560 104.27914\n[12,] 480.11559 254.46995   9.353868 346.93680 141.00419\n[13,] 496.60600 993.21448 593.936401 607.96994  87.83614\n[14,]   9.12045 706.22907 172.317181 602.96727 529.15490\n[15,] 776.14027 478.15762 231.736929 665.85397  24.15717\n[16,] 102.49829  53.40347 694.475180  62.35594 242.85735\n[17,] 658.89287 113.28564 795.230475 926.51203 132.19633\n[18,] 552.50242 245.58374 710.244399 672.24174 793.82991\n[19,] 631.48460 488.09697 461.142976  58.97069 172.55375\n[20,] 906.99910 382.30154 670.915532 310.87803  37.72161\n\n\n\nreplace all values in the 3rd column of this matrix that are larger than 500 with NA.\n\n\n\n\n\n\n\nTipTip\n\n\n\nuse the function runif to create random values",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Rintro2.html#data-frames",
    "href": "Rintro2.html#data-frames",
    "title": "2  Data structures",
    "section": "2.3 Data frames",
    "text": "2.3 Data frames\nData frames are the R equivalent of spread sheets. Like matrices, they are two-dimensional, however they may combine different data types. Most biological data sets you will encounter will be data frames.\nLets create a data frame\n\n# create some data\nspecies &lt;- rep(c(\"beech\",\"ash\",\"elm\",\"maple\", \"sycamore\"),40)\nspecies\n\n  [1] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n  [7] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [13] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [19] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [25] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [31] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [37] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [43] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [49] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [55] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [61] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [67] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [73] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [79] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [85] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [91] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [97] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[103] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[109] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[115] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[121] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[127] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[133] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[139] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[145] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[151] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[157] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[163] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[169] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[175] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[181] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[187] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[193] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[199] \"maple\"    \"sycamore\"\n\ndbh &lt;- runif(200, 5, 40)\ndbh\n\n  [1] 38.661478 35.165052 12.372344 28.373796  7.938509  5.140963 22.675742\n  [8] 38.645541 32.479254 38.375795  5.892895 25.624173 13.994242 38.726706\n [15] 27.573738  9.413766 16.240492 24.003927 33.688846 16.435163  9.853125\n [22] 17.195451 34.894299  9.465857 27.885648 26.826155 19.807412 12.176889\n [29] 14.861570 33.336551 36.017648 26.104376  8.456482 34.836519 17.996797\n [36] 17.295787 10.440867 38.107385 29.830109 11.661402 35.310937 16.210313\n [43] 34.967534 34.626130 25.861292 24.549519 37.761696 11.669043 18.893983\n [50] 38.275544 13.274795 26.870189 27.377955 12.652465 23.490489  5.332502\n [57]  5.672127 24.553599 30.619875  7.826655 19.122781 21.601444 14.523765\n [64] 32.997257 13.328309 29.424545 35.854304  7.837236 29.022848 10.725314\n [71] 24.390326 32.779471 29.134944 23.605564 38.680832 31.248307 27.450671\n [78] 33.644973 23.166495 11.323378 27.292471 22.298525 31.160959 39.651256\n [85] 23.541920 38.061685 22.894891 39.932970 27.130961 38.056049 10.723130\n [92]  9.319783 21.272908 34.433075  5.726323  7.368067 17.514678 15.383359\n [99]  5.284050 13.065209 16.399815 24.336125 17.595459 35.070796 14.013116\n[106] 37.610332  9.215295  8.340009 31.840446 37.882820 18.869012 27.397117\n[113]  9.767600 31.627004 31.125689  8.262641 21.305937 17.498945 14.146182\n[120] 36.115397 18.656328 30.190093  8.741519 35.704829 38.506420 23.718106\n[127] 12.428679 14.294346 24.500302 33.105637 17.506331 27.572766  7.232943\n[134]  6.848890 28.788894 20.767561 24.548576 32.094432  9.101336 39.805444\n[141] 28.249881 11.770586  7.842802  8.296083 29.081251 16.355766 39.406286\n[148] 29.274872 27.218331 21.670232 11.687910 36.182045 11.074399 27.508057\n[155] 28.826223 34.384941 30.899512 10.084346 21.641602 21.757260  7.350661\n[162] 29.754322 15.165254 12.117600 34.558184 30.524023 24.844228 18.871202\n[169] 26.199815 13.365497 34.383091 30.319369 25.731398  9.351797 26.403737\n[176] 35.862405 20.636603 25.393259 38.546352 26.649564 39.202758  8.378768\n[183] 10.278505 32.336205 11.581948 35.309759  7.501322 10.688837 39.063838\n[190] 29.908512  8.243569 31.564507 18.584374  9.112082  5.821096 13.136200\n[197] 28.469118 33.463887 11.379311 17.150446\n\nage &lt;- as.integer(runif(200, 20, 120))\nage\n\n  [1]  61  49  59  41  31  20  27  45  54  70  56  32  71  90  32  47  78  54\n [19]  78  21  35  96 119  57  74  22  40  36  28 114  62  67  42  26  28  62\n [37]  60  57 114  28  30  37  81  52  95  98 112  87  50  56  21  24  67  94\n [55]  45  56  59  46  39  44  32  90 114 105  50  51  48 105  41 112 112  33\n [73]  51  82  85  35  21 101  85  98  28  64  71  90 113 115  87  89  62  93\n [91] 116  21  67  91 110 117 101  57 113  85  97  64  77  20  97  40  32  80\n[109]  89  59  39  65  52  96  33  89  70  37 116  98  99  51  91  36 111  62\n[127]  26  82  37  97 118  57  20  71 112  35  70  37  37  82  98  92  33 111\n[145]  20  49  91  25 109  94  46  57  48 104 104  91 111  76  67  96  21  62\n[163]  37 113  63 107 105 102 117  31  58  28  34  55  49  98  82  74  51  67\n[181]  87 118  62 104  70  85  52 119  63 102  46  42  75  20  74  20  60  68\n[199]  50  37\n\ndf1 &lt;- data.frame(species, dbh, age)\ndf1\n\n\n  \n\n\n\nTo access values, we can use the same approaches as for matrices:\n\ndf1[1:12, 1:2]\n\n\n  \n\n\n\nbut can also access and filter the columns directly using their names like this:\n\ndf1$species\n\n  [1] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n  [7] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [13] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [19] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [25] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [31] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [37] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [43] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [49] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [55] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [61] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [67] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [73] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [79] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [85] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [91] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [97] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[103] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[109] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[115] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[121] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[127] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[133] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[139] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[145] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[151] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[157] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[163] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[169] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[175] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[181] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[187] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[193] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[199] \"maple\"    \"sycamore\"\n\ndf1[df1$dbh &gt; 15, ]\n\n\n  \n\n\n\n\n2.3.1 Exercise\n\nUsing df1, select only entries corresponding to ash and maple with an age over 50 and a diameter less than 30.\nAdd a new column to the dataframe called “year”. Generate data for this column so that there are 10 different years and the same number of entries for each tree species per year.\n\n\ndf1\n\n\n  \n\n\nyear &lt;- rep(2015:2024, 20)\n\ndf1[,4] &lt;- year\n\ncolnames(df1)[4] &lt;- \"year\"\n\n\ndf1\n\n\n  \n\n\n\n\n\n\n\n\n\nTipTip\n\n\n\nUse the function rep for this exercise",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Rintro3.html",
    "href": "Rintro3.html",
    "title": "3  Data, packages, and some more functions",
    "section": "",
    "text": "3.1 Setting up your working environment\nUsually when working in R, you want to look at your own data, and not generate it from random distributions. To read in a data file, we first need to tell R where the working directory is located.\nsetwd(\"/home/of22haqi/Documents/TEACHING/MEE-WS25-26/data/\")\nThe path will look different on your machine of course.\nNow that R knows where to find it, we are ready to read in a data file.\n# The table contains headers, and the fields are separated by commas\nbe &lt;-  read.table(\"data/butterfly_ecology.csv\", header = TRUE, sep = \",\")\n\n# Let's have a glimpse at the data \nhead(be)\nIn order to save your entire working environment, so you don’t have to re-run potentially time intensive pieces of your code, just save it and load it back into your work space the next time you use R.\nsave.image(\"myenv.Rdata\")\nYou can also use the panel “Environment” in RStudio to save and load your data.\nAll of the functions we used today are so “base R” functions, which means they come preinstalled with R. Lots of the functionality of R is in external packages which need to be installed manually. The majority of relevant packages are found on CRAN (The Comprehensive R Archive Network), and there is a special archive for packages relevant to the life sciences (Bioconductor). In order to install packages from CRAN, simply run\ninstall.packages(\"tidyverse\", dependencies = TRUE)\nHere, tidyverse is the package we want to install we ask to also install any packages that tidyverse may require to function. You can find a list of all packages currently installed in the packages tab in the panel on the bottom left in RStudio. It is good practise to keep the packages, as well as your R installation up to date.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#setting-up-your-working-environment",
    "href": "Rintro3.html#setting-up-your-working-environment",
    "title": "3  Data, packages, and some more functions",
    "section": "",
    "text": "Use a text editor outside of Rstudio to look at the data file as well. Why do you think this is a good format to store data in? WHat is the advantage to e.g., an Excel file? What does using a text file format mean for your data entry requirements?",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#functions",
    "href": "Rintro3.html#functions",
    "title": "3  Data, packages, and some more functions",
    "section": "3.2 Functions",
    "text": "3.2 Functions\nWe have already used plenty of functions. Most of them require at least an object an which to perform the function on, and may also have some options. For example, consider the following function:\n\nmean(be$range.size, na.rm = TRUE)\n\n[1] 261.4116\n\n\nmean is the function, be$range.size is the object (1 vector from the dataframe we just read into R) and na.rm = TRUE is the option to remove NAs from the vector before calculating the mean.\nIn some cases, you may want to do things to your data that cannot be addressed by a single function. In this case, you may have to perform a number of different operations on the dataset. If you are likely to use the same set of operations in the future, it may be advisable to use your own functions.\nA very simple example. Let’s assume the mean function didn’t exist and we would need to write our own.\n\nmean2 &lt;- function(x){\n  x &lt;- na.omit(x)\n  sum(x) / length(x) \n}\n\nmean2(be$range.size)\n\n[1] 261.4116\n\n\nWe define mean2 as a function that requires an object (here called x as an input). Looking into the function, we can see that it first removes the NAs from the object and next calculates the sum of x divided by the number of elements of x (this is how the mean is defined). Testing it, we can see that it gives the same result as the native mean function.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#loops",
    "href": "Rintro3.html#loops",
    "title": "3  Data, packages, and some more functions",
    "section": "3.3 Loops",
    "text": "3.3 Loops\nIn many cases, we need to apply a function t a number of elements. In this case, loops come in handy. In the simple examples below, the structure of a for loop is illustrated.\n\nfor(i in 1:10) # how often is the loop repeated\n{\n   print(i)    # what is to be done each iteration\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\nj&lt;-0\nfor(i in 1:5)\n{\n   j&lt;-i+j\n   print(j)\n}\n\n[1] 1\n[1] 3\n[1] 6\n[1] 10\n[1] 15\n\n\nObserve and try to explain what happens in each iteration to the variables used in these examples.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#plots",
    "href": "Rintro3.html#plots",
    "title": "3  Data, packages, and some more functions",
    "section": "3.4 Plots",
    "text": "3.4 Plots\nFor many use cases ggplot2 is the best approach of plotting, and we will get to know this package later. However, for very simple and quick plots, base R plotting functions are sufficient and superior to othe options because of simplicity and speed.\nScatter plots can be created by just naming the variables to be plotted against each other.\n\nplot(be$WSP_Female_average, be$ALT_Range)\n\n\n\n\n\n\n\n\nHistograms showing frequency distributions are also very easily generated\n\nhist(be$WSP_Female_average)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#exercises",
    "href": "Rintro3.html#exercises",
    "title": "3  Data, packages, and some more functions",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nUsing a loop, plot histograms for the columns “WSP_Female_average”, “Alt_Range”, “Alt_min”, and “range.size”.\nWrite a function that creates these plots with only the dataframe as argument.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "tidyverse.html",
    "href": "tidyverse.html",
    "title": "4  Tidyverse",
    "section": "",
    "text": "4.1 What is the tidyverse?\nWe will only be looking at a couple of functions from a 2 packages (dplyr & ggplot2). All functions are about data manipulation and visualisation and are especially well suited for exploring very large data sets.\nYou can install all tidyverse packages by running\ninstall.packages(\"tidyverse\", dependencies = TRUE)\n(Remember, you can just add the answers into this document for future reference!)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#what-is-the-tidyverse",
    "href": "tidyverse.html#what-is-the-tidyverse",
    "title": "4  Tidyverse",
    "section": "",
    "text": "A collection of R packages for data science\nAll packages share a “philosophy” about design and data structure\nAll packages are highly compatible and functions complement each other\n\n\n\n\n\nLet’s refresh what we learned earlier this week:\n\nWhat different types of data structures are used in R?\nWhich of these do you think is most likely to be used in the tidyverse?",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#our-data-set-for-today",
    "href": "tidyverse.html#our-data-set-for-today",
    "title": "4  Tidyverse",
    "section": "4.2 Our data set for today",
    "text": "4.2 Our data set for today\nWe will be looking at a data set of ecological traits of european butterflies. Download the table and read it into R.\n\n# The table contains headers, and the fields are separated by commas\nbe &lt;-  read.table(\"data/butterfly_ecology.csv\", header = TRUE, sep = \",\")\n\n# Let's have a glimpse at the data using head\nhead(be)\n\n\n  \n\n\n\nEach of the rows contains data for 1 European species, and the columns contain the following information:\n\n\n\n\n\n\n\n\n\nTrait abbreviation\nMeaning\nStates\nNotes\n\n\n\n\nOWS\nOverwintering stage\negg, larvae, pupae, adult\n\n\n\nGEN\nGenerations\naverage, min, max, range\n\n\n\nWSP\nWingspan\naverage, range\nMeasured in mm\n\n\nHSI\nHostplant index\nN/A\nMeasured from 0-1\n\n\nLEV\nLarval environment\nburied, ground layer, field layer, shrub layer, canopy layer\n\n\n\nELT\nEgg laying type\nsingle, small batch, large batch\n\n\n\nALT\nAltitude\nmin, range\n\n\n\nFM\nFlight months\naverage, range\n\n\n\nAFB\nAdult feeding behaviour\nherb flower, grass, shrub flower, honeydew, sap, animal, mineral\n\n\n\n\nNow that we are familiar with the dataset, lets look at some tidyverse functions.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#filter-for-filtering-data-frames",
    "href": "tidyverse.html#filter-for-filtering-data-frames",
    "title": "4  Tidyverse",
    "section": "4.3 filter() for filtering data frames",
    "text": "4.3 filter() for filtering data frames\nAs the name suggests, this is used to filter data frames, with a simple and efficient syntax:\n\n# first, we have to load the tidyverse packages\nlibrary(tidyverse, quietly = TRUE)\n\n# the command always takes a dataframe as first argument, and a filtering criterion as second argument\n# Here, we only consider butterflies that overwinter as eggs\nfilter(be, OWS_egg == 1)\n\n\n  \n\n\n\nThe filtering criterion can be specified using the methods you are already familiar with (e.g., &gt;, &gt;=, !=, %in%).\nNotice that the variable names can be used directly here, so instead of using be$OWS_egg, filter() lets you use OWS_egg directly. All tidyverse functions work like that. Let’s look at more complex filtering:\n\n# combine 2 filters with boolean \"AND\" ...\nfilter(be, OWS_egg == 1 , ALT_Min &gt; 500)\n\n\n  \n\n\n# ... or boolean \"OR\"\nfilter(be, OWS_egg == 1 | AFB_honeydew == 1)\n\n\n  \n\n\n\n\nNOTE\nfilter() (and many other tidyverse functions) return a data frame. In the tidyverse, these are called tibble() and behave slightly different to regular data frames. For our purposes however, these differences are not important.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#the-pipe-for-combining-commands",
    "href": "tidyverse.html#the-pipe-for-combining-commands",
    "title": "4  Tidyverse",
    "section": "4.4 The pipe %>% for combining commands",
    "text": "4.4 The pipe %&gt;% for combining commands\nThe filtering using filter() is very useful, but you can see that the commands can become very long when you have many filters. Also, trying out many different filters to see what they do with the data can be cumbersome. This where %&gt;% comes in really handy.\nThe “pipe” %&gt;% (keyboard shortcut: Ctrl+Shift+M) simply passes the result of one function to the next function. For the next function, one does not have to specify the data frame. Let’s see an example.\n\n# this is how we filtered our data frame earlier \nfilter(be, OWS_egg == 1)\n\n\n  \n\n\n# same command, this time using the pipe\nbe %&gt;%  \n  filter(OWS_egg == 1)\n\n\n  \n\n\n\nNote how in the second command, the output of be (which is our data frame) gets passed on to the filter() command. There, you don’t have to specify the name of the data frame again. The result of this can be piped further to other commands:\n\n# Multiple filters are connected by pipes\nbe %&gt;% \n  filter(OWS_egg == 1) %&gt;% \n  filter(LEV_ground_layer == 1) %&gt;% \n  filter(AFB_honeydew == 1)\n\n\n  \n\n\n# As always in R, assign the result to a new variable using \"&lt;-\" \nbe_filtered &lt;- be %&gt;% \n  filter(OWS_egg == 1) %&gt;% \n  filter(LEV_ground_layer == 1) %&gt;% \n  filter(AFB_honeydew == 1)\n\nNote how easy this command is to read (you could write it in a single line, but it’s much easier to follow with line breaks)! The usefulness of the pipe will become more obvious when we combine multiple different commands. In all the following examples, I will always use the pipe.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#sort-by-column-with-arrange",
    "href": "tidyverse.html#sort-by-column-with-arrange",
    "title": "4  Tidyverse",
    "section": "4.5 Sort by column with arrange()",
    "text": "4.5 Sort by column with arrange()\nThis doesn’t change the dataframe itself, it simply orders the columns (similar to the sort function in Excel):\n\n# sort by age (ascending) and weight (descending)\nbe %&gt;% \n  arrange(conserv.eu, -range.size)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#select-columns-with-select",
    "href": "tidyverse.html#select-columns-with-select",
    "title": "4  Tidyverse",
    "section": "4.6 Select columns with select()",
    "text": "4.6 Select columns with select()\n\n# choose which columns to keep\nbe %&gt;% \n  select(species, range.size, conserv.eu, FM_Average, WSP_Female_average)\n\n\n  \n\n\n# or specify which columns to remove\nbe %&gt;% \n  select(-(OWS_egg:OWS_adult))\n\n\n  \n\n\n# contains is another useful command to select columns. \nbe %&gt;% \n  select(species, contains(\"LEV\")) %&gt;% \n  drop_na()\n\n\n  \n\n\n\nFuntions like contains can be powerful for filtering and selecting. starts_with and ends_with work just the same way and are equally useful.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#create-new-variables-with-mutate",
    "href": "tidyverse.html#create-new-variables-with-mutate",
    "title": "4  Tidyverse",
    "section": "4.7 Create new variables with mutate()",
    "text": "4.7 Create new variables with mutate()\nThis is a very powerful and flexible function that uses existing variables to create novel ones. Let’s look at a simple example\n\n# Create a new variable summarizing all the overwintering stages that are not adults\nbe %&gt;% \n  mutate(OWS_juvenile = 1-OWS_adult) %&gt;% \n  select(OWS_juvenile, OWS_adult) %&gt;% \n  drop_na()\n\n\n  \n\n\n# Determine how different the protection levels between EU and Europe and extract the species for which the difference is striking\nbe %&gt;% \n  mutate(protect_diff = abs(conserv.europe - conserv.eu)) %&gt;% \n  filter(protect_diff &gt; 2)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#exercise",
    "href": "tidyverse.html#exercise",
    "title": "4  Tidyverse",
    "section": "4.8 Exercise",
    "text": "4.8 Exercise\n\nFrom our dataset, remove all butterflies with average wingspans larger than 60mm and smaller than 30mm. Only keep the species that have a conservation classification on the EU level. Only keep the species names and all variables associated with adult feeding, and store this in a new data frame. How many rows and columns does the new data frame have?\n\n\nnew_be &lt;- be %&gt;% \n  filter(WSP_Female_average &lt; 60) %&gt;% # filter1 \n  filter(WSP_Female_average &gt; 30) %&gt;% \n  drop_na(conserv.eu) %&gt;% \n  select(species, starts_with(\"AFB\")) \n\nnew_be\n\n\n  \n\n\n\n\nRe-calculate the generation range from the provided minima and maxima. Check if your calculations match the original range values given in the data.\n\n\nbe %&gt;% \n  mutate(GEN_Range2 = GEN_Max - GEN_Min) %&gt;% \n  select(species, GEN_Max, GEN_Min, GEN_Range2, GEN_Range) %&gt;% \n  mutate(GEN_compare = GEN_Range2 - GEN_Range)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#group_by-and-summarise-as-powerful-data-exploration-tools",
    "href": "tidyverse.html#group_by-and-summarise-as-powerful-data-exploration-tools",
    "title": "4  Tidyverse",
    "section": "4.9 group_by() and summarise() as powerful data exploration tools",
    "text": "4.9 group_by() and summarise() as powerful data exploration tools\nAlthough dplyr has a simpler syntax overall, everything we have looked at so far could have been done fairly easily with base R functions: data frame filtering, sorting, and adding and removing columns. One of the strengths of dplyr is explorative data analysis, and this is where group_by() and summarize() are really helpful. We’ll only look at very simple examples today.\nWhen browsing through the complete data table, it is very hard to recognize any patterns. Let’s assume we wanted to compare the average wing span of butterflies with that overwinter as adults vs all other butterflies:\n\n# Are butterflies that overwinter as adults larger than other species?\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(OWS_adult) %&gt;% \n  summarise(mean_wsp = mean(WSP_Female_average))\n\n\n  \n\n\n\nAfter choosing which variable to group by (here: OWS_adult), summarise() then calculates a function for each group. In our simple example, there are 2 groups: 0 (not overwintering as adult) and 1 (overwintering as adult); and the function to be calculated is the mean of the female wing span. This is a very flexible set of functions, because you can group by multiple groups and also use summarise() with many different functions (e.g., mean(), sum(), min(), max(), median() – just to name a few). Let’s look at a more complex example:\n\n# Let's add another group. How large is the standard deviation? How large is each group?\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(OWS_adult, LEV_ground_layer) %&gt;% \n  summarise(mean_wsp = mean(WSP_Female_average),\n            sd = sd(WSP_Female_average),\n            group_size=n())\n\n`summarise()` has grouped output by 'OWS_adult'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\nbe %&gt;% \n  mutate(alt_bins = case_when(ALT_Min &gt; 500 ~ \"high\",\n                              ALT_Min &lt;= 500 ~ \"low\"))",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#more-exercises",
    "href": "tidyverse.html#more-exercises",
    "title": "4  Tidyverse",
    "section": "4.10 More exercises",
    "text": "4.10 More exercises\nUsing dplyr functions, determine\n\nIf butterflies overwintering as pupae have higher level of legal protection\n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(OWS_pupae) %&gt;% \n  summarise(mean_conserve = mean(conserv.eu))\n\n\n  \n\n\n\n\nIf butterflies occurring at higher altitudes on average have a higher level of protection\n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  mutate(ALT_cat = case_when(ALT_Min &lt; 200 ~ \"niedrig\",\n                            ALT_Min &gt; 1000 ~ \"hoch\",\n                            ALT_Min &lt;= 1000 & ALT_Min &gt;= 200 ~ \"mittel\" )) %&gt;% \n  group_by(ALT_cat) %&gt;% \n  summarise(mean_conserv = mean(conserv.eu))\n\n\n  \n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(conserv.eu) %&gt;% \n  summarise(mean_ALT = mean(ALT_Min))\n\n\n  \n\n\n\n\nIf feeding on honeydew is more common in larger butterflies.\n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(AFB_honeydew) %&gt;% \n  summarise(size = mean(WSP_Female_average))\n\n\n  \n\n\n\n\nHow many butterfly species are there per family?\n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(family) %&gt;% \n  tally()\n\n\n  \n\n\n\nFor a–c also determine how many species belong to each group.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "ggplot.html",
    "href": "ggplot.html",
    "title": "5  The ggplot2 package",
    "section": "",
    "text": "5.1 Very (!) brief introduction\nggplot2 is a graphing library, i.e., a tool to make graphs in R. Compared with base graphs and other graphics packages, it comes with a number of advantages:\nCompared with other packages the major drawbacks would be that it comes with a steep(ish) learning curve and is probably less intuitive for beginners. The reason is that ggplot2 doesn’t have fixed commands for scatterplots, boxplots, barplots, etc, but rather creates the plot in layers. The most important elements (or layers) of a plot in ggplot2 are:",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#very-brief-introduction",
    "href": "ggplot.html#very-brief-introduction",
    "title": "5  The ggplot2 package",
    "section": "",
    "text": "Beautiful!\nHighly customizable (which is not always necessary though)\nEasiest way to create very complex plots\nTightly integrated into the tidyverse\n\n\n\nData: as we are still in the tidyverse, this is always a data frame\nAesthetics: i.e., what you want to plot. Often, this will correpond to variables (columns) in your dataframe\nGeometric objects: i.e., how you want to plot the data. This can be points, bars, boxplots, lines, etc..\nFacets: more about this later\nAdditional (optional) adjustments: this includes themes that specify the overall design",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#building-up-the-plot",
    "href": "ggplot.html#building-up-the-plot",
    "title": "5  The ggplot2 package",
    "section": "5.2 Building up the plot",
    "text": "5.2 Building up the plot\nWe will start off with a very simple scatterplot and gradually increase the complexity to illustrate ggplot2 functionality.\n\n# data and packages\nlibrary(tidyverse, quietly = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nbe &lt;-  read.table(\"data/butterfly_ecology.csv\", header = TRUE, sep = \",\")\n\n# simple plot\nbe %&gt;%                                # DATA\n  ggplot(aes(x = WSP_Female_average,  # AESTHETICS\n             y = range.size))   \n\n\n\n\n\n\n\n\nIn the above example, the data is the data frame that we have been using the whole time. Notice how we can simply pipe it to ggplot2. aes specifies our aesthetics, i.e., what we want to plot. What is missing?\n\n# simple scatter plot\nbe %&gt;%                               # DATA\n  ggplot(aes(x = WSP_Female_average, # Aesthetics\n             y = range.size)) +\n  geom_point()                       # Geometric object\n\n\n\n\n\n\n\n\nThe geometric object, i.e., how we want to plot our aesthetics. Notice that elements in ggplot2 are added with the + symbol (this is specific to ggplot2). We can add more geometric objects that will use the same aesthetics:\n\n# lets add another geom (a regression line), and also change the theme\nbe %&gt;%                               # DATA\n  ggplot(aes(x = WSP_Female_average, # Aesthetics\n             y = range.size)) +\n  geom_point() +                     # Geometric object\n  geom_smooth(method = \"lm\") +       # Another geometric object\n  theme_light()                      # Let's also change the theme\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nChanging the theme changes many layout options. For different applications, different themes might be appropriate. There are many additional themes available through packages such as ggthemr or ggthemes.\nA bit more on aesthetics: have you noticed that you only specify x and y once, and all geoms know what you want to plot. You can also specify additional aesthetics for each geom.\n\n# Add additional aesthetics, here: we want to plot the sonservation status. How? With colour! \n\nbe %&gt;%                                   # DATA\n  ggplot(aes(x = WSP_Female_average,     # Aesthetics\n             y = range.size)) +\n  geom_point(aes(color = conserv.eu)) +  # aesthetics specific to the points only\n  geom_smooth(method = \"lm\",\n              color = \"black\",\n              se = FALSE) +              \n  theme_light() +\n  scale_color_viridis_c()                # let's use some nicer colors                         \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nAesthetics can be added through colors or shapes",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#faceting",
    "href": "ggplot.html#faceting",
    "title": "5  The ggplot2 package",
    "section": "5.3 Faceting",
    "text": "5.3 Faceting\nSo far, we have cramped as much information as possible into the plot. This was useful to illustrate the functionality of ggplot2, but did not create very readable plots. Often, faceting is a better solution. The implementation in ggplot2 is very straightforward.\n\n# Same example as before, faceted over family\nbe %&gt;%                                   \n  ggplot(aes(x = WSP_Female_average,     \n             y = range.size)) +\n  geom_point(aes(color = conserv.eu)) + \n  geom_smooth(method = \"lm\",\n              color = \"black\",\n              se = FALSE) +              \n  theme_light() +\n  scale_color_viridis_c() +                \n  facet_wrap(~family, scales = \"free\")   \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# And now, faceting over 2 variables\nbe %&gt;%   \n  ggplot(aes(x = WSP_Female_average,     \n             y = range.size)) +\n  geom_point(aes(color = conserv.eu)) +  \n  geom_smooth(method = \"lm\",\n              color = \"black\",\n              se = FALSE) +              \n  theme_light() +\n  scale_color_viridis_c() +                \n  facet_grid(OWS_egg ~ family, scales = \"free\") # faceting\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThe above plot would need a little ‘cleaning up’. How would you do that?",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#some-common-plot-types",
    "href": "ggplot.html#some-common-plot-types",
    "title": "5  The ggplot2 package",
    "section": "5.4 Some common plot types",
    "text": "5.4 Some common plot types\nWe have looked at scatterplots, now let’s look at a number of other commonly used plots.\n\n# histograms\nbe %&gt;% \n  ggplot(aes(x = WSP_Female_average, fill = family)) +\n  geom_histogram(bins = 50) +\n  theme_light() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n# better alternative are often density plots\nbe %&gt;% \n  ggplot(aes(x = WSP_Female_average, fill = family)) +\n  geom_density(alpha = 0.5, colour = NA) +\n  theme_light() +\n  scale_fill_brewer(palette = \"Set1\") \n\n\n\n\n\n\n\n# boxplots\nbe %&gt;% \n  ggplot(aes(y = WSP_Female_average, x = family)) +\n  geom_boxplot() +\n  theme_light() \n\n\n\n\n\n\n\n# better alternative are violin plots\nbe %&gt;% \n  ggplot(aes(y = WSP_Female_average, x = family)) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  theme_light()",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#fine-tuning-plots",
    "href": "ggplot.html#fine-tuning-plots",
    "title": "5  The ggplot2 package",
    "section": "5.5 Fine tuning plots",
    "text": "5.5 Fine tuning plots\nWe now know how to plot some common chart types but most of these don’t look publishable yet. Lets return to one of our first examples and see how to polish it a little.\n\n# A publication ready plot\nbe %&gt;% \n  filter(family != \"Riodinidae\") %&gt;%\n  ggplot(aes(y = WSP_Female_average, x = family, color = family)) +\n  geom_boxplot(lwd = 1, outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width = 2), alpha = 0.5) +\n  theme_light() +\n  labs(title = \"Wing span across European butterfly families\",\n       y = \"Average wing span of female\")+ \n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title.y = element_text(size = 12),\n        axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        strip.text = element_text(size = 11)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Family\")",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#exercise",
    "href": "ggplot.html#exercise",
    "title": "5  The ggplot2 package",
    "section": "5.6 Exercise",
    "text": "5.6 Exercise\nUsing ggplot2, explore how range size differs between butterfly families and plot check if butterflies with smaller ranges have higher protection status. Try to find appropriate plot types for this, use the help pages to find plot types that were not introduced to you yet. Explore other variables that may explain some trends in the data. Find a theme that you like! Remember, start with the data, add aesthetics (what do you want to plot), and then think about geometric objects (how do you want to plot the data). How can colour help in your visualisations? Does faceting make sense?\n\nbe %&gt;% \n  mutate(OWS_egg = as.factor(OWS_egg)) %&gt;% \n  ggplot(aes(x = conserv.europe, group = OWS_egg, fill = OWS_egg)) +\n  geom_bar(position = position_dodge()) \n\nWarning: Removed 37 rows containing non-finite outside the scale range\n(`stat_count()`).",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "unixIntro.html",
    "href": "unixIntro.html",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "",
    "text": "6.1 The Unix / Linux environment\nUnix and Linux (a variant of Unix) are operating systems (like Windows or macOS). They belong to a “family” of operating systems that share a common ancestor, have been around since 1969 and it’s not likely to disappear any time soon.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#the-unix-linux-environment",
    "href": "unixIntro.html#the-unix-linux-environment",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "",
    "text": "Commonly used among the scientific and technical community (e.g. servers and scientific clusters).\nmacOS is Unix-based system.\nMost supercomputers are powered by Unix-like operating systems.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#why-learn-unix-command-line",
    "href": "unixIntro.html#why-learn-unix-command-line",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "6.2 Why learn Unix command line?",
    "text": "6.2 Why learn Unix command line?\n\nIs the foundation of scientific computing (e.g. bioinformatics and data analysis)\nPowerful for working on large datasets and files\nHelps automate repetitive tasks (e.g. imagine you need to need to rename or modify 1,000 files?)\nEnables use of higher-powered computers elsewhere (clusters and servers/cloud-computing)",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#some-terminology",
    "href": "unixIntro.html#some-terminology",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "6.3 Some terminology",
    "text": "6.3 Some terminology\n\n\n\nimage1.png\n\n\nAs a user you can “communicate” with your Linux system either by a Graphical User Interface (GUI) or by typing instructions (commands) using a Command Line Interface (CLI). At first it might look quite complex and confusing but once you understand the concept and the basics then its quite simple and intuitive!\nCommand Line: is the written instructions we type.\nTerminal: also known as terminal emulator is the text-based environment (software) capable of taking input and providing output.\nShell: a program that interprets command-line input and executes commands. There are different shells available e.g bash, zsh, sh, csh etc. Each of them offering unique features and functionalities. Some are more basic and some are more fancy but all serve the same purpose, to interpret the commands provided by the user and output the results. The most commonly used is the bash shell.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#some-important-rules",
    "href": "unixIntro.html#some-important-rules",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "6.4 Some important rules",
    "text": "6.4 Some important rules\n\nBe aware of the case! The command line is case sensitive so be careful when typing. For example, typing Echo is not the same as echo, nor are the directory names “/Results” and “/results”.\nSpaces are having special use! The command line uses spaces as separators between arguments. Using spaces in filenames or directory names will certainly cause problems sooner or later. Avoid using names that contain spaces, but rather it’s better to use dashes (-) or underscores (_). e.g., “results_2026.txt” is preferred over “results 2026.txt”.\nApart from spaces there are several other characters (special characters) that can be used to perform special operations. See some examples bellow.\n\n\n\n\n\n\n\n\nCharacter\nDescription\n\n\n\n\n/\nDirectory separator, used to separate a string of directory names. Example: /usr/src/linux\n\n\n\\\nEscape — (backslash) prevents the next character from being interpreted as a special character. This works outside of quoting, inside double quotes, and generally ignored in single quotes.\n\n\n.\nCurrent directory. Can also “hide” files when it is the first character in a filename.\n\n\n..\nParent directory\n\n\n~\nThe tilde is a representation of the current user’s home directory.\n\n\n*\nRepresents 0 or more characters in a filename, or by itself, all files in a directory.\n\n\n?\nRepresents a single character in a filename.\n\n\n$\nExpansion — introduces various types of expansion: parameter expansion (e.g. $var or ${var}), command substitution (e.g. $(command)), or arithmetic expansion (e.g. $((expression))). More on expansions later.\n\n\n[ ]\nCan be used to represent a range of values, e.g. [0-9], [A-Z], etc. Example: hello[0-2].txt represents the names hello0.txt, hello1.txt, and hello2.txt\n\n\n|\nPipe — send the output from one command to the input of another command. This is a method of chaining commands together. Example: echo “Hello beautiful.” | grep -o beautiful.\n\n\n&gt;\nRedirect output of a command into a new file. If the file already exists, over-write it. Example: ls &gt; myfiles.txt\n\n\n&gt;&gt;\nRedirect and appends the output of a command onto the end of an existing file.\n\n\n&lt;\nRedirect a file as input to a program.\n\n\n;\nCommand separator. Allows you to execute multiple commands on a single line. Example: cd /var/log ; ls -l\n\n\n&&\nCommand separator as above, but only runs the second command if the first one finished without errors.\n\n\n&\nBackground – when used at the end of a command, run the command in the background (do not wait for it to complete).\n\n\n#\nComment — the # character begins a commentary that extends to the end of the line. Comments are notes of explanation and are not processed by the shell.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#accessing-your-terminal",
    "href": "unixIntro.html#accessing-your-terminal",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "6.5 Accessing your Terminal",
    "text": "6.5 Accessing your Terminal\nYou are using a Kubuntu Linux system. In Kubuntu, the terminal emulator is called Konsole. You can start it in any of the following ways:\n1. Press Ctrl + Alt + T to open Konsole instantly.\n2. Open the Krunner by pressing Alt + Space,  type Konsole, then press Enter.\n3. Open the Application Launcher → System → Konsole.\nNow you are ready to start typing your first commands!",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixPart1.html",
    "href": "unixPart1.html",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "",
    "text": "7.1 Finding out where you are\nUnix systems, like most operating systems, store file locations in a hierarchical structure. In the UNIX file-system each file and directory has its own “address”, and that address is called a “path”.\nThere are two special locations in all Unix-based systems That you should be familiar. The “root” location is where the address system of the computer starts. The “home” location is where the current user’s location starts.\nBy default every time you open a new terminal you start in your own “home” directory(containing files and directories that only you can modify). The path of home directory is usually represented by the “~” character.\nBasic commands we will use\nThe pwd command in Linux is short for print working directory. It’s only function is to print the absolute path of the current directory. It’s handy when you’re not exactly sure what directory you’re in. So make it a good habit to get used to running the pwd command a lot.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#finding-out-where-you-are",
    "href": "unixPart1.html#finding-out-where-you-are",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "",
    "text": "# Example. Try running pwd. What do you see?\npwd\n# What do you see if you run PWD instead? \nPWD\n# Try now to run this \"echo $PWD\". The command echo just prints the parameters we give it. The $PWD is a environment variable and we will discuss about their use a bit later on.\necho $PWD",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#the-command-ls",
    "href": "unixPart1.html#the-command-ls",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "7.2 The command ls",
    "text": "7.2 The command ls\nls is short for list, and is used to list the files and sub-directories in your present working directory or some other directory if you specify one.\n# Examples\n# List the files and directories in your current directory\nls\n# or\nls .\n# ls can accept several options. Try running the following commands and observe how their output differs from the previous one.\nls -l\n# or\nls -lh\n# You can use ls to list the contents of any directory. Try the following.\nls -l /etc \n\n\n\n\n\n\nNoteNote\n\n\n\nI. The anatomy of a command (or command syntax)\nEach command is usually composed of three parts:\n\nThe command itself\nThe options: These are optional parameters that can be used to customise the behavior of a command. (e.g. on the previous examples ls -l shows the list of files in a long format)\nThe arguments: specify the target of the command. (e.g. on the previous example ls -l /etc you instructed the command to list the contents of the /etc directory)\n\nII. Getting help\nMost Unix commands can accept several parameters. How do we know which ones to use and why? Luckily, most Unix commands have built-in help documentation that we can access by providing –help as the only argument.\nTry for example: ls --help\nAnother way to access the documentation for a command is by using the man command and providing the command’s name as an argument. For example:\nman ls",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#relative-vs-absolute-path-and-getting-help",
    "href": "unixPart1.html#relative-vs-absolute-path-and-getting-help",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "7.3 Relative vs absolute path and getting help",
    "text": "7.3 Relative vs absolute path and getting help\nThere are two ways to specify the path (the file’s address on the computer):\n\nAn absolute path starts from a fixed location, either the root directory (/) or the home directory (~/). Note: A “full path” usually refers to an absolute path that starts from the root (/).\nA relative path starts from your current directory.\n\n\nWhen working at the command line, it’s important to always be aware of your current location in the system. One of the most common mistakes is trying to operate on a file that isn’t where you think it is. To avoid this, it’s good practice to use absolute paths, which clearly specify a file’s exact location regardless of where you are.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#moving-around",
    "href": "unixPart1.html#moving-around",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "7.4 Moving around",
    "text": "7.4 Moving around\nOne of the most commonly used commands in Linux is the change directory command, or cd. It allows you to change your working directory from the current location to another directory you want to navigate to. The cd command takes a positional argument: the path (address) of the directory you want to move into. This path can be either absolute or relative. Let’s try moving from our current directory to a directory present in your home directory called Documents.\n# The relative way\ncd Documents\n# The absolute way (~ stands for /home/&lt;user&gt;/). Can you see the change in your command prompt?\ncd ~/Documents\n# But how do we go back “up” to the parent directory? We can use the \"..\" special characters that act as a relative path, telling the system to move up one directory from our current location.\ncd ..\n# When you need to navigate back to the previous working directory from the current working directory, you can use the \"–\" option.\ncd -\n# Note: running the cd command without any arguments will always bring you back to your home\ncd",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#exercise",
    "href": "unixPart1.html#exercise",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "7.5 Exercise",
    "text": "7.5 Exercise\nPractice moving around the filesystem with cd and listing directory contents with ls, including navigating by relative and absolute paths or using special characters “..” . Use pwd frequently to see your current working directory. Practice navigating home with cd.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart2.html",
    "href": "unixPart2.html",
    "title": "8  Working with Files and Directories",
    "section": "",
    "text": "8.1 Creating new directories and files\nYou can create a new directory using the mkdir command. It takes as a parameter a relative or absolute path to the directory to create\nExercise\nIn the previous example we created the two directories in two separate steps. Can we do it in one step instead? (check the help page for mkdir command).\nNow that we saw how to create a new directory lets see how we can create a new file. There are several ways to do this in Linux command line but we will start with the basic one. The command touch will create a new, empty file.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#creating-new-directories-and-files",
    "href": "unixPart2.html#creating-new-directories-and-files",
    "title": "8  Working with Files and Directories",
    "section": "",
    "text": "# Let's try some examples. From your home directory navigate to Documents/ and create 2 new nested sub-directories with the names projects and project1\npwd\ncd Documents\nmkdir projects\ncd projects\nmkdir projects1\npwd\n# Try to create a new directory with the same name e.g. projects1. What do you see?\n\n\n\n# Create an Empty File within the new directory project1.\ntouch notes.txt\n# can you do it if you are outside the project1 directory?",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#move-or-rename-a-file-or-directory",
    "href": "unixPart2.html#move-or-rename-a-file-or-directory",
    "title": "8  Working with Files and Directories",
    "section": "8.2 Move or Rename a File or Directory",
    "text": "8.2 Move or Rename a File or Directory\nThe mv command serves for both moving and renaming files and directories. Works by specifying a “source_path” and a “destination_path”, wheres “source_path” is the path (absolute or relative) of the file/directory to move or rename, and “destination_path” is the new name or location to give it.\nCopying files and directories is similar operation, except that the original file or directory is not removed! We will use the cp command for this.\n\nNOTE: copying, moving or renaming files at the command line will overwrite files if they have the same name!!\n\nLet’s try them both.\n# move the notes.txt file one directory up assuming your current directory is project1\nmv notes.txt ../notes.txt\n# rename the notes.txt to README.txt\nmv ../notes.txt ../README.txt\n# copy the README.txt file to project1 directory as README1.txt\ncp ../README.txt ./README1.txt\nExercise\nTry to make a copy of the complete project1 directory on the same parent location with the name project2. What do you see?\n\n\n\n\n\n\nNoteNote\n\n\n\nWildcards: friend and foe at the command prompt.\nAs we saw earlier some characters have special use in Unix command line. For example:\n\nthe asterisk character “*” represents 0 or more characters in a filename, or by itself, all files in a directory.\nthe square brackets […] can be used define a range of values e.g. [0-9], [A-Z], etc.\nthe question mark “?” can represent any single character.\n\nThese characters can be used as “Wildcards” to perform operations on multiple files at the same time. Lets see some examples.\n# Create a directory with the name wildcard_practice and, inside it, create some empty files.\nmkdir wildcard_practice\ncd wildcard_practice\ntouch file1.txt file2.txt file10.txt\ntouch image1.png image2.png imageA.png\ntouch data_2022.csv data_2023.csv data_2024.csv\n# List all the .txt files\nls *.txt\n# create a directory called data and move the .csv files into data/\nmkdir data\nmv *.csv data/\nExercise List only the .csv files with years 2022 or 2023",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#delete-remove-files-and-directories",
    "href": "unixPart2.html#delete-remove-files-and-directories",
    "title": "8  Working with Files and Directories",
    "section": "8.3 Delete (remove) files and directories",
    "text": "8.3 Delete (remove) files and directories\n\n\n\n\n\n\nWarningWARNING!!\n\n\n\nUsing the rm command permanently deletes files and directories without moving them to a trash or recycle bin. This action cannot be undone! Whenever you use the rm command, ALWAYS double-check your syntax.\n\n\n# Lets remove some unwanted files and directories\nrm ~/Documents/projects/README.txt\n# the command rmdir can remove an empty directory, so it is safer!\nmkdir test_dir\nrmdir test_dir\n# The dangerous way. Use rm recursively \"-r\" to remove a non-empty directory. BE CAUTIOUS! YOU HAVE BEEN WARNED!\nrm -r ~/Documents/projects/project1\n# check the help page for rm command. Is there a safer way to do this? \nExercise\n\nCreate a directory data.\nInside data, create three empty files: sample1.txt, sample2.txt, sample3.txt\nCopy sample1.txt into a new directory called backup.\nRename sample2.txt to s2.txt.\nRemove sample3.txt.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#viewing-and-inspecting-files",
    "href": "unixPart2.html#viewing-and-inspecting-files",
    "title": "8  Working with Files and Directories",
    "section": "8.4 Viewing and inspecting Files",
    "text": "8.4 Viewing and inspecting Files\nSometimes we want to quickly look at files, either to inspect their structure or to get some basic information about their contents. These are some commands we can use to do so.\n\n\n\nCommand\nDescription\n\n\n\n\ncat\nprint entire file\n\n\nless\nscroll through file\n\n\nhead\nshow first n lines\n\n\ntail\nshow last n lines\n\n\nwc\ncount (word count)\n\n\n\n# Lets use the file \"butterfly_ecology.csv\" you previously used with R.\n# You can either navigate to the folder you saved the file copy it to a new directory.\n# You can use the \"cat\" command for viewing the contents of a file\ncat butterfly_ecology.csv\n# Another command for viewing the contents of a file is \"less\". This command allows to scroll through the document and view it line my line or page by page. We can even do some text searching. To exit less, press q (for quit).\nless butterfly_ecology.csv\n# However, most of the time we only need to see part of a file rather than the entire file. The commands \"head\" and \"tail\" come in handy. These commands print only the first (head) or the last (tail) lines of a file.\nhead butterfly_ecology.csv\ntail butterfly_ecology.csv\n# or you can specify the number of lines to print (by default will print 10)\nhead -n 2 butterfly_ecology.csv\ntail -n 3 butterfly_ecology.csv\n# The command wc (word count) is useful for counting how many lines, words, and characters there are in a file.\nwc butterfly_ecology.csv\n# Can you find an option for wc that will let us get only the number of lines of the file? \n\n\n\n\n\n\nNoteNote\n\n\n\nThe cat command can also be used to create new files or to concatenate existing files. Lets try:\ncat &gt; file1\nStefanos\n# Exit by pressing Ctrl + D\ncat &gt; file2\nsome random text\n# Exit by pressing Ctrl + D\ncat file1 file2 &gt; file3",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#a-text-editor-for-the-terminal",
    "href": "unixPart2.html#a-text-editor-for-the-terminal",
    "title": "8  Working with Files and Directories",
    "section": "8.5 A text editor for the terminal",
    "text": "8.5 A text editor for the terminal\nA simple text editor available on most systems is nano. To run it, simply specify a file name to edit. If the file doesn’t exist already, it will be created after saving. \nnano NOTES.txt",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart3.html",
    "href": "unixPart3.html",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "",
    "text": "9.1 Redirecting and piping\nRedirection and piping are fundamental features of the UNIX command line that allow us to create powerful workflows for automating tasks. By default, when we run a command, its output is printed to the screen (the terminal). In many situations, however, we may want to save this output to a file or pass it directly to another command.\nWe have already seen how a command output can be redirected to a file. For example: cat file1 file2 &gt; combined_file. In this command, the redirection operator &gt; tells the shell to send the output of cat to a new file called combined_file instead of displaying it on the screen. Let look at another example.\nWhile redirection allows us to save a command’s output to a file, UNIX also allows us to connect commands directly to one another. We can send the output of one command directly as input to another using the pipe (|) operator.\nExercise",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#redirecting-and-piping",
    "href": "unixPart3.html#redirecting-and-piping",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "",
    "text": "# list the files in the /etc directory and save the output to a file called etc_content.txt.\nls /etc &gt; etc_content.txt\nless etc_content.txt\n\n\n\n\n\n\nWarningWARNING!!\n\n\n\nIt’s important to remember that the &gt; operator will overwrite a file if it already exists. If you want to append an output to an existing file, rather than overwrite it, you can use instead &gt;&gt;.\n\nTry it out yourself!\n\n\n\n\n#list only the first 10 files of the /etc directory\nls /etc | head -n 10\n\n\nUsing the pipe operator, find a way to count all contents (files and directories) in the /etc directory.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#cut-sort-and-uniq",
    "href": "unixPart3.html#cut-sort-and-uniq",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "9.2 cut, sort, and uniq",
    "text": "9.2 cut, sort, and uniq\nThe cut command is used for extracting specific sections from lines of text in a file or piped data. It’s a great tools for or data manipulation. Lets try some examples:\n# Selecting fields separated by a delimiter\necho \"name,age,city,country\" | cut -d ',' -f 3\n# lets manipulate some real data\ncat butterfly_ecology.csv | cut -d ',' -f 1,2 | head\n# you can even change the delimiter in the output\ncat butterfly_ecology.csv | cut -d ',' -f 1,2  --output-delimiter $'\\t' | head\n# or\ncat butterfly_ecology.csv | cut -d ',' -f 1,2,5-7  --output-delimiter $'\\t' | head\nThe sort command is used to arrange the lines of text files in a specified order, such as alphabetically or numerically. It can also handle options for sorting in reverse order or by specific columns. NOTE: sort command will not modify your file, it will only print the reordered content on the terminal! However, you can specify redirect the output to a separate file.\ncat butterfly_ecology.csv | cut -d ',' -f 1-3 | sort\ncat butterfly_ecology.csv | cut -d ',' -f 1-3 | sort -t ',' -k3 | less\nThe uniq command in Linux is used to filter out repeated lines from a text file or standard input, displaying only unique entries or counting repetitions. It works best when the input is sorted, as it only removes adjacent duplicate lines.\n# families are represented in the butterfly_ecology.csv data?\ncat butterfly_ecology.csv | cut -d ',' -f 2 | sort | uniq\nExercise\n\nCan you use cut, sort and uniq commands to count the number of species per family species per family?",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#grep-and-regular-expressions",
    "href": "unixPart3.html#grep-and-regular-expressions",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "9.3 grep and regular expressions",
    "text": "9.3 grep and regular expressions\nThegrep (global regular expression) command in Linux is used to search for specific patterns or strings within files and display the matching lines. It is a powerful tool for text processing and can be customized with various options to refine search results. The basic usage is: grep “searchword” filename\n# Let's say you wished to identify every line which contain the string \"Papilionidae\" from the butterfly_ecology.csv\ngrep 'Papilionidae' butterfly_ecology.csv\n# We can find the number of lines that matches the given string/pattern instead\ngrep -c 'Papilionidae' butterfly_ecology.csv\n# Use -f  option to read patterns from a file\ncat &gt; family.txt\nPapilionidae\nHesperiidae\n# exit by pressing Ctrl + D\ngrep -f family.txt butterfly_ecology.csv\n# We can search for patterns in multiple files (e.g. all the files in the directory)\ngrep 'Papilionidae' *\ngrep command is particularly powerful when used in combination with Regular Expressions. A regular expression (regex) in Linux is a sequence of characters that defines a search pattern, similar to the wildcards, and are commonly used for searching and manipulating text.\nExercises\n\nIn the butterfly_ecology.csv file find all “Aglais” species and save the fields species, family, range.size in a new file.\nThe shell keeps a record of the commands you have previously run. You can display this list using the history command. Using this information, determine how many times you have used the ls command in your shell history. Hint: You may need to combine history with other command-line tools such as grep.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#awk",
    "href": "unixPart3.html#awk",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "9.4 awk",
    "text": "9.4 awk\nawk is the Unix command to work with tabular data. The basic awk syntax is: awk [options] ‘pattern {action}’ input-file &gt; output-file\nawk  -F ',' '{print $0}' butterfly_ecology.csv\n\nawk -F ',' '$3 &gt; 1000 && $3 != \"NA\" {print $0}' butterfly_ecology.csv\n\nawk -F ',' '$3 &gt; 1000 && $3 != \"NA\" {print $1, $3}' butterfly_ecology.csv\n\nawk -F ',' '{if($3 &gt; 1000 && $3 != \"NA\") {print $1\":::\"$3}}' butterfly_ecology.csv\nA more complex example\nUsing only command line try calculate the average range.size for each family separately and store this information in a new file.\ngrep -v \"^species\" butterfly_ecology.csv | \\\nawk -F ',' '\n$3 != \"NA\" {\n  sum[$2] += $3\n  count[$2]++\n}\nEND {\n  for (family in sum) {\n    print family, sum[family] / count[family]\n  }\n}\n' OFS=',' &gt; average_range_by_family.csv",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart1.html",
    "href": "NGSanalysisPart1.html",
    "title": "10  Installing bioinformatics tools and the conda package manager",
    "section": "",
    "text": "10.1 What is Conda and why is needed\nAnalysis of Next-Generation Sequencing (NGS) data relies on specialized bioinformatics tools (e.g., FastQC, BWA, SAMtools).\nThese tools are often developed by different people, in different programming languages, and with different software dependencies, and installing them is not always straightforward (or at least it used to be).\nConda is a cross-platform package/tool and environment managing system that runs on Windows, macOS, and Linux.\nIt allows to:",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing bioinformatics tools and the conda package manager</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart1.html#what-is-conda-and-why-is-needed",
    "href": "NGSanalysisPart1.html#what-is-conda-and-why-is-needed",
    "title": "10  Installing bioinformatics tools and the conda package manager",
    "section": "",
    "text": "Install bioinformatics tools and not only - No need to compile software from source, after all we all have other things to worry about!\nAutomatically resolve dependencies - No need to worry about your missing library!\nCreate isolated software environments - Less chances to mess up with the rest of the software or the system itself!\n\n\n10.1.1 Installing and setting-up a conda environment in your linux system\nFo the purpose of this course we will use the miniforge minimal installer. Use your terminal window (konsole) to download the miniforge installer by running the following command. Yes! you can use your terminal to download files from the web. For this we will use the UNIX command wget. Feel free to copy paste the command to avoid typos.\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nand run the script like this:\nbash Miniforge3-$(uname)-$(uname -m).sh\nNow follow the instructions and accept the license terms. When prompt confirm the installation location “/home/idiv/miniforge3”.\nReply yes to “Proceed with initialaization?”\nCongratulations! You have installed conda. But in order for the installation to take effect you should re-load (restart) your terminal or run the following command:\nsource .bashrc\nRun the following command to prevent the activation of the base conda environment on startup. This saves you from having problems later!\nconda config --set auto_activate_base false\nNow it is time to configure some settings, such as adding the necessary channels. Software packages are stored in locations called channels. Most bioinformatics packages are available through the bioconda channel.\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nconda config --show channels\nCreate a conda environment for installing the necessary packages/tools. You can use either conda or mamba command. Mamba is a faster implementation of conda designed to improve the speed of package installation and environment management.\nFor simplicity, we will create our conda environment using an environment.yaml file, which contains a list of the packages required for this course. You will need to download the environment.yaml file from STUDIP and place it in your home directory.\n# Create the environment and install required packages. It might take a few minutes!\nconda env create -f environment.yaml\n# Inspect the environment is there\nconda info -e\n# Activate the environment. You can deactivate the environment using the comand \"conda deactivate\"\nconda activate mlu2026\nAlternatively, you can create an empty conda environment and then install the required packages/tools. You can search for bioconda packages here.\n# using mamba\nmamba create -n mlu2026 # This command will create an empty environment with the name \"mlu2026\". You can use any name for your environment, just avoid using the space character or any other special character!\n# similarly using conda\nconda create -n mlu2026\n# You can now activate your environment using one of the following commands.\nmamba activate mlu2026\n# or\nconda activate mlu2026\nHere you can find a conda-cheatsheet that you can use as a quick reference for managing your Conda environments.",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing bioinformatics tools and the conda package manager</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html",
    "href": "NGSanalysisPart2.html",
    "title": "11  NGS data quality control",
    "section": "",
    "text": "11.1 Obtain some high-throughput sequencing data\nThis practical will provide hands-on experience with quality control of high-throughput sequencing data. You will learn how to:\nFor this practical we will need to download some training sequencing data. We will use the following data (Bioproject PRJNA675888) associated with this article:\nIllumina: SRA paired-end dataset SRR13070681\nNanopore: SRA ONT dataset SRR13070731\nFirst you will need to create a data/ directory in your home folder. Within this directory create two sub-directories one for Illumian and one for the nanopore data.\nOption 1. Browser Navigate to the European Nucleotide Archive (ENA) and search for the sra accession numbers (). Download the files and move them to the respective directory.\nOption 2. You can use the command line to download the data.",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html#obtain-some-high-throughput-sequencing-data",
    "href": "NGSanalysisPart2.html#obtain-some-high-throughput-sequencing-data",
    "title": "11  NGS data quality control",
    "section": "",
    "text": "mkdir -p ~/data/illumina\nmkdir -p ~/data/nanopore\n\n\n# For the Illumina dataset\ncd ~/data/illumina\nwget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR130/081/SRR13070681/SRR13070681_1.fastq.gz\nwget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR130/081/SRR13070681/SRR13070681_2.fastq.gz\n# For the nanopore dataset\ncd ~/data/nanopore\nwget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR130/031/SRR13070731/SRR13070731_1.fastq.gz\n\n\n\n\n\n\nNoteNOTE\n\n\n\nWhat do the _1 and _2 in the Illumina dataset mean?\nMost Illumina sequencing is paired-end, meaning that after DNA fragmentation, both ends of each fragment are sequenced. This produces two reads for each DNA fragment: one read from one end of the fragment (_1, read 1) and a second read from the opposite end (_2, read 2).\n\n\n\npaired-end reads",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html#short-read-data-qc-illumina",
    "href": "NGSanalysisPart2.html#short-read-data-qc-illumina",
    "title": "11  NGS data quality control",
    "section": "11.2 Short-read data QC (Illumina)",
    "text": "11.2 Short-read data QC (Illumina)\nWe can inspect the fastq files\n# make sure you are directory ~/data/illumina\npwd\n# read the 10 first sequences\nzcat SRR13070681_1.fastq.gz | head -40\nWe can now generate some quality metrics for our data using the tool FastQC. For each file, FastQC will produced both a .zip archive containing all the plots, and a html report. You can run FastQC on the two files together or individually.\nmkdir fastqc_output\nfastqc -o fastqc_output *.fastq.gz\nQuestions:\n\nHow many total reads are in both files?\nWhat is the length of the read?\nWhich read files is of better quality?\n\n\n11.2.1 Quality control\nQuality control generally comes in two forms: 1. Trimming: involves removing poor quality bases from the reads (usually the ends) 2. Filtering: involves removing whole sequences either due to poor quality or they are too short\nTo carry this out we will use sickle. Let’s first have a look on the documentation page of sickle.\nsickle --help\n#  You can run now sickle with the Illumina reads using the following command.\nsickle pe -t sanger -f SRR13070681_1.fastq.gz -r SRR13070681_2.fastq.gz -o SRR13070681_1_Q28_MinL100.fastq.gz -p SRR13070681_2_Q28_MinL100.fastq.gz -s SRR13070681_unpaired.fastq.gz -q 28 -l 100\nWe can now re-run fastqc on the trimmed dataset.\nmkdir sickle_fastqc_output\nfastqc -o sickle_fastqc_output SRR13070681_*_Q28_MinL100.fastq.gz\nBonus. We can use the tool MultiQC to aggregate all the FastQC reports into one html report.\nmultiqc ~/data/illumina/fastqc_output ~/data/illumina/sickle_fastqc_output",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html#long-read-data-qc-oxford-nanopore",
    "href": "NGSanalysisPart2.html#long-read-data-qc-oxford-nanopore",
    "title": "11  NGS data quality control",
    "section": "11.3 Long-read data QC (Oxford Nanopore)",
    "text": "11.3 Long-read data QC (Oxford Nanopore)\nExercise\nCheck the quality of the Nanopore dataset. How does it differ from the Illumina reads?\n\n\n\n\n\n\nNoteNOTE\n\n\n\nSubsampling the sequencing data\nSometimes ngs data can be quite large. In this cases it is useful to downsample the data to a more manageable dataset.\nWe can use the tool seqkit to do this.\n# we can do this by number\nzcat file.fastq.gz | seqkit sample -n 100000 -o sample.fastq.gz\n# or by proportion e.g. 10%\nzcat file.fastq.gz | seqkit sample -p 0.1 -o sample.fastq.gz",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html",
    "href": "NGSanalysisPart3.html",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "",
    "text": "12.1 Prepare the materials for the practical\nThis tutorial provides hands-on experience with a typical metabarcoding workflow for analyzing microbial communities using full-length 16S rRNA ONT sequencing. We will begin with pre-basecalled data, and after performing initial quality control and demultiplexing of the reads, we will assign taxonomy directly to the reads and generate taxon abundance tables for subsequent diversity analysis. The dataset originates from 16S rRNA amplicon sequencing of multiple samples from two tick species: Dermacentor marginatus and Dermacentor reticulatus.\nDownload the materials (ONT_metabarcoding.tar.gz) using the link below.\nlink\nDdecompress the folder with the material using the command\nNavigate to the extracted follder using the cd command",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html#prepare-the-materials-for-the-practical",
    "href": "NGSanalysisPart3.html#prepare-the-materials-for-the-practical",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "",
    "text": "tar -xvzf ONT_metabarcoding.tar.gz  \n\ncd ONT_metabarcoding",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html#quality-filtering-and-inspection.",
    "href": "NGSanalysisPart3.html#quality-filtering-and-inspection.",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "12.2 Quality filtering and inspection.",
    "text": "12.2 Quality filtering and inspection.\nBefore beginning any analyses is a good practice to perform a QC assessment and quality/length filtering of the data.\nFirst create a directory for the filtered reads\nmkdir filtered_reads  \nPerform quality and length filtering using SeqKit. (Length filtering is optional, as it will also be performed during the demultiplexing step.)\nseqkit seq --min-qual 10 --max-len 1800 --min-len 1200 calls_sup_ticks_16S.fastq &gt; filtered_reads/calls_sup_ticks_16S_Q10_1200-1800.fastq\nInspect and compare the reads before and after filtering. How many reads were retained? What is the average read length? How do the quality scores look (mean quality)?\nseqkit stats calls_sup_ticks_16S.fastq filtered_reads/calls_sup_ticks_16S_Q10_1200-1800.fastq\nfastqc calls_sup_ticks_16S.fastq filtered_reads/calls_sup_ticks_16S_Q10_1200-1800.fastq",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html#demultiplexing",
    "href": "NGSanalysisPart3.html#demultiplexing",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "12.3 Demultiplexing",
    "text": "12.3 Demultiplexing\nNow we are ready to demultiplex your data (split the reads by sample). To do this we need some demuliplexing information that usually comes in a form of a table (depending on the specific tool). This table contains information on the sample IDs, the primers used for amplification and the unique dual indices used for each sample. We will use the tool speimux (https://github.com/joshuaowalker/specimux).\nInspect the demultiplexing files in the directory.\nless 16S_specimens.txt\n# press q to exit\nless 16S_primers.fasta\n# press q to exit\nRun specimux command for demultiplexing the ONT reads.\nspecimux 16S_primers.fasta 16S_specimens_sub.txt calls_sup_ticks_16S_Q10_1200-1800.fastq\\\n--output-to-files\\\n--trim primers\\\n--search-len 100\\\n--index-edit-distance 2\\\n--primer-edit-distance 10\\\n--min-length 1300\\\n--max-length 1700\nThe option meanings are:\n--output-to-files: Create individual sample files for sequences\n--trim primers: Remove the primer and the adapters from the read sequence\n--search-len: Length to search for index and primer at start and end of sequence\n--index-edit-distance: The maximum allowed edit distance when matching indices during the demultiplexing process. How many mismatches (or differences) are permitted\n--primer-edit-distance: The maximum allowed edit distance when matching primers. How many mismatches (or differences) are permitted\n--min-length: Minimum sequence length. Shorter sequences will be skipped\n--max-length: Maximum sequence length. Longer sequences will be skipped\nThe demultiplexed reads can be found in the directory \\~/ONT_metabarcoding/data/intermediate/tutorial_2/specimux_16S_demuliplexed/full. There should be X .fastq files corresponding to the X samples.\nInspect the demultiplexed files. How many reads are there per sample? What is the average read size? Why we removed the primers?\nseqkit stats ./16S/full/*.fastq\n\n# Or you can save the summary in a file\n\nseqkit stats ./16S/full/*.fastq | sed 's/,//g' &gt; summary.txt \n\n\n\n\n\n\nNoteNote\n\n\n\nAmong our samples, we have included negative controls (PN.fastq) to monitor for contamination during the PCR amplifications. As indicated in your summary, some of the samples did not generate enough reads, similar to the negative controls. It is always good practice to remove such samples from further analysis, as they may be affected by contamination. What would be an appropriate cutoff for the number of reads to use when identifying and removing failed samples?\n# Create a directory to store the files that have few reads\nmkdir failed\n# Identify the files with not enougth reads and create a list with their names\ncat summary.txt | sed 's/,//g' | awk '{ if ($4 &lt; 250) { print } }' | cut -f1 -d \" \" &gt; remove.txt\n# Now you can move the failed files to the faild/ directory using a for loop\nfor i in $(cat remove.txt); do mv $i failed/; done\n# Or using another unix command `xargs`\nxargs -a remove.txt -I {} mv {} failed/",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html#taxonomic-profiling",
    "href": "NGSanalysisPart3.html#taxonomic-profiling",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "12.4 Taxonomic profiling",
    "text": "12.4 Taxonomic profiling\nThe next goal is to generate taxonomic abundance profiles from our demultiplexed ONT data. This can be done using two different methodologies depending on the approach used for taxonomic assignment. The first methodology assigns taxonomy directly to the individual reads while the second performs clustering of the reads and builds consensus sequences before proceeding with taxonomic assignment.\nPerform taxonomic classification of the reads and estimate taxonomic abundance in each sample using the Emu software. For additional details on the method see the original article (Curry, K.D., Wang, Q., Nute, M.G. et al. Emu: species-level microbial community profiling of full-length 16S rRNA Oxford Nanopore sequencing data. Nat Methods 19, 845–853 (2022). https://doi.org/10.1038/s41592-022-01520-4).\nCreate a directory for the results\nmkdir emu_results\nAssign taxonomy and calculate relative abundances using the emu abundance command.\nfor i in 16S/full/*.fastq; do emu abundance ${i}\\\n--db ~/ONTworkshop_10_06_2025/db/gtdb_ssu_emu\\\n--min-abundance 0.01\\\n--output-dir emu_results; done\nThe option meanings are:\n--db: path to emu database; directory must include the following files, species_taxid.fasta, taxonomy.tsv\n\n\n\n\n\n\n\nFilename\nDescription\n\n\n\n\ntaxonomy.tsv\ntab separated datasheet of database taxonomy lineages containing at columns: ‘tax_id’ and any taxonomic ranks (i.e. species, genus, etc)\n\n\nspecies_taxid.fasta\ndatabase sequences where each sequence header starts with the respective species-level tax id (or lowest level above species-level if missing) preceeding a colon [:]\n\n\n\n--min-abundance: By default emu will report abundances equal or above 0.01% (0.0001). You can adjust this using this option. This will generates results with species relative abundance above this value in addition to full results; e.g. 0.01 = 1%.\nMove the full abundance reports in a separate directory.\nmkdir emu_results/full_abundance mv emu_results/\\*-abundance.tsv emu_results /full_abundance/\nCombine the outputs to create a single table containing all the emu output relative abundances using the combine-outputs function at the desired taxonomic rank (e.g. genus or species). Accepted ranks: [‘species’, ‘genus’, ‘family’, ‘order’, ‘class’, ‘phylum’, ‘superkingdom’]. Note this function will select all the .tsv files in the provided directory that contain ‘rel-abundance’ in the filename.\nemu combine-outputs emu_results/ genus\nNow we are ready to use the combined abundance table(s) for downstream diversity analysis and plotting in R using the phyloseq package. Follow the instructions in the emu-R-analysis.R file.",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "phylo1.html",
    "href": "phylo1.html",
    "title": "13  Retrieving and aligning sequences",
    "section": "",
    "text": "13.1 Compiling data\nThe following chapters are intended as a guide through the practical phylogenetics of the course, and as a starting point and reference for your future phylogenetics endeavours. It will cover the usage of the tools that will be introduced to you which together form a very basic phylogenetic workflow.\nYou usually already have a question in mind before you start you phylogenetic analysis. The choice of phylogenetic markers strongly depends on your question: in general you want to use fast evolving markers to resolve recent evolutionary events, and slow evolving markers for ancient evolutionary events. A good starting point for marker selection is the literature: likely someone has already worked on a similar type of question and has identified markers that are potentially useful for you. If your interest is protein evolution you would of course use the protein sequences of interest directly.\nPhylogenies can in principle be reconstructed from many different types of data. In this course, we will focus on DNA sequences. Once you have decided on a marker, you will need to compile your sequences. A good starting point for this is NCBI (National Center for Biotechnology Information) which hosts a massive database of protein and nucleotide sequences.. You will find almost any protein or nucleotide sequence ever published in these databases. The search function is powerful and supports boolean operators (Fig. 1).\nFig. 1 | NCBI landing page with search bar.\nLet’s download some sequences! Select the file accessions_coi.txt from the data folder and use the Batch Entrez feature of NCBI. This convenient tool allows us to simply upload a list of accession numbers and NCBI will retrieve the entries, which you can then download (Fig. 2).\nFig. 2 | Downloading sequences on NCBI. 1. Click Send to; 2. select File; 3. select FASTA; 4. click Create file to download.\nSome words about the sequence data we will be using in the course. We will look at two loci from 60 different species of bees, all occuring in Germany. One locus is the mitochondrial cytochrome subunit 1 (COI, also known as the “barcoding locus”). The other locus is the partial long-wavelength rhodopsin gene (lwr), which contains both coding regions and non-coding, intronic regions.\nBefore we move on, we should check and fix the sequence names. Often one has little control over the identifiers other people put on databases, and it is good to check and correct names at this stage. We can use the UNIX tools we learned about earlier to do this\nWhat are the names of the sequences?\ngrep '&gt;' data/coi.fas | head\n\n&gt;JN262171.1 Andrena chrysosceles voucher BC ZSM HYM 05913 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ836763.1 Andrena proxima voucher BC ZSM HYM 00016 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ837959.1 Epeolus schummeli voucher BC ZSM HYM 08497 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838028.1 Nomada sexfasciata voucher BC ZSM HYM 06153 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838166.1 Colletes succinctus voucher BC ZSM HYM 02017 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838491.1 Epeoloides coecutiens voucher BC ZSM HYM 01997 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838499.1 Nomada leucophthalma voucher BC ZSM HYM 01713 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838914.1 Colletes nasutus voucher BC ZSM HYM 02013 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838921.1 Andrena clarkella voucher BC ZSM HYM 00208 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838946.1 Nomada fabriciana voucher BC ZSM HYM 06144 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\nThese are too long and contain white spaces, which will lead to problems downstream. Let’s fix the names! Before actually changing the file, make sure the script works as expected. Can you try to figure out what the three sed substitution patterns do? If not, run them one by one and look at the result.\nsed -e 's/ voucher.*//' -e 's/&gt;.*1 /&gt;/' -e 's/ /_/' data/coi.fas | head -n 20\n\n&gt;Andrena_chrysosceles\nATTGGGGCTTCACTAAGATTCATTATTCGCATAGAACTAAGAAACCCAGGAAACTGAATCAATAATGACC\nAAATCTATAACTCAATTGTAACTTCTCACGCCTTTATTATAATTTTCTTCATAGTTATGCCATTCATAAT\nCGGAGGTTTCGGAAACTGACTCACACCGTTAATATTAGGAGCGCCCGACATGGCCTTCCCACGAATAAAC\nAATATAAGATTCTGACTTCTACCACCTTCAATTCTTATTATTTTAATAAGAATAGTTATAAATTCAGGAT\nCCGGTACAGGATGAACAGTGTACCCCCCCCTATCTTCCTACGCATTTCACCCATCATCATCCGTAGACCT\nGACAATTTTTTCACTACACATCGCAGGAGTATCATCAATCATAGGAGCAATTAATTTTATTGTCACAATC\nTTAAATATAAAAAATATTTCAATAAATTATGATCAACTACCATTATTCCCATGATCAGTATTCATTACAA\nCAATTCTACTACTAATTTCACTGCCAGTACTAGCTGGGGCCATTACAATATTATTATCAGACCGAAACTT\nANATTCATCATTTTTTGACCCCATGGGGGGCGGAGATCC\n\n&gt;Andrena_proxima\nAATATTATACTTCATCTTCGCTATATGATCCGGAATAATTGGCGCCTCACTAAGATTTATCATCCGTATA\nGAATTAAGAAATCCAGGAAATTGAATCAACAATGATCAAATTTATAATTCTATCGTAACCTCACACGCTT\nTCATTATAATTTTTTTCATAGTAATACCATTTATAATCGGAGGATTCGGAAACTGACTTACACCACTAAT\nATTAGGAGCACCTGATATAGCCTTCCCACGAATAAATAATATAAGATTTTGATTACTCCCTCCATCCATT\nACAATACTTTTAATAAGAACAATCTTAAATTCAGGATCTGGAACAGGATGAACTGTTTATCCTCCTTTAT\nCCTCCTACTCATATCATCCATCATCATCTGTAGATTTAACAATTTTTTCACTTCACATTGCAGGTATCTC\nATCAATTATAGGAGCTATTAACTTCATTGTAACCATCTTAAATATAAAAAATATCTCAATAAATTATGAT\nCAAATACCCCTATTCCCATGATCTGTCTTTATTACAACAATCCTATTATTAATTTCATTACCAGTCCTCG\nLooking good? Let’s make the change!\nsed -e 's/ voucher.*//' -e 's/&gt;.*1 /&gt;/' -e 's/ /_/' data/coi.fas &gt; data/coi_new.fas",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#compiling-data",
    "href": "phylo1.html#compiling-data",
    "title": "13  Retrieving and aligning sequences",
    "section": "",
    "text": "At this point you will need to decide in which format to store your sequence data in. Frustratingly, there is a plethora of file formats, and some pieces of software only support a particular file format. My recommendation is to use the fasta file format for AA and nucleotide sequences and alignments, and to convert this if needed (e.g., using the excellent online tool ALTER). The fasta file format has the big advantage of being very easily readable by human and machines and follows the following format:\n&gt;Sequence_A\nTAGTAGCGATCGACTAAGCTAGCT\n&gt;Sequence_B\nCGACTAAGCTAGCTTAGTAGCGAT\nDescriptions (usually your sequence names) always start with a &gt; and should provide a unique identifier for your sequences. The sequences may or may not contain line breaks:\n&gt;Sequence_A\nTAGTAGCGATCG\nACTAAGCTAGCT\n&gt;Sequence_B\nCGACTAAGCTAG\nCTTAGTAGCGAT\nIMPORTANT: Never use white spaces in fasta decription lines. This will inevitably lead to problems in downstream applications. You can use any text editor to replace white spaces e.g., with underscores by using the search & replace function.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#aligning-sequences",
    "href": "phylo1.html#aligning-sequences",
    "title": "13  Retrieving and aligning sequences",
    "section": "13.2 Aligning sequences",
    "text": "13.2 Aligning sequences\nWe will use the Mafft online server to align our sequences. Upload your fasta file, and let Mafft decide on the optimal alignment strategy (this is the default behaviour; Fig. 3). For more specific applications you may want to check out the other alignment options.\n\n\n\n\n\nFig. 3 | Using the Mafft web server 1. Select your fasta file ; 2. provide email address to retrieve your results later; 3. Click Submit to start alignment\n\n\n\n\nWhen the alignment is done, download the fasta file (Fig. 4) and give it a meaningful name.\n\n\n\n\n\nFig. 4 | Download the aligned sequences from Mafft\n\n\n\n\nAlternatively, you can use the command-line version of Mafft which should already be in your conda environment.\n\nmafft --auto data/coi.fas &gt; data/coi_ali.fas",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#using-an-alignment-editor",
    "href": "phylo1.html#using-an-alignment-editor",
    "title": "13  Retrieving and aligning sequences",
    "section": "13.3 Using an alignment editor",
    "text": "13.3 Using an alignment editor\nYou should always visually check your alignments to make sure that everything looks as expected. Remember, your hypothesis for any aligned position is that all of the characters are homologous. Alignment viewers make it easy to view and manipulate your alignment files. We will use Aliview, a fast and versatile alignment viewer (Fig. 5). It can be started by double-clicking the corresponding file.\n\n\n\n\n\nFig. 5 | Some functions available in aliview illustrated (opening a file, scrolling through the sequences, zooming in and out, aligning, removing alignment positions, renaming a sequence, saving an alignment file). Make sure to check out the entire range of operations of this versatile tool.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#automated-alignment-trimming",
    "href": "phylo1.html#automated-alignment-trimming",
    "title": "13  Retrieving and aligning sequences",
    "section": "13.4 Automated alignment trimming",
    "text": "13.4 Automated alignment trimming\nIn many cases, you will want to trim your alignment to exclude positions that you consider unreliably aligned or positions that contain mostly gaps. This can be done manually in Aliview as we have seen above. However, for a more reproducible workflow it is recommendable to automate this task using clearly defined criteria. We will use trimAl for automated alignment trimming. This programme (as most phylogenetics software) does not have a graphical user interface (GUI), so most be used via the command line. No worries, this is not as difficult as it may seem.\nInstall trimal via conda if you have not done so already, and run\n\ntrimal -in data/coi_ali.fas -out data/coi_trimmed.fas -htmlout data/coi_ali.html -gt .5\n\nLet’s go through this command step by step: trimal is our executable. -in specifies the input file. Note that we have to specify the entire path of the file unless the file is in the same directory as the executable. -out is the name of the output file. Again, we are using the entire path here. -htmlout specifies the path to a useful html file that displays the trimmed characters. -gt .5 specifies how trimming should be performed: we are asking to remove any position in the alignment that has a gap in more than 50% of all sequences. This is only one of multiple trimming criteria trimAl can employ, make sure to have a look at the other ones as well.\nLet’s have a look at the html output file trimAl has generated for us (Fig. 6). What can you observe? Check the alignment again in Aliview before moving on.\n\n\n\n\n\nFig. 6 | Html output of trimAl illustrating the results of the trimming. Positions that were retained are highlighted in grey and positions trimmed are shown with white background",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  }
]