[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods in evolutionary ecology WS25/26",
    "section": "",
    "text": "About\nThis script covers the computational and bioinformatics parts of the module “Methods in Evolutionary Ecology”. We will introduce you to R and BASH, two of the most widely used scripting languages, and make you familiar with navigating in a UNIX environment. These skills are important for any biologist, irrespective of the field you may want to specialise in in the future. Building upon your new knowledge, we will learn how to reconstruct phylogenies from sequencing data, how to work with genomic data, and how to characterise microbiomes. At the end of three weeks computational work, you will tackle a small computational group project, putting your new skills into practise.\nThe script is designed to cover the entire course content. While we will go you through all of the material together in detail during the course, the script should also enable you to work through the content on your own, e.g., to recap after the course has finished and as a reference and starting point for future computational endeavours.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Methods in evolutionary ecology WS25/26",
    "section": "Quarto",
    "text": "Quarto\nThe text is formatted using Quarto, which comes with a number of benefits. It allows us to provide explanations as structured and nicely formatted regular text, and to include code blocks for all computational steps. When compiling Quarto documents, all of the code is run, which means that you not only see the code, but also the outputs it creates.\nHere is an example:\nThis little block of R code generates 100 random coordinates and plots them. The code is shown below, together with the output the code has produced (in this case, a plot).\n\nx &lt;- runif(100)\ny &lt;- runif(100)\nplot(x, y)\n\n\n\n\n\n\n\n\nThe code can conveniently be copied from the block into your own scripts.\nQuarto supports many formats, we here provide the script as a webpage and a printable pdf. Writing Quarto documents is very simple and can be done using RStudio as an editor. The entire script is available for you on github – feel free to download it and modify it with your own comments, notes, and code. We will provide a short introduction to github and Quarto in the course.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#how-to-find-your-way-around",
    "href": "index.html#how-to-find-your-way-around",
    "title": "Methods in evolutionary ecology WS25/26",
    "section": "How to find your way around",
    "text": "How to find your way around\nSimply use the navigation on the left to quickly access the different topics, or flip through the individual pages using the buttons at the bottom of the page. You may wish to download the pdf version of the script (click the pdf icon in the top left) which is ideal for printing. The script is organized by topics, rather than course days, because we will adapt the tempo according to your needs.\n\nPlease note: The script will very likely only be complete at the end of the course. We will still be modifying and correcting it throughtout the three weeks you are with us. So make sure to check out the final version at the end of the course.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "Rintro1.html",
    "href": "Rintro1.html",
    "title": "1  First steps",
    "section": "",
    "text": "1.1 Operators and functions\nR is a statistical programming environment that has become a standard tool in the data and life sciences and many other fields. You may have used R already to run some statistics in a course you took in your studies, and this will be a likely use case for your remaining degree. However, R is much more: it can be used to analyse massive the datasets of the “omics”- age, build webpages, blogs, and interactive apps, and even for art!\nBefore taking full advantage of what the various R packages have to offer, we need to become familiar with its basic structure and commands. It pays off to invest a little effort in practicing the basics, because all R packages use the same syntax – a solid familiarity with base R thus allows you to explore the entire R universe independently.\nR can be used just like an arithmetic calculator. You are familiar with all of the basic syntax already, if you know how to use a calculator!\nSome examples:\n3 + 4  \n\n[1] 7\n\n3 - 4  \n\n[1] -1\n\n3 * 4  \n\n[1] 12\n\n3 / 4  \n\n[1] 0.75\n\n3 ^ 4  # power of\n\n[1] 81\nAs with a regular calculator, there is operator precedence: power &gt; multiplicative operations &gt; additive operations:\n(1 + 2) * 3\n\n[1] 9\n\n2^3 * 3\n\n[1] 24\n\n2^(3 * 3)\n\n[1] 512\nSquare roots, exponentials, and logarithms also work just as with a calculator:\nsqrt(9) \n\n[1] 3\n\nexp(3) \n\n[1] 20.08554\n\nlog(3) \n\n[1] 1.098612\n\nlog(exp(3)) # natural logarithm\n\n[1] 3\n\nlog10(100)  # logarithm to base 10\n\n[1] 2\nIn order to “save” a value for use later on, you have to assign it to a variable! &lt;- is the assignment operator you need to use for this (handy shortcut in RStudio is ALT + -).\nx &lt;- 3 + 4\nCalling the variable will then print the result to the R console, and can be used in other calculations.\nx\n\n[1] 7\n\nx + 10\n\n[1] 17\nYou can call your variables whatever you want, but be careful: R will overwrite any variable if you tell it to, without a warning! You should also avoid giving your variables names that are already assigned to functions.\nmy_favourite_variable &lt;- 100\nmy_favourite_variable\n\n[1] 100\nAll variables (among other things) are visible in the environment panel in RStudio (default: top right part of the screen).",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Rintro1.html#operators-and-functions",
    "href": "Rintro1.html#operators-and-functions",
    "title": "1  First steps",
    "section": "",
    "text": "“=” can also be used to assign variables but is discouraged, because the direction of the assignment is not immediately obvious. It is best practise to always start with the variable, followed by the assignment operator",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Rintro1.html#data-types",
    "href": "Rintro1.html#data-types",
    "title": "1  First steps",
    "section": "1.2 Data types",
    "text": "1.2 Data types\nYou need to be familiar with at least three important data types in R: logical, numeric, and character. Data being stored in a different data type than required is one of the most frequent error messages you will encounter as an R beginner.\nlogical simply means true or false. R also understands the abbreviations T and F. To determine which types you data is in, you can use mode or class.\n\nvar1 &lt;- FALSE \nmode(var1)\n\n[1] \"logical\"\n\n\nnumeric means numbers\n\nvar2 &lt;- 10\nclass(10)\n\n[1] \"numeric\"\n\n\nA character is any form of text, a so called “string”. It must always be surrounded by quotation marks!\n\nvar3 &lt;- \"A so called string\"\nmode(var3)\n\n[1] \"character\"\n\n\nIf in doubt, R will often convert or read in data as characters. Watch out for some common errors!\n\nvar4 &lt;- \"5\"\nvar4\n\n[1] \"5\"\n\nis.numeric(var4)\n\n[1] FALSE\n\nvar5 &lt;- \"TRUE\"\nvar5\n\n[1] \"TRUE\"\n\nis.logical(var5)\n\n[1] FALSE\n\n\nYou can convert between types easily!\n\nvar6 &lt;- as.numeric(var4)\nvar6\n\n[1] 5\n\nclass(var6)\n\n[1] \"numeric\"",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Rintro1.html#exercises",
    "href": "Rintro1.html#exercises",
    "title": "1  First steps",
    "section": "Exercises",
    "text": "Exercises\n\nSum the values of 1 to 5\n\n\nsum(1:5)\n\n[1] 15\n\n1 + 2 + 3 + 4 + 5\n\n[1] 15\n\n\n\nCreate a variable v1 and assign it a character value\n\n\nv1 &lt;- \"text\"\n\n\nCopy variable v1 to v2\n\n\nv2 &lt;- v1\n\n\nCompare the value of v1 against v2\n\n\nv1 == v2\n\n[1] TRUE\n\nidentical(v1, v2)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nTipTip\n\n\n\nCompare values and variables using the following operators\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nequals\n\n\n!=\nnot equal\n\n\n\nPlease note, = and == do very different things! Don’t mix them up.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "Rintro2.html",
    "href": "Rintro2.html",
    "title": "2  Data structures",
    "section": "",
    "text": "2.1 Vectors\nSo far we’ve only looked at simple variables consisting of a single value or character. Typically, your data will be more complex. In R, there are three structures relevant for the data you will be working with.\nA vector is a number of elements of the same data type (logical, numeric, character). It can be generated by concatenating the elements using the function c.\nvec1 &lt;- c(T, F, T, F)\nvec1\n\n[1]  TRUE FALSE  TRUE FALSE\n\nmode(vec1)\n\n[1] \"logical\"\n\nvec2 &lt;- c(1, 2, 3, 4, 5)\nvec2\n\n[1] 1 2 3 4 5\n\nmode(vec2)\n\n[1] \"numeric\"\n\nvec3 &lt;- c(\"Spring\", \"Summer\", \"Autumn\", \"Winter\")\nvec3\n\n[1] \"Spring\" \"Summer\" \"Autumn\" \"Winter\"\n\nmode(vec3)\n\n[1] \"character\"\nOther ways to generate vectors are rep and seq. rep is used to repeat any number of elements any number of times.\nrep(5, 10)\n\n [1] 5 5 5 5 5 5 5 5 5 5\n\nrep(vec3, 5)\n\n [1] \"Spring\" \"Summer\" \"Autumn\" \"Winter\" \"Spring\" \"Summer\" \"Autumn\" \"Winter\"\n [9] \"Spring\" \"Summer\" \"Autumn\" \"Winter\" \"Spring\" \"Summer\" \"Autumn\" \"Winter\"\n[17] \"Spring\" \"Summer\" \"Autumn\" \"Winter\"\nseq can be used to create numerical sequences.\nseq(from = 0, to = 100, by = 5)\n\n [1]   0   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90\n[20]  95 100\nThe command above is easy to read and understand for humans, which is good. R will also understand if you specify it as\nseq(0, 100, 5)\n\n [1]   0   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90\n[20]  95 100\nAs a shortcut for a common sequences, you can use\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\nAs mentioned above,, vectors can only combine elements of a single data type. Combining multiple different data types may result in some unwanted behaviour.\nvec_mix1 &lt;- c(5, TRUE, 65)\nmode(vec_mix1)\n\n[1] \"numeric\"\n\nvec_mix2 &lt;- c(\"blue\", TRUE, \"red\")\nmode(vec_mix2)\n\n[1] \"character\"\nIn many cases you may wish to access a single element of a vector. You can do so using square brackets.\nz &lt;- c(\"order\", \"family\", \"genus\", \"species\")\nz[2]\n\n[1] \"family\"\nSimilarly, you can access any combination of elements from the vector.\nz[1:2]\n\n[1] \"order\"  \"family\"\n\ni &lt;- c(1, 3)\nz[i]\n\n[1] \"order\" \"genus\"\n\nz[c(1, 1, 1, 4)]\n\n[1] \"order\"   \"order\"   \"order\"   \"species\"\n\nz[-1]\n\n[1] \"family\"  \"genus\"   \"species\"\nThe square brackets are also used if you need to change elements of the vector. Changes are made using the assignment operator which you already know.\nx &lt;- 1:5\nx\n\n[1] 1 2 3 4 5\n\nx[c(1, 4)] &lt;- 10\nx\n\n[1] 10  2  3 10  5\nWhich elements of a vector have certain characteristics? This is important for filtering/selecting in your dataset. You can combine different queries using logical operators.\nx &gt;= 5\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\nx[x &gt;= 5]\n\n[1] 10 10  5\n\nwhich(x &gt;= 5)\n\n[1] 1 4 5\n\nz\n\n[1] \"order\"   \"family\"  \"genus\"   \"species\"\n\nwhich(z == \"genus\")\n\n[1] 3\n\nz[z== \"genus\"]\n\n[1] \"genus\"\n\nz[z != \"genus\"]\n\n[1] \"order\"   \"family\"  \"species\"\n\nwhich(z== \"genus\" | z == \"order\")\n\n[1] 1 3\nLogical operators in R\nConveniently, the elements of a vector can be named and accessed using the names. Let’s first create a vector…\ndmel &lt;- c(\"Hexapoda\", \"Diptera\", \"Drosophilidae\", \"Drosophila\", \"Drosophila melanogaster\")\ndmel\n\n[1] \"Hexapoda\"                \"Diptera\"                \n[3] \"Drosophilidae\"           \"Drosophila\"             \n[5] \"Drosophila melanogaster\"\n… and then add names for each element\nnames(dmel) &lt;- c(\"Class\", \"Order\", \"Family\", \"Genus\", \"Species\")\ndmel\n\n                    Class                     Order                    Family \n               \"Hexapoda\"                 \"Diptera\"           \"Drosophilidae\" \n                    Genus                   Species \n             \"Drosophila\" \"Drosophila melanogaster\" \n\nstr(dmel)\n\n Named chr [1:5] \"Hexapoda\" \"Diptera\" \"Drosophilidae\" \"Drosophila\" ...\n - attr(*, \"names\")= chr [1:5] \"Class\" \"Order\" \"Family\" \"Genus\" ...\nNow we can use the names to access the values\ndmel[c(\"Class\", \"Species\")]\n\n                    Class                   Species \n               \"Hexapoda\" \"Drosophila melanogaster\" \n\ndmel[names(dmel) == \"Order\"]\n\n    Order \n\"Diptera\"",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Rintro2.html#vectors",
    "href": "Rintro2.html#vectors",
    "title": "2  Data structures",
    "section": "",
    "text": "|\nOR\n\n\n&\nAND\n\n\n!\nNOT\n\n\n\n\n\n\n\n\n\n\nExercise\n\nCreate a vector consecutively numbering all days of the year 2026. Assign the correct weekday names for all elements of the vector.\nUse the vector to determine how many days in 2026 are weekend days.\n\n\n\n\n\n\n\nTipTip\n\n\n\nIf you struggle to assign the correct names, have a look at the help for rep.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Rintro2.html#matrices",
    "href": "Rintro2.html#matrices",
    "title": "2  Data structures",
    "section": "2.2 Matrices",
    "text": "2.2 Matrices\nA matrix in R can be thought of as a two-dimensional vector. All elements must be of the same data type. There are various ways to create a matrix. For example, one can use the matrix function like this.\n\nmat1 &lt;- matrix(data = 1:12, nrow = 3, ncol = 4, byrow=T) \nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\nAlternatively, a vector can be transformed into a matrix\n\nmat2 &lt;- 1:12\ndim(mat2) &lt;- c(3, 4)\nmat2\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nOften you will want to combine multiple vectors into a matrix\n\ndmel &lt;- c(\"Hexapoda\", \"Diptera\", \"Drosophilidae\", \"Drosophila\", \"Drosophila melanogaster\")\ndhyd &lt;- c(\"Hexapoda\", \"Diptera\", \"Drosophilidae\", \"Drosophila\", \"Drosophila hydei\")\nmat3 &lt;- cbind(dmel, dhyd)\nmat3\n\n     dmel                      dhyd              \n[1,] \"Hexapoda\"                \"Hexapoda\"        \n[2,] \"Diptera\"                 \"Diptera\"         \n[3,] \"Drosophilidae\"           \"Drosophilidae\"   \n[4,] \"Drosophila\"              \"Drosophila\"      \n[5,] \"Drosophila melanogaster\" \"Drosophila hydei\"\n\nmat4 &lt;- rbind(dmel, dhyd)\nmat4\n\n     [,1]       [,2]      [,3]            [,4]        \ndmel \"Hexapoda\" \"Diptera\" \"Drosophilidae\" \"Drosophila\"\ndhyd \"Hexapoda\" \"Diptera\" \"Drosophilidae\" \"Drosophila\"\n     [,5]                     \ndmel \"Drosophila melanogaster\"\ndhyd \"Drosophila hydei\"       \n\n\nJust like vectors, matrix elements can have names\n\nmat3\n\n     dmel                      dhyd              \n[1,] \"Hexapoda\"                \"Hexapoda\"        \n[2,] \"Diptera\"                 \"Diptera\"         \n[3,] \"Drosophilidae\"           \"Drosophilidae\"   \n[4,] \"Drosophila\"              \"Drosophila\"      \n[5,] \"Drosophila melanogaster\" \"Drosophila hydei\"\n\ncolnames(mat3)\n\n[1] \"dmel\" \"dhyd\"\n\nrownames(mat3) &lt;- c(\"Class\", \"Order\", \"Family\", \"Genus\", \"Species\")\nmat3\n\n        dmel                      dhyd              \nClass   \"Hexapoda\"                \"Hexapoda\"        \nOrder   \"Diptera\"                 \"Diptera\"         \nFamily  \"Drosophilidae\"           \"Drosophilidae\"   \nGenus   \"Drosophila\"              \"Drosophila\"      \nSpecies \"Drosophila melanogaster\" \"Drosophila hydei\"\n\n\nAnd just like with vectors, we can use square brackets to access and replace values. Because there are 2 dimensions, we need to provide 2 values (one for rows, one for columns, separated by ,).\n\nmat3\n\n        dmel                      dhyd              \nClass   \"Hexapoda\"                \"Hexapoda\"        \nOrder   \"Diptera\"                 \"Diptera\"         \nFamily  \"Drosophilidae\"           \"Drosophilidae\"   \nGenus   \"Drosophila\"              \"Drosophila\"      \nSpecies \"Drosophila melanogaster\" \"Drosophila hydei\"\n\nmat3[1:3, 2]\n\n          Class           Order          Family \n     \"Hexapoda\"       \"Diptera\" \"Drosophilidae\" \n\nmat3[1:3, ]\n\n       dmel            dhyd           \nClass  \"Hexapoda\"      \"Hexapoda\"     \nOrder  \"Diptera\"       \"Diptera\"      \nFamily \"Drosophilidae\" \"Drosophilidae\"\n\nmat3[c(\"Class\", \"Species\"), ]\n\n        dmel                      dhyd              \nClass   \"Hexapoda\"                \"Hexapoda\"        \nSpecies \"Drosophila melanogaster\" \"Drosophila hydei\"\n\n\n\nExercise\n\ncreate a matrix using with 20 rows & 5 columns, using 100 randomly generated numbers between 0 and 1000.\n\n\nrandom_numbers &lt;- runif(100, 0, 1000)\n\n\nmatrix(data = runif(100, 0, 1000), nrow = 20)\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n [1,] 638.6522  33.74063 122.13616 447.33581  23.25899\n [2,] 844.7673 537.84504 447.47162 709.27848 885.34658\n [3,] 952.1572 474.68069 510.41541 769.09841 992.26852\n [4,] 797.9956 436.02998 232.14608  76.69306 760.42885\n [5,] 442.8975 583.08694 669.40288 291.18211 919.01050\n [6,] 400.9008 168.59381 146.23410 266.17123 117.03639\n [7,] 303.0210 638.28677 774.90838  27.39931 817.34740\n [8,] 303.1231 966.60944  32.94279  21.51523 822.37577\n [9,] 447.2747 118.68902 495.24338 227.67429 152.10488\n[10,] 736.2648 324.26578 290.81179  79.37585 452.72538\n[11,] 502.0346 892.49835 924.35381  81.09021  52.28820\n[12,] 807.5650  53.48549 762.03706 723.47301  95.25455\n[13,] 988.3546 993.53251 285.35150 167.64932 701.22103\n[14,] 790.3240 679.13273 117.26354 102.34102 560.41459\n[15,] 815.0160 235.28150 463.39223 888.94595 248.59597\n[16,] 313.2495  19.05742 320.24769 756.93409  82.50069\n[17,] 987.0678 607.71442 927.96476 977.01673 571.52219\n[18,] 564.6298 422.16989 872.24325 759.62398 979.16268\n[19,] 448.4858 730.16447  31.58054 199.81537 728.40608\n[20,] 502.6501 246.01421 744.65265 620.26683 108.87483\n\n\n\nreplace all values in the 3rd column of this matrix that are larger than 500 with NA.\n\n\n\n\n\n\n\nTipTip\n\n\n\nuse the function runif to create random values",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Rintro2.html#data-frames",
    "href": "Rintro2.html#data-frames",
    "title": "2  Data structures",
    "section": "2.3 Data frames",
    "text": "2.3 Data frames\nData frames are the R equivalent of spread sheets. Like matrices, they are two-dimensional, however they may combine different data types. Most biological data sets you will encounter will be data frames.\nLets create a data frame\n\n# create some data\nspecies &lt;- rep(c(\"beech\",\"ash\",\"elm\",\"maple\", \"sycamore\"),40)\nspecies\n\n  [1] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n  [7] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [13] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [19] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [25] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [31] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [37] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [43] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [49] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [55] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [61] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [67] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [73] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [79] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [85] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [91] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [97] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[103] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[109] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[115] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[121] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[127] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[133] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[139] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[145] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[151] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[157] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[163] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[169] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[175] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[181] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[187] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[193] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[199] \"maple\"    \"sycamore\"\n\ndbh &lt;- runif(200, 5, 40)\ndbh\n\n  [1] 24.923825 30.778743  5.764232 32.295454 27.830735 19.950178 12.550854\n  [8] 20.252888  7.654959 13.090884 15.555623 16.240694  9.428703 37.937806\n [15]  5.958372 20.797036 25.995522 19.037807 27.232429 22.692962 10.495536\n [22] 20.408314 17.434626 29.589453 37.676322 17.813333 27.784113 33.992126\n [29] 36.311495 10.161297  6.963598 14.857425 26.458226 11.749829 28.556770\n [36] 17.294162 27.804099 22.068805 12.027200  8.072665 15.043750 22.510892\n [43] 11.582426 29.965348 32.249774 15.018579 38.825020 12.589945 36.587962\n [50]  7.797180 18.369931  8.963797 16.886608 36.237748  7.943704 15.029483\n [57] 37.314120 36.365618 22.048187 36.991575 36.412374  7.752207 12.293671\n [64] 18.364065 29.476939 11.186639 33.553685 25.306601 13.172983 29.412262\n [71] 27.510629 27.115535  7.651663 39.653167 35.384564 13.899059 21.590723\n [78] 32.333851 16.671656 30.187885 26.082495 33.814840 30.338756 15.668131\n [85] 37.672203  7.446070  8.393343 29.302390 23.123149 36.755829 11.006801\n [92] 13.818596 28.101935 36.447180 12.704100 31.494691 10.233431 15.791248\n [99] 34.022420 26.403130 34.598052 14.896695 13.319902 14.607418 14.679570\n[106] 29.848659 16.133930 32.806094 37.616608 27.268523 32.330347 28.148141\n[113] 16.527536 23.706787 15.636322 18.995048 21.776409 28.055160 22.330805\n[120] 17.381572 19.254668 20.044230  6.338795 31.231273 17.700134  9.184537\n[127] 22.505835 19.436568 32.666031 21.418892 19.435278 39.400857 35.164471\n[134] 23.418072 24.372431 15.610896 11.475910 36.494055 22.763125 31.236183\n[141]  7.459208 18.950242 15.698991 12.205773 28.978627  9.051138 38.393668\n[148] 14.227589 35.617194  6.574650 10.956367 35.680654 33.198203 29.891207\n[155] 18.531772  6.611867 34.494316 28.694996 19.387543 16.839497  7.694954\n[162] 20.012845 11.724029 16.394743  9.732983 24.836126 22.887118 34.928290\n[169] 14.692355 21.065133  9.062307 39.941234 13.185746 28.313048 23.070139\n[176] 27.812572 26.114921 20.164905 27.093640 14.910135 18.104358 36.360676\n[183] 18.164104 21.803294 32.790255  8.666658 22.421837 19.710681 14.546880\n[190] 36.180798 28.231436 17.401257 15.696557 16.079886 18.836148 35.045036\n[197] 39.690714 20.494956 32.879786 31.277316\n\nage &lt;- as.integer(runif(200, 20, 120))\nage\n\n  [1]  84  59  73  62  32 115  46  39  95  48 113  26  54  32  84 114  23  88\n [19] 104  91 100  35 112  48 101  88 119  46  23  78  71 119  58  50  67 112\n [37]  71 114 100  64  22  86  63  36  70  66  81 110 114  35  76  54  32  32\n [55]  62  59 117  26  85  99  21  39  38  64  27 107 119  83 100  92  49  84\n [73]  79  83  24  70  88 116  25  80  25  59  47  34  97 101  91  32  27  79\n [91]  36  66  64  87  35  24  48  85  87  62  20  47 103  72  66  59  32  57\n[109]  86  69  98 102  36  56 114 106  48  55 107  34  40  80  90  59  47  77\n[127] 106  32  65 102  40  55 113  58 117  89  28  41  62 108  47  48  70  81\n[145] 119  86 117  65  64  81 115  91 100 103  89  84 107  58  92  43  56  96\n[163]  60  61  75  65  21  38  27  62  79  34  63 119  97  62  22  39 107  57\n[181] 117  51  36  29 114 105  35  51  68  67  24 115 104  52  82 115  48 102\n[199]  94  44\n\ndf1 &lt;- data.frame(species, dbh, age)\ndf1\n\n\n  \n\n\n\nTo access values, we can use the same approaches as for matrices:\n\ndf1[1:12, 1:2]\n\n\n  \n\n\n\nbut can also access and filter the columns directly using their names like this:\n\ndf1$species\n\n  [1] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n  [7] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [13] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [19] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [25] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [31] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [37] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [43] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [49] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [55] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [61] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [67] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n [73] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n [79] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n [85] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n [91] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n [97] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[103] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[109] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[115] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[121] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[127] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[133] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[139] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[145] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[151] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[157] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[163] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[169] \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"   \n[175] \"sycamore\" \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\"\n[181] \"beech\"    \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"   \n[187] \"ash\"      \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"     \n[193] \"elm\"      \"maple\"    \"sycamore\" \"beech\"    \"ash\"      \"elm\"     \n[199] \"maple\"    \"sycamore\"\n\ndf1[df1$dbh &gt; 15, ]\n\n\n  \n\n\n\n\nExercise\n\nUsing df1, select only entries corresponding to ash and maple with an age over 50 and a diameter less than 30.\nAdd a new column to the dataframe called “year”. Generate data for this column so that there are 10 different years and the same number of entries for each tree species per year.\n\n\ndf1\n\n\n  \n\n\nyear &lt;- rep(2015:2024, 20)\n\ndf1[,4] &lt;- year\n\ncolnames(df1)[4] &lt;- \"year\"\n\n\ndf1\n\n\n  \n\n\n\n\n\n\n\n\n\nTipTip\n\n\n\nUse the function rep for this exercise",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "Rintro3.html",
    "href": "Rintro3.html",
    "title": "3  Data, packages, and some more functions",
    "section": "",
    "text": "3.1 Setting up your working environment\nUsually when working in R, you want to look at your own data, and not generate it from random distributions. To read in a data file, we first need to tell R where the working directory is located.\nsetwd(\"/home/of22haqi/Documents/TEACHING/MEE-WS25-26/data/\")\nThe path will look different on your machine of course.\nNow that R knows where to find it, we are ready to read in a data file.\n# The table contains headers, and the fields are separated by commas\nbe &lt;-  read.table(\"data/butterfly_ecology.csv\", header = TRUE, sep = \",\")\n\n# Let's have a glimpse at the data \nhead(be)\nIn order to save your entire working environment, so you don’t have to re-run potentially time intensive pieces of your code, just save it and load it back into your work space the next time you use R.\nsave.image(\"myenv.Rdata\")\nYou can also use the panel “Environment” in RStudio to save and load your data.\nAll of the functions we used today are so “base R” functions, which means they come preinstalled with R. Lots of the functionality of R is in external packages which need to be installed manually. The majority of relevant packages are found on CRAN (The Comprehensive R Archive Network), and there is a special archive for packages relevant to the life sciences (Bioconductor). In order to install packages from CRAN, simply run\ninstall.packages(\"tidyverse\", dependencies = TRUE)\nHere, tidyverse is the package we want to install we ask to also install any packages that tidyverse may require to function. You can find a list of all packages currently installed in the packages tab in the panel on the bottom left in RStudio. It is good practise to keep the packages, as well as your R installation up to date.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#setting-up-your-working-environment",
    "href": "Rintro3.html#setting-up-your-working-environment",
    "title": "3  Data, packages, and some more functions",
    "section": "",
    "text": "Use a text editor outside of Rstudio to look at the data file as well. Why do you think this is a good format to store data in? WHat is the advantage to e.g., an Excel file? What does using a text file format mean for your data entry requirements?",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#functions",
    "href": "Rintro3.html#functions",
    "title": "3  Data, packages, and some more functions",
    "section": "3.2 Functions",
    "text": "3.2 Functions\nWe have already used plenty of functions. Most of them require at least an object an which to perform the function on, and may also have some options. For example, consider the following function:\n\nmean(be$range.size, na.rm = TRUE)\n\n[1] 261.4116\n\n\nmean is the function, be$range.size is the object (1 vector from the dataframe we just read into R) and na.rm = TRUE is the option to remove NAs from the vector before calculating the mean.\nIn some cases, you may want to do things to your data that cannot be addressed by a single function. In this case, you may have to perform a number of different operations on the dataset. If you are likely to use the same set of operations in the future, it may be advisable to use your own functions.\nA very simple example. Let’s assume the mean function didn’t exist and we would need to write our own.\n\nmean2 &lt;- function(x){\n  x &lt;- na.omit(x)\n  sum(x) / length(x) \n}\n\nmean2(be$range.size)\n\n[1] 261.4116\n\n\nWe define mean2 as a function that requires an object (here called x as an input). Looking into the function, we can see that it first removes the NAs from the object and next calculates the sum of x divided by the number of elements of x (this is how the mean is defined). Testing it, we can see that it gives the same result as the native mean function.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#loops",
    "href": "Rintro3.html#loops",
    "title": "3  Data, packages, and some more functions",
    "section": "3.3 Loops",
    "text": "3.3 Loops\nIn many cases, we need to apply a function t a number of elements. In this case, loops come in handy. In the simple examples below, the structure of a for loop is illustrated.\n\nfor(i in 1:10) # how often is the loop repeated\n{\n   print(i)    # what is to be done each iteration\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\nj&lt;-0\nfor(i in 1:5)\n{\n   j&lt;-i+j\n   print(j)\n}\n\n[1] 1\n[1] 3\n[1] 6\n[1] 10\n[1] 15\n\n\nObserve and try to explain what happens in each iteration to the variables used in these examples.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#plots",
    "href": "Rintro3.html#plots",
    "title": "3  Data, packages, and some more functions",
    "section": "3.4 Plots",
    "text": "3.4 Plots\nFor many use cases ggplot2 is the best approach of plotting, and we will get to know this package later. However, for very simple and quick plots, base R plotting functions are sufficient and superior to othe options because of simplicity and speed.\nScatter plots can be created by just naming the variables to be plotted against each other.\n\nplot(be$WSP_Female_average, be$ALT_Range)\n\n\n\n\n\n\n\n\nHistograms showing frequency distributions are also very easily generated\n\nhist(be$WSP_Female_average)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "Rintro3.html#exercises",
    "href": "Rintro3.html#exercises",
    "title": "3  Data, packages, and some more functions",
    "section": "Exercises",
    "text": "Exercises\n\nUsing a loop, plot histograms for the columns “WSP_Female_average”, “Alt_Range”, “Alt_min”, and “range.size”.\nWrite a function that creates these plots with only the dataframe as argument.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data, packages, and some more functions</span>"
    ]
  },
  {
    "objectID": "tidyverse.html",
    "href": "tidyverse.html",
    "title": "4  Tidyverse",
    "section": "",
    "text": "4.1 What is the tidyverse?\nWe will only be looking at a couple of functions from a 2 packages (dplyr & ggplot2). All functions are about data manipulation and visualisation and are especially well suited for exploring very large data sets.\nYou can install all tidyverse packages by running\ninstall.packages(\"tidyverse\", dependencies = TRUE)\n(Remember, you can just add the answers into this document for future reference!)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#what-is-the-tidyverse",
    "href": "tidyverse.html#what-is-the-tidyverse",
    "title": "4  Tidyverse",
    "section": "",
    "text": "A collection of R packages for data science\nAll packages share a “philosophy” about design and data structure\nAll packages are highly compatible and functions complement each other\n\n\n\n\n\nLet’s refresh what we learned earlier this week:\n\nWhat different types of data structures are used in R?\nWhich of these do you think is most likely to be used in the tidyverse?",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#our-data-set-for-today",
    "href": "tidyverse.html#our-data-set-for-today",
    "title": "4  Tidyverse",
    "section": "4.2 Our data set for today",
    "text": "4.2 Our data set for today\nWe will be looking at a data set of ecological traits of european butterflies. Download the table and read it into R.\n\n# The table contains headers, and the fields are separated by commas\nbe &lt;-  read.table(\"data/butterfly_ecology.csv\", header = TRUE, sep = \",\")\n\n# Let's have a glimpse at the data using head\nhead(be)\n\n\n  \n\n\n\nEach of the rows contains data for 1 European species, and the columns contain the following information:\n\n\n\n\n\n\n\n\n\nTrait abbreviation\nMeaning\nStates\nNotes\n\n\n\n\nOWS\nOverwintering stage\negg, larvae, pupae, adult\n\n\n\nGEN\nGenerations\naverage, min, max, range\n\n\n\nWSP\nWingspan\naverage, range\nMeasured in mm\n\n\nHSI\nHostplant index\nN/A\nMeasured from 0-1\n\n\nLEV\nLarval environment\nburied, ground layer, field layer, shrub layer, canopy layer\n\n\n\nELT\nEgg laying type\nsingle, small batch, large batch\n\n\n\nALT\nAltitude\nmin, range\n\n\n\nFM\nFlight months\naverage, range\n\n\n\nAFB\nAdult feeding behaviour\nherb flower, grass, shrub flower, honeydew, sap, animal, mineral\n\n\n\n\nNow that we are familiar with the dataset, lets look at some tidyverse functions.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#filter-for-filtering-data-frames",
    "href": "tidyverse.html#filter-for-filtering-data-frames",
    "title": "4  Tidyverse",
    "section": "4.3 filter() for filtering data frames",
    "text": "4.3 filter() for filtering data frames\nAs the name suggests, this is used to filter data frames, with a simple and efficient syntax:\n\n# first, we have to load the tidyverse packages\nlibrary(tidyverse, quietly = TRUE)\n\n# the command always takes a dataframe as first argument, and a filtering criterion as second argument\n# Here, we only consider butterflies that overwinter as eggs\nfilter(be, OWS_egg == 1)\n\n\n  \n\n\n\nThe filtering criterion can be specified using the methods you are already familiar with (e.g., &gt;, &gt;=, !=, %in%).\nNotice that the variable names can be used directly here, so instead of using be$OWS_egg, filter() lets you use OWS_egg directly. All tidyverse functions work like that. Let’s look at more complex filtering:\n\n# combine 2 filters with boolean \"AND\" ...\nfilter(be, OWS_egg == 1 , ALT_Min &gt; 500)\n\n\n  \n\n\n# ... or boolean \"OR\"\nfilter(be, OWS_egg == 1 | AFB_honeydew == 1)\n\n\n  \n\n\n\n\nNOTE\nfilter() (and many other tidyverse functions) return a data frame. In the tidyverse, these are called tibble() and behave slightly different to regular data frames. For our purposes however, these differences are not important.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#the-pipe-for-combining-commands",
    "href": "tidyverse.html#the-pipe-for-combining-commands",
    "title": "4  Tidyverse",
    "section": "4.4 The pipe %>% for combining commands",
    "text": "4.4 The pipe %&gt;% for combining commands\nThe filtering using filter() is very useful, but you can see that the commands can become very long when you have many filters. Also, trying out many different filters to see what they do with the data can be cumbersome. This where %&gt;% comes in really handy.\nThe “pipe” %&gt;% (keyboard shortcut: Ctrl+Shift+M) simply passes the result of one function to the next function. For the next function, one does not have to specify the data frame. Let’s see an example.\n\n# this is how we filtered our data frame earlier \nfilter(be, OWS_egg == 1)\n\n\n  \n\n\n# same command, this time using the pipe\nbe %&gt;%  \n  filter(OWS_egg == 1)\n\n\n  \n\n\n\nNote how in the second command, the output of be (which is our data frame) gets passed on to the filter() command. There, you don’t have to specify the name of the data frame again. The result of this can be piped further to other commands:\n\n# Multiple filters are connected by pipes\nbe %&gt;% \n  filter(OWS_egg == 1) %&gt;% \n  filter(LEV_ground_layer == 1) %&gt;% \n  filter(AFB_honeydew == 1)\n\n\n  \n\n\n# As always in R, assign the result to a new variable using \"&lt;-\" \nbe_filtered &lt;- be %&gt;% \n  filter(OWS_egg == 1) %&gt;% \n  filter(LEV_ground_layer == 1) %&gt;% \n  filter(AFB_honeydew == 1)\n\nNote how easy this command is to read (you could write it in a single line, but it’s much easier to follow with line breaks)! The usefulness of the pipe will become more obvious when we combine multiple different commands. In all the following examples, I will always use the pipe.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#sort-by-column-with-arrange",
    "href": "tidyverse.html#sort-by-column-with-arrange",
    "title": "4  Tidyverse",
    "section": "4.5 Sort by column with arrange()",
    "text": "4.5 Sort by column with arrange()\nThis doesn’t change the dataframe itself, it simply orders the columns (similar to the sort function in Excel):\n\n# sort by age (ascending) and weight (descending)\nbe %&gt;% \n  arrange(conserv.eu, -range.size)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#select-columns-with-select",
    "href": "tidyverse.html#select-columns-with-select",
    "title": "4  Tidyverse",
    "section": "4.6 Select columns with select()",
    "text": "4.6 Select columns with select()\n\n# choose which columns to keep\nbe %&gt;% \n  select(species, range.size, conserv.eu, FM_Average, WSP_Female_average)\n\n\n  \n\n\n# or specify which columns to remove\nbe %&gt;% \n  select(-(OWS_egg:OWS_adult))\n\n\n  \n\n\n# contains is another useful command to select columns. \nbe %&gt;% \n  select(species, contains(\"LEV\")) %&gt;% \n  drop_na()\n\n\n  \n\n\n\nFuntions like contains can be powerful for filtering and selecting. starts_with and ends_with work just the same way and are equally useful.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#create-new-variables-with-mutate",
    "href": "tidyverse.html#create-new-variables-with-mutate",
    "title": "4  Tidyverse",
    "section": "4.7 Create new variables with mutate()",
    "text": "4.7 Create new variables with mutate()\nThis is a very powerful and flexible function that uses existing variables to create novel ones. Let’s look at a simple example\n\n# Create a new variable summarizing all the overwintering stages that are not adults\nbe %&gt;% \n  mutate(OWS_juvenile = 1-OWS_adult) %&gt;% \n  select(OWS_juvenile, OWS_adult) %&gt;% \n  drop_na()\n\n\n  \n\n\n# Determine how different the protection levels between EU and Europe and extract the species for which the difference is striking\nbe %&gt;% \n  mutate(protect_diff = abs(conserv.europe - conserv.eu)) %&gt;% \n  filter(protect_diff &gt; 2)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#exercise",
    "href": "tidyverse.html#exercise",
    "title": "4  Tidyverse",
    "section": "Exercise",
    "text": "Exercise\n\nFrom our dataset, remove all butterflies with average wingspans larger than 60mm and smaller than 30mm. Only keep the species that have a conservation classification on the EU level. Only keep the species names and all variables associated with adult feeding, and store this in a new data frame. How many rows and columns does the new data frame have?\n\n\nnew_be &lt;- be %&gt;% \n  filter(WSP_Female_average &lt; 60) %&gt;% # filter1 \n  filter(WSP_Female_average &gt; 30) %&gt;% \n  drop_na(conserv.eu) %&gt;% \n  select(species, starts_with(\"AFB\")) \n\nnew_be\n\n\n  \n\n\n\n\nRe-calculate the generation range from the provided minima and maxima. Check if your calculations match the original range values given in the data.\n\n\nbe %&gt;% \n  mutate(GEN_Range2 = GEN_Max - GEN_Min) %&gt;% \n  select(species, GEN_Max, GEN_Min, GEN_Range2, GEN_Range) %&gt;% \n  mutate(GEN_compare = GEN_Range2 - GEN_Range)",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#group_by-and-summarise-as-powerful-data-exploration-tools",
    "href": "tidyverse.html#group_by-and-summarise-as-powerful-data-exploration-tools",
    "title": "4  Tidyverse",
    "section": "4.8 group_by() and summarise() as powerful data exploration tools",
    "text": "4.8 group_by() and summarise() as powerful data exploration tools\nAlthough dplyr has a simpler syntax overall, everything we have looked at so far could have been done fairly easily with base R functions: data frame filtering, sorting, and adding and removing columns. One of the strengths of dplyr is explorative data analysis, and this is where group_by() and summarize() are really helpful. We’ll only look at very simple examples today.\nWhen browsing through the complete data table, it is very hard to recognize any patterns. Let’s assume we wanted to compare the average wing span of butterflies with that overwinter as adults vs all other butterflies:\n\n# Are butterflies that overwinter as adults larger than other species?\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(OWS_adult) %&gt;% \n  summarise(mean_wsp = mean(WSP_Female_average))\n\n\n  \n\n\n\nAfter choosing which variable to group by (here: OWS_adult), summarise() then calculates a function for each group. In our simple example, there are 2 groups: 0 (not overwintering as adult) and 1 (overwintering as adult); and the function to be calculated is the mean of the female wing span. This is a very flexible set of functions, because you can group by multiple groups and also use summarise() with many different functions (e.g., mean(), sum(), min(), max(), median() – just to name a few). Let’s look at a more complex example:\n\n# Let's add another group. How large is the standard deviation? How large is each group?\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(OWS_adult, LEV_ground_layer) %&gt;% \n  summarise(mean_wsp = mean(WSP_Female_average),\n            sd = sd(WSP_Female_average),\n            group_size=n())\n\n`summarise()` has grouped output by 'OWS_adult'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\nbe %&gt;% \n  mutate(alt_bins = case_when(ALT_Min &gt; 500 ~ \"high\",\n                              ALT_Min &lt;= 500 ~ \"low\"))",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "tidyverse.html#more-exercises",
    "href": "tidyverse.html#more-exercises",
    "title": "4  Tidyverse",
    "section": "More exercises",
    "text": "More exercises\nUsing dplyr functions, determine\n\nIf butterflies overwintering as pupae have higher level of legal protection\n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(OWS_pupae) %&gt;% \n  summarise(mean_conserve = mean(conserv.eu))\n\n\n  \n\n\n\n\nIf butterflies occurring at higher altitudes on average have a higher level of protection\n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  mutate(ALT_cat = case_when(ALT_Min &lt; 200 ~ \"niedrig\",\n                            ALT_Min &gt; 1000 ~ \"hoch\",\n                            ALT_Min &lt;= 1000 & ALT_Min &gt;= 200 ~ \"mittel\" )) %&gt;% \n  group_by(ALT_cat) %&gt;% \n  summarise(mean_conserv = mean(conserv.eu))\n\n\n  \n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(conserv.eu) %&gt;% \n  summarise(mean_ALT = mean(ALT_Min))\n\n\n  \n\n\n\n\nIf feeding on honeydew is more common in larger butterflies.\n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(AFB_honeydew) %&gt;% \n  summarise(size = mean(WSP_Female_average))\n\n\n  \n\n\n\n\nHow many butterfly species are there per family?\n\n\nbe %&gt;% \n  drop_na() %&gt;% \n  group_by(family) %&gt;% \n  tally()\n\n\n  \n\n\n\nFor a–c also determine how many species belong to each group.",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "ggplot.html",
    "href": "ggplot.html",
    "title": "5  The ggplot2 package",
    "section": "",
    "text": "5.1 Very (!) brief introduction\nggplot2 is a graphing library, i.e., a tool to make graphs in R. Compared with base graphs and other graphics packages, it comes with a number of advantages:\nCompared with other packages the major drawbacks would be that it comes with a steep(ish) learning curve and is probably less intuitive for beginners. The reason is that ggplot2 doesn’t have fixed commands for scatterplots, boxplots, barplots, etc, but rather creates the plot in layers. The most important elements (or layers) of a plot in ggplot2 are:",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#very-brief-introduction",
    "href": "ggplot.html#very-brief-introduction",
    "title": "5  The ggplot2 package",
    "section": "",
    "text": "Beautiful!\nHighly customizable (which is not always necessary though)\nEasiest way to create very complex plots\nTightly integrated into the tidyverse\n\n\n\nData: as we are still in the tidyverse, this is always a data frame\nAesthetics: i.e., what you want to plot. Often, this will correpond to variables (columns) in your dataframe\nGeometric objects: i.e., how you want to plot the data. This can be points, bars, boxplots, lines, etc..\nFacets: more about this later\nAdditional (optional) adjustments: this includes themes that specify the overall design",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#building-up-the-plot",
    "href": "ggplot.html#building-up-the-plot",
    "title": "5  The ggplot2 package",
    "section": "5.2 Building up the plot",
    "text": "5.2 Building up the plot\nWe will start off with a very simple scatterplot and gradually increase the complexity to illustrate ggplot2 functionality.\n\n# data and packages\nlibrary(tidyverse, quietly = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nbe &lt;-  read.table(\"data/butterfly_ecology.csv\", header = TRUE, sep = \",\")\n\n# simple plot\nbe %&gt;%                                # DATA\n  ggplot(aes(x = WSP_Female_average,  # AESTHETICS\n             y = range.size))   \n\n\n\n\n\n\n\n\nIn the above example, the data is the data frame that we have been using the whole time. Notice how we can simply pipe it to ggplot2. aes specifies our aesthetics, i.e., what we want to plot. What is missing?\n\n# simple scatter plot\nbe %&gt;%                               # DATA\n  ggplot(aes(x = WSP_Female_average, # Aesthetics\n             y = range.size)) +\n  geom_point()                       # Geometric object\n\n\n\n\n\n\n\n\nThe geometric object, i.e., how we want to plot our aesthetics. Notice that elements in ggplot2 are added with the + symbol (this is specific to ggplot2). We can add more geometric objects that will use the same aesthetics:\n\n# lets add another geom (a regression line), and also change the theme\nbe %&gt;%                               # DATA\n  ggplot(aes(x = WSP_Female_average, # Aesthetics\n             y = range.size)) +\n  geom_point() +                     # Geometric object\n  geom_smooth(method = \"lm\") +       # Another geometric object\n  theme_light()                      # Let's also change the theme\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nChanging the theme changes many layout options. For different applications, different themes might be appropriate. There are many additional themes available through packages such as ggthemr or ggthemes.\nA bit more on aesthetics: have you noticed that you only specify x and y once, and all geoms know what you want to plot. You can also specify additional aesthetics for each geom.\n\n# Add additional aesthetics, here: we want to plot the sonservation status. How? With colour! \n\nbe %&gt;%                                   # DATA\n  ggplot(aes(x = WSP_Female_average,     # Aesthetics\n             y = range.size)) +\n  geom_point(aes(color = conserv.eu)) +  # aesthetics specific to the points only\n  geom_smooth(method = \"lm\",\n              color = \"black\",\n              se = FALSE) +              \n  theme_light() +\n  scale_color_viridis_c()                # let's use some nicer colors                         \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nAesthetics can be added through colors or shapes",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#faceting",
    "href": "ggplot.html#faceting",
    "title": "5  The ggplot2 package",
    "section": "5.3 Faceting",
    "text": "5.3 Faceting\nSo far, we have cramped as much information as possible into the plot. This was useful to illustrate the functionality of ggplot2, but did not create very readable plots. Often, faceting is a better solution. The implementation in ggplot2 is very straightforward.\n\n# Same example as before, faceted over family\nbe %&gt;%                                   \n  ggplot(aes(x = WSP_Female_average,     \n             y = range.size)) +\n  geom_point(aes(color = conserv.eu)) + \n  geom_smooth(method = \"lm\",\n              color = \"black\",\n              se = FALSE) +              \n  theme_light() +\n  scale_color_viridis_c() +                \n  facet_wrap(~family, scales = \"free\")   \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# And now, faceting over 2 variables\nbe %&gt;%   \n  ggplot(aes(x = WSP_Female_average,     \n             y = range.size)) +\n  geom_point(aes(color = conserv.eu)) +  \n  geom_smooth(method = \"lm\",\n              color = \"black\",\n              se = FALSE) +              \n  theme_light() +\n  scale_color_viridis_c() +                \n  facet_grid(OWS_egg ~ family, scales = \"free\") # faceting\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThe above plot would need a little ‘cleaning up’. How would you do that?",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#some-common-plot-types",
    "href": "ggplot.html#some-common-plot-types",
    "title": "5  The ggplot2 package",
    "section": "5.4 Some common plot types",
    "text": "5.4 Some common plot types\nWe have looked at scatterplots, now let’s look at a number of other commonly used plots.\n\n# histograms\nbe %&gt;% \n  ggplot(aes(x = WSP_Female_average, fill = family)) +\n  geom_histogram(bins = 50) +\n  theme_light() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n# better alternative are often density plots\nbe %&gt;% \n  ggplot(aes(x = WSP_Female_average, fill = family)) +\n  geom_density(alpha = 0.5, colour = NA) +\n  theme_light() +\n  scale_fill_brewer(palette = \"Set1\") \n\n\n\n\n\n\n\n# boxplots\nbe %&gt;% \n  ggplot(aes(y = WSP_Female_average, x = family)) +\n  geom_boxplot() +\n  theme_light() \n\n\n\n\n\n\n\n# better alternative are violin plots\nbe %&gt;% \n  ggplot(aes(y = WSP_Female_average, x = family)) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  theme_light()",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#fine-tuning-plots",
    "href": "ggplot.html#fine-tuning-plots",
    "title": "5  The ggplot2 package",
    "section": "5.5 Fine tuning plots",
    "text": "5.5 Fine tuning plots\nWe now know how to plot some common chart types but most of these don’t look publishable yet. Lets return to one of our first examples and see how to polish it a little.\n\n# A publication ready plot\nbe %&gt;% \n  filter(family != \"Riodinidae\") %&gt;%\n  ggplot(aes(y = WSP_Female_average, x = family, color = family)) +\n  geom_boxplot(lwd = 1, outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width = 2), alpha = 0.5) +\n  theme_light() +\n  labs(title = \"Wing span across European butterfly families\",\n       y = \"Average wing span of female\")+ \n  theme(plot.title = element_text(face = \"bold\"),\n        axis.title.y = element_text(size = 12),\n        axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        strip.text = element_text(size = 11)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Family\")",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "ggplot.html#exercise",
    "href": "ggplot.html#exercise",
    "title": "5  The ggplot2 package",
    "section": "Exercise",
    "text": "Exercise\nUsing ggplot2, explore how range size differs between butterfly families and plot check if butterflies with smaller ranges have higher protection status. Try to find appropriate plot types for this, use the help pages to find plot types that were not introduced to you yet. Explore other variables that may explain some trends in the data. Find a theme that you like! Remember, start with the data, add aesthetics (what do you want to plot), and then think about geometric objects (how do you want to plot the data). How can colour help in your visualisations? Does faceting make sense?\n\nbe %&gt;% \n  mutate(OWS_egg = as.factor(OWS_egg)) %&gt;% \n  ggplot(aes(x = conserv.europe, group = OWS_egg, fill = OWS_egg)) +\n  geom_bar(position = position_dodge()) \n\nWarning: Removed 37 rows containing non-finite outside the scale range\n(`stat_count()`).",
    "crumbs": [
      "`R`",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The `ggplot2` package</span>"
    ]
  },
  {
    "objectID": "unixIntro.html",
    "href": "unixIntro.html",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "",
    "text": "6.1 The Unix / Linux environment\nUnix and Linux (a variant of Unix) are operating systems (like Windows or macOS). They belong to a “family” of operating systems that share a common ancestor, have been around since 1969 and it’s not likely to disappear any time soon.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#the-unix-linux-environment",
    "href": "unixIntro.html#the-unix-linux-environment",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "",
    "text": "Commonly used among the scientific and technical community (e.g. servers and scientific clusters).\nmacOS is Unix-based system.\nMost supercomputers are powered by Unix-like operating systems.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#why-learn-unix-command-line",
    "href": "unixIntro.html#why-learn-unix-command-line",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "6.2 Why learn Unix command line?",
    "text": "6.2 Why learn Unix command line?\n\nIs the foundation of scientific computing (e.g. bioinformatics and data analysis)\nPowerful for working on large datasets and files\nHelps automate repetitive tasks (e.g. imagine you need to need to rename or modify 1,000 files?)\nEnables use of higher-powered computers elsewhere (clusters and servers/cloud-computing)",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#some-terminology",
    "href": "unixIntro.html#some-terminology",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "6.3 Some terminology",
    "text": "6.3 Some terminology\n\n\n\nimage1.png\n\n\nAs a user you can “communicate” with your Linux system either by a Graphical User Interface (GUI) or by typing instructions (commands) using a Command Line Interface (CLI). At first it might look quite complex and confusing but once you understand the concept and the basics then its quite simple and intuitive!\nCommand Line: is the written instructions we type.\nTerminal: also known as terminal emulator is the text-based environment (software) capable of taking input and providing output.\nShell: a program that interprets command-line input and executes commands. There are different shells available e.g bash, zsh, sh, csh etc. Each of them offering unique features and functionalities. Some are more basic and some are more fancy but all serve the same purpose, to interpret the commands provided by the user and output the results. The most commonly used is the bash shell.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#some-important-rules",
    "href": "unixIntro.html#some-important-rules",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "6.4 Some important rules",
    "text": "6.4 Some important rules\n\nBe aware of the case! The command line is case sensitive so be careful when typing. For example, typing Echo is not the same as echo, nor are the directory names “/Results” and “/results”.\nSpaces are having special use! The command line uses spaces as separators between arguments. Using spaces in filenames or directory names will certainly cause problems sooner or later. Avoid using names that contain spaces, but rather it’s better to use dashes (-) or underscores (_). e.g., “results_2026.txt” is preferred over “results 2026.txt”.\nApart from spaces there are several other characters (special characters) that can be used to perform special operations. See some examples bellow.\n\n\n\n\n\n\n\n\nCharacter\nDescription\n\n\n\n\n/\nDirectory separator, used to separate a string of directory names. Example: /usr/src/linux\n\n\n\\\nEscape — (backslash) prevents the next character from being interpreted as a special character. This works outside of quoting, inside double quotes, and generally ignored in single quotes.\n\n\n.\nCurrent directory. Can also “hide” files when it is the first character in a filename.\n\n\n..\nParent directory\n\n\n~\nThe tilde is a representation of the current user’s home directory.\n\n\n*\nRepresents 0 or more characters in a filename, or by itself, all files in a directory.\n\n\n?\nRepresents a single character in a filename.\n\n\n$\nExpansion — introduces various types of expansion: parameter expansion (e.g. $var or ${var}), command substitution (e.g. $(command)), or arithmetic expansion (e.g. $((expression))). More on expansions later.\n\n\n[ ]\nCan be used to represent a range of values, e.g. [0-9], [A-Z], etc. Example: hello[0-2].txt represents the names hello0.txt, hello1.txt, and hello2.txt\n\n\n|\nPipe — send the output from one command to the input of another command. This is a method of chaining commands together. Example: echo “Hello beautiful.” | grep -o beautiful.\n\n\n&gt;\nRedirect output of a command into a new file. If the file already exists, over-write it. Example: ls &gt; myfiles.txt\n\n\n&gt;&gt;\nRedirect and appends the output of a command onto the end of an existing file.\n\n\n&lt;\nRedirect a file as input to a program.\n\n\n;\nCommand separator. Allows you to execute multiple commands on a single line. Example: cd /var/log ; ls -l\n\n\n&&\nCommand separator as above, but only runs the second command if the first one finished without errors.\n\n\n&\nBackground – when used at the end of a command, run the command in the background (do not wait for it to complete).\n\n\n#\nComment — the # character begins a commentary that extends to the end of the line. Comments are notes of explanation and are not processed by the shell.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixIntro.html#accessing-your-terminal",
    "href": "unixIntro.html#accessing-your-terminal",
    "title": "6  Introduction to Linux Environment and Command Line.",
    "section": "6.5 Accessing your Terminal",
    "text": "6.5 Accessing your Terminal\nYou are using a Kubuntu Linux system. In Kubuntu, the terminal emulator is called Konsole. You can start it in any of the following ways:\n1. Press Ctrl + Alt + T to open Konsole instantly.\n2. Open the Krunner by pressing Alt + Space,  type Konsole, then press Enter.\n3. Open the Application Launcher → System → Konsole.\nNow you are ready to start typing your first commands!",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to Linux Environment and Command Line.</span>"
    ]
  },
  {
    "objectID": "unixPart1.html",
    "href": "unixPart1.html",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "",
    "text": "7.1 Finding out where you are\nUnix systems, like most operating systems, store file locations in a hierarchical structure. In the UNIX file-system each file and directory has its own “address”, and that address is called a “path”.\nThere are two special locations in all Unix-based systems That you should be familiar. The “root” location is where the address system of the computer starts. The “home” location is where the current user’s location starts.\nBy default every time you open a new terminal you start in your own “home” directory(containing files and directories that only you can modify). The path of home directory is usually represented by the “~” character.\nBasic commands we will use\nThe pwd command in Linux is short for print working directory. It’s only function is to print the absolute path of the current directory. It’s handy when you’re not exactly sure what directory you’re in. So make it a good habit to get used to running the pwd command a lot.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#finding-out-where-you-are",
    "href": "unixPart1.html#finding-out-where-you-are",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "",
    "text": "# Example. Try running pwd. What do you see?\npwd\n# What do you see if you run PWD instead? \nPWD\n# Try now to run this \"echo $PWD\". The command echo just prints the parameters we give it. The $PWD is a environment variable and we will discuss about their use a bit later on.\necho $PWD",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#the-command-ls",
    "href": "unixPart1.html#the-command-ls",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "7.2 The command ls",
    "text": "7.2 The command ls\nls is short for list, and is used to list the files and sub-directories in your present working directory or some other directory if you specify one.\n# Examples\n# List the files and directories in your current directory\nls\n# or\nls .\n# ls can accept several options. Try running the following commands and observe how their output differs from the previous one.\nls -l\n# or\nls -lh\n# You can use ls to list the contents of any directory. Try the following.\nls -l /etc \n\n\n\n\n\n\nNoteNote\n\n\n\nI. The anatomy of a command (or command syntax)\nEach command is usually composed of three parts:\n\nThe command itself\nThe options: These are optional parameters that can be used to customise the behavior of a command. (e.g. on the previous examples ls -l shows the list of files in a long format)\nThe arguments: specify the target of the command. (e.g. on the previous example ls -l /etc you instructed the command to list the contents of the /etc directory)\n\nII. Getting help\nMost Unix commands can accept several parameters. How do we know which ones to use and why? Luckily, most Unix commands have built-in help documentation that we can access by providing –help as the only argument.\nTry for example: ls --help\nAnother way to access the documentation for a command is by using the man command and providing the command’s name as an argument. For example:\nman ls",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#relative-vs-absolute-path-and-getting-help",
    "href": "unixPart1.html#relative-vs-absolute-path-and-getting-help",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "7.3 Relative vs absolute path and getting help",
    "text": "7.3 Relative vs absolute path and getting help\nThere are two ways to specify the path (the file’s address on the computer):\n\nAn absolute path starts from a fixed location, either the root directory (/) or the home directory (~/). Note: A “full path” usually refers to an absolute path that starts from the root (/).\nA relative path starts from your current directory.\n\n\nWhen working at the command line, it’s important to always be aware of your current location in the system. One of the most common mistakes is trying to operate on a file that isn’t where you think it is. To avoid this, it’s good practice to use absolute paths, which clearly specify a file’s exact location regardless of where you are.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#moving-around",
    "href": "unixPart1.html#moving-around",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "7.4 Moving around",
    "text": "7.4 Moving around\nOne of the most commonly used commands in Linux is the change directory command, or cd. It allows you to change your working directory from the current location to another directory you want to navigate to. The cd command takes a positional argument: the path (address) of the directory you want to move into. This path can be either absolute or relative. Let’s try moving from our current directory to a directory present in your home directory called Documents.\n# The relative way\ncd Documents\n# The absolute way (~ stands for /home/&lt;user&gt;/). Can you see the change in your command prompt?\ncd ~/Documents\n# But how do we go back “up” to the parent directory? We can use the \"..\" special characters that act as a relative path, telling the system to move up one directory from our current location.\ncd ..\n# When you need to navigate back to the previous working directory from the current working directory, you can use the \"–\" option.\ncd -\n# Note: running the cd command without any arguments will always bring you back to your home\ncd",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart1.html#exercise",
    "href": "unixPart1.html#exercise",
    "title": "7  Your first Unix commands - Navigating the Unix File-System Structure",
    "section": "Exercise",
    "text": "Exercise\nPractice moving around the filesystem with cd and listing directory contents with ls, including navigating by relative and absolute paths or using special characters “..” . Use pwd frequently to see your current working directory. Practice navigating home with cd.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Your first Unix commands - Navigating the Unix File-System Structure</span>"
    ]
  },
  {
    "objectID": "unixPart2.html",
    "href": "unixPart2.html",
    "title": "8  Working with Files and Directories",
    "section": "",
    "text": "8.1 Creating new directories and files\nYou can create a new directory using the mkdir command. It takes as a parameter a relative or absolute path to the directory to create",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#creating-new-directories-and-files",
    "href": "unixPart2.html#creating-new-directories-and-files",
    "title": "8  Working with Files and Directories",
    "section": "",
    "text": "# Let's try some examples. From your home directory navigate to Documents/ and create 2 new nested sub-directories with the names projects and project1\npwd\ncd Documents\nmkdir projects\ncd projects\nmkdir projects1\npwd\n# Try to create a new directory with the same name e.g. projects1. What do you see?",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#exercise",
    "href": "unixPart2.html#exercise",
    "title": "8  Working with Files and Directories",
    "section": "Exercise",
    "text": "Exercise\nIn the previous example we created the two directories in two separate steps. Can we do it in one step instead? (check the help page for mkdir command).\nNow that we saw how to create a new directory lets see how we can create a new file. There are several ways to do this in Linux command line but we will start with the basic one. The command touch will create a new, empty file.\n# Create an Empty File within the new directory project1.\ntouch notes.txt\n# can you do it if you are outside the project1 directory?",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#move-or-rename-a-file-or-directory",
    "href": "unixPart2.html#move-or-rename-a-file-or-directory",
    "title": "8  Working with Files and Directories",
    "section": "8.2 Move or Rename a File or Directory",
    "text": "8.2 Move or Rename a File or Directory\nThe mv command serves for both moving and renaming files and directories. Works by specifying a “source_path” and a “destination_path”, wheres “source_path” is the path (absolute or relative) of the file/directory to move or rename, and “destination_path” is the new name or location to give it.\nCopying files and directories is similar operation, except that the original file or directory is not removed! We will use the cp command for this.\n\nNOTE: copying, moving or renaming files at the command line will overwrite files if they have the same name!!\n\nLet’s try them both.\n# move the notes.txt file one directory up assuming your current directory is project1\nmv notes.txt ../notes.txt\n# rename the notes.txt to README.txt\nmv ../notes.txt ../README.txt\n# copy the README.txt file to project1 directory as README1.txt\ncp ../README.txt ./README1.txt",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#exercise-1",
    "href": "unixPart2.html#exercise-1",
    "title": "8  Working with Files and Directories",
    "section": "Exercise",
    "text": "Exercise\nTry to make a copy of the complete project1 directory on the same parent location with the name project2. What do you see?\n\n\n\n\n\n\nNoteNote\n\n\n\nWildcards: friend and foe at the command prompt.\nAs we saw earlier some characters have special use in Unix command line. For example:\n\nthe asterisk character “*” represents 0 or more characters in a filename, or by itself, all files in a directory.\nthe square brackets […] can be used define a range of values e.g. [0-9], [A-Z], etc.\nthe question mark “?” can represent any single character.\n\nThese characters can be used as “Wildcards” to perform operations on multiple files at the same time. Lets see some examples.\n# Create a directory with the name wildcard_practice and, inside it, create some empty files.\nmkdir wildcard_practice\ncd wildcard_practice\ntouch file1.txt file2.txt file10.txt\ntouch image1.png image2.png imageA.png\ntouch data_2022.csv data_2023.csv data_2024.csv\n# List all the .txt files\nls *.txt\n# create a directory called data and move the .csv files into data/\nmkdir data\nmv *.csv data/\n\nExercise\nList only the .csv files with years 2022 or 2023\n\n\n8.3 Delete (remove) files and directories\n\n\n\n\n\n\nWarningWARNING!!\n\n\n\nUsing the rm command permanently deletes files and directories without moving them to a trash or recycle bin. This action cannot be undone! Whenever you use the rm command, ALWAYS double-check your syntax.\n\n\n# Lets remove some unwanted files and directories\nrm ~/Documents/projects/README.txt\n# the command rmdir can remove an empty directory, so it is safer!\nmkdir test_dir\nrmdir test_dir\n# The dangerous way. Use rm recursively \"-r\" to remove a non-empty directory. BE CAUTIOUS! YOU HAVE BEEN WARNED!\nrm -r ~/Documents/projects/project1\n# check the help page for rm command. Is there a safer way to do this? \n\n\nExercise\n\nCreate a directory data.\nInside data, create three empty files: sample1.txt, sample2.txt, sample3.txt\nCopy sample1.txt into a new directory called backup.\nRename sample2.txt to s2.txt.\nRemove sample3.txt.\n\n\n\n8.4 Viewing and inspecting Files\nSometimes we want to quickly look at files, either to inspect their structure or to get some basic information about their contents. These are some commands we can use to do so.\n\n\n\nCommand\nDescription\n\n\n\n\ncat\nprint entire file\n\n\nless\nscroll through file\n\n\nhead\nshow first n lines\n\n\ntail\nshow last n lines\n\n\nwc\ncount (word count)\n\n\n\n# Lets use the file \"butterfly_ecology.csv\" you previously used with R.\n# You can either navigate to the folder you saved the file copy it to a new directory.\n# You can use the \"cat\" command for viewing the contents of a file\ncat butterfly_ecology.csv\n# Another command for viewing the contents of a file is \"less\". This command allows to scroll through the document and view it line my line or page by page. We can even do some text searching. To exit less, press q (for quit).\nless butterfly_ecology.csv\n# However, most of the time we only need to see part of a file rather than the entire file. The commands \"head\" and \"tail\" come in handy. These commands print only the first (head) or the last (tail) lines of a file.\nhead butterfly_ecology.csv\ntail butterfly_ecology.csv\n# or you can specify the number of lines to print (by default will print 10)\nhead -n 2 butterfly_ecology.csv\ntail -n 3 butterfly_ecology.csv\n# The command wc (word count) is useful for counting how many lines, words, and characters there are in a file.\nwc butterfly_ecology.csv\n# Can you find an option for wc that will let us get only the number of lines of the file? \n\n\n\n\n\n\nNoteNote\n\n\n\nThe cat command can also be used to create new files or to concatenate existing files. Lets try:\ncat &gt; file1\nStefanos\n# Exit by pressing Ctrl + D\ncat &gt; file2\nsome random text\n# Exit by pressing Ctrl + D\ncat file1 file2 &gt; file3\n\n\n\n\n8.5 A text editor for the terminal\nA simple text editor available on most systems is nano. To run it, simply specify a file name to edit. If the file doesn’t exist already, it will be created after saving. \nnano NOTES.txt",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#exercise-2",
    "href": "unixPart2.html#exercise-2",
    "title": "8  Working with Files and Directories",
    "section": "Exercise",
    "text": "Exercise\nList only the .csv files with years 2022 or 2023",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#delete-remove-files-and-directories",
    "href": "unixPart2.html#delete-remove-files-and-directories",
    "title": "8  Working with Files and Directories",
    "section": "8.3 Delete (remove) files and directories",
    "text": "8.3 Delete (remove) files and directories\n\n\n\n\n\n\nWarningWARNING!!\n\n\n\nUsing the rm command permanently deletes files and directories without moving them to a trash or recycle bin. This action cannot be undone! Whenever you use the rm command, ALWAYS double-check your syntax.\n\n\n# Lets remove some unwanted files and directories\nrm ~/Documents/projects/README.txt\n# the command rmdir can remove an empty directory, so it is safer!\nmkdir test_dir\nrmdir test_dir\n# The dangerous way. Use rm recursively \"-r\" to remove a non-empty directory. BE CAUTIOUS! YOU HAVE BEEN WARNED!\nrm -r ~/Documents/projects/project1\n# check the help page for rm command. Is there a safer way to do this?",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#exercise-3",
    "href": "unixPart2.html#exercise-3",
    "title": "8  Working with Files and Directories",
    "section": "Exercise",
    "text": "Exercise\n\nCreate a directory data.\nInside data, create three empty files: sample1.txt, sample2.txt, sample3.txt\nCopy sample1.txt into a new directory called backup.\nRename sample2.txt to s2.txt.\nRemove sample3.txt.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#viewing-and-inspecting-files",
    "href": "unixPart2.html#viewing-and-inspecting-files",
    "title": "8  Working with Files and Directories",
    "section": "8.4 Viewing and inspecting Files",
    "text": "8.4 Viewing and inspecting Files\nSometimes we want to quickly look at files, either to inspect their structure or to get some basic information about their contents. These are some commands we can use to do so.\n\n\n\nCommand\nDescription\n\n\n\n\ncat\nprint entire file\n\n\nless\nscroll through file\n\n\nhead\nshow first n lines\n\n\ntail\nshow last n lines\n\n\nwc\ncount (word count)\n\n\n\n# Lets use the file \"butterfly_ecology.csv\" you previously used with R.\n# You can either navigate to the folder you saved the file copy it to a new directory.\n# You can use the \"cat\" command for viewing the contents of a file\ncat butterfly_ecology.csv\n# Another command for viewing the contents of a file is \"less\". This command allows to scroll through the document and view it line my line or page by page. We can even do some text searching. To exit less, press q (for quit).\nless butterfly_ecology.csv\n# However, most of the time we only need to see part of a file rather than the entire file. The commands \"head\" and \"tail\" come in handy. These commands print only the first (head) or the last (tail) lines of a file.\nhead butterfly_ecology.csv\ntail butterfly_ecology.csv\n# or you can specify the number of lines to print (by default will print 10)\nhead -n 2 butterfly_ecology.csv\ntail -n 3 butterfly_ecology.csv\n# The command wc (word count) is useful for counting how many lines, words, and characters there are in a file.\nwc butterfly_ecology.csv\n# Can you find an option for wc that will let us get only the number of lines of the file? \n\n\n\n\n\n\nNoteNote\n\n\n\nThe cat command can also be used to create new files or to concatenate existing files. Lets try:\ncat &gt; file1\nStefanos\n# Exit by pressing Ctrl + D\ncat &gt; file2\nsome random text\n# Exit by pressing Ctrl + D\ncat file1 file2 &gt; file3",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart2.html#a-text-editor-for-the-terminal",
    "href": "unixPart2.html#a-text-editor-for-the-terminal",
    "title": "8  Working with Files and Directories",
    "section": "8.5 A text editor for the terminal",
    "text": "8.5 A text editor for the terminal\nA simple text editor available on most systems is nano. To run it, simply specify a file name to edit. If the file doesn’t exist already, it will be created after saving. \nnano NOTES.txt",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Files and Directories</span>"
    ]
  },
  {
    "objectID": "unixPart3.html",
    "href": "unixPart3.html",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "",
    "text": "9.1 Redirecting and piping\nRedirection and piping are fundamental features of the UNIX command line that allow us to create powerful workflows for automating tasks. By default, when we run a command, its output is printed to the screen (the terminal). In many situations, however, we may want to save this output to a file or pass it directly to another command.\nWe have already seen how a command output can be redirected to a file. For example: cat file1 file2 &gt; combined_file. In this command, the redirection operator &gt; tells the shell to send the output of cat to a new file called combined_file instead of displaying it on the screen. Let look at another example.\nWhile redirection allows us to save a command’s output to a file, UNIX also allows us to connect commands directly to one another. We can send the output of one command directly as input to another using the pipe (|) operator.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#redirecting-and-piping",
    "href": "unixPart3.html#redirecting-and-piping",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "",
    "text": "# list the files in the /etc directory and save the output to a file called etc_content.txt.\nls /etc &gt; etc_content.txt\nless etc_content.txt\n\n\n\n\n\n\nWarningWARNING!!\n\n\n\nIt’s important to remember that the &gt; operator will overwrite a file if it already exists. If you want to append an output to an existing file, rather than overwrite it, you can use instead &gt;&gt;.\n\nTry it out yourself!\n\n\n\n\n#list only the first 10 files of the /etc directory\nls /etc | head -n 10",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#exercise",
    "href": "unixPart3.html#exercise",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "Exercise",
    "text": "Exercise\nUsing the pipe operator, find a way to count all contents (files and directories) in the /etc directory.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#cut-sort-and-uniq",
    "href": "unixPart3.html#cut-sort-and-uniq",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "9.2 cut, sort, and uniq",
    "text": "9.2 cut, sort, and uniq\nThe cut command is used for extracting specific sections from lines of text in a file or piped data. It’s a great tools for or data manipulation. Lets try some examples:\n# Selecting fields separated by a delimiter\necho \"name,age,city,country\" | cut -d ',' -f 3\n# lets manipulate some real data\ncat butterfly_ecology.csv | cut -d ',' -f 1,2 | head\n# you can even change the delimiter in the output\ncat butterfly_ecology.csv | cut -d ',' -f 1,2  --output-delimiter $'\\t' | head\n# or\ncat butterfly_ecology.csv | cut -d ',' -f 1,2,5-7  --output-delimiter $'\\t' | head\nThe sort command is used to arrange the lines of text files in a specified order, such as alphabetically or numerically. It can also handle options for sorting in reverse order or by specific columns. NOTE: sort command will not modify your file, it will only print the reordered content on the terminal! However, you can specify redirect the output to a separate file.\ncat butterfly_ecology.csv | cut -d ',' -f 1-3 | sort\ncat butterfly_ecology.csv | cut -d ',' -f 1-3 | sort -t ',' -k3 | less\nThe uniq command in Linux is used to filter out repeated lines from a text file or standard input, displaying only unique entries or counting repetitions. It works best when the input is sorted, as it only removes adjacent duplicate lines.\n# families are represented in the butterfly_ecology.csv data?\ncat butterfly_ecology.csv | cut -d ',' -f 2 | sort | uniq",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#exercise-1",
    "href": "unixPart3.html#exercise-1",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "Exercise",
    "text": "Exercise\nCan you use cut, sort and uniq commands to count the number of species per family species per family?",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#grep-and-regular-expressions",
    "href": "unixPart3.html#grep-and-regular-expressions",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "9.3 grep and regular expressions",
    "text": "9.3 grep and regular expressions\nThegrep (global regular expression) command in Linux is used to search for specific patterns or strings within files and display the matching lines. It is a powerful tool for text processing and can be customized with various options to refine search results. The basic usage is: grep “searchword” filename\n# Let's say you wished to identify every line which contain the string \"Papilionidae\" from the butterfly_ecology.csv\ngrep 'Papilionidae' butterfly_ecology.csv\n# We can find the number of lines that matches the given string/pattern instead\ngrep -c 'Papilionidae' butterfly_ecology.csv\n# Use -f  option to read patterns from a file\ncat &gt; family.txt\nPapilionidae\nHesperiidae\n# exit by pressing Ctrl + D\ngrep -f family.txt butterfly_ecology.csv\n# We can search for patterns in multiple files (e.g. all the files in the directory)\ngrep 'Papilionidae' *\ngrep command is particularly powerful when used in combination with Regular Expressions. A regular expression (regex) in Linux is a sequence of characters that defines a search pattern, similar to the wildcards, and are commonly used for searching and manipulating text.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#exercise-2",
    "href": "unixPart3.html#exercise-2",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "Exercise",
    "text": "Exercise\n\nIn the butterfly_ecology.csv file find all “Aglais” species and save the fields species, family, range.size in a new file.\nThe shell keeps a record of the commands you have previously run. You can display this list using the history command. Using this information, determine how many times you have used the ls command in your shell history. Hint: You may need to combine history with other command-line tools such as grep.",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "unixPart3.html#awk",
    "href": "unixPart3.html#awk",
    "title": "9  Redirection, Pipes, and Text Processing in Unix",
    "section": "9.4 awk",
    "text": "9.4 awk\nawk is the Unix command to work with tabular data. The basic awk syntax is: awk [options] ‘pattern {action}’ input-file &gt; output-file\nawk  -F ',' '{print $0}' butterfly_ecology.csv\n\nawk -F ',' '$3 &gt; 1000 && $3 != \"NA\" {print $0}' butterfly_ecology.csv\n\nawk -F ',' '$3 &gt; 1000 && $3 != \"NA\" {print $1, $3}' butterfly_ecology.csv\n\nawk -F ',' '{if($3 &gt; 1000 && $3 != \"NA\") {print $1\":::\"$3}}' butterfly_ecology.csv\nA more complex example\nUsing only command line try calculate the average range.size for each family separately and store this information in a new file.\ngrep -v \"^species\" butterfly_ecology.csv | \\\nawk -F ',' '\n$3 != \"NA\" {\n  sum[$2] += $3\n  count[$2]++\n}\nEND {\n  for (family in sum) {\n    print family, sum[family] / count[family]\n  }\n}\n' OFS=',' &gt; average_range_by_family.csv",
    "crumbs": [
      "UNIX",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Redirection, Pipes, and Text Processing in Unix</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart1.html",
    "href": "NGSanalysisPart1.html",
    "title": "10  Installing bioinformatics tools and the conda package manager",
    "section": "",
    "text": "10.1 What is Conda and why is needed\nAnalysis of Next-Generation Sequencing (NGS) data relies on specialized bioinformatics tools (e.g., FastQC, BWA, SAMtools).\nThese tools are often developed by different people, in different programming languages, and with different software dependencies, and installing them is not always straightforward (or at least it used to be).\nConda is a cross-platform package/tool and environment managing system that runs on Windows, macOS, and Linux.\nIt allows to:",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing bioinformatics tools and the conda package manager</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart1.html#what-is-conda-and-why-is-needed",
    "href": "NGSanalysisPart1.html#what-is-conda-and-why-is-needed",
    "title": "10  Installing bioinformatics tools and the conda package manager",
    "section": "",
    "text": "Install bioinformatics tools and not only - No need to compile software from source, after all we all have other things to worry about!\nAutomatically resolve dependencies - No need to worry about your missing library!\nCreate isolated software environments - Less chances to mess up with the rest of the software or the system itself!\n\n\n10.1.1 Installing and setting-up a conda environment in your linux system\nFo the purpose of this course we will use the miniforge minimal installer. Use your terminal window (konsole) to download the miniforge installer by running the following command. Yes! you can use your terminal to download files from the web. For this we will use the UNIX command wget. Feel free to copy paste the command to avoid typos.\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nand run the script like this:\nbash Miniforge3-$(uname)-$(uname -m).sh\nNow follow the instructions and accept the license terms. When prompt confirm the installation location “/home/idiv/miniforge3”.\nReply yes to “Proceed with initialaization?”\nCongratulations! You have installed conda. But in order for the installation to take effect you should re-load (restart) your terminal or run the following command:\nsource .bashrc\nRun the following command to prevent the activation of the base conda environment on startup. This saves you from having problems later!\nconda config --set auto_activate_base false\nNow it is time to configure some settings, such as adding the necessary channels. Software packages are stored in locations called channels. Most bioinformatics packages are available through the bioconda channel.\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nconda config --show channels\nCreate a conda environment for installing the necessary packages/tools. You can use either conda or mamba command. Mamba is a faster implementation of conda designed to improve the speed of package installation and environment management.\nFor simplicity, we will create our conda environment using an environment.yaml file, which contains a list of the packages required for this course. You will need to download the environment.yaml file from STUDIP and place it in your home directory.\n# Create the environment and install required packages. It might take a few minutes!\nconda env create -f environment.yaml\n# Inspect the environment is there\nconda info -e\n# Activate the environment. You can deactivate the environment using the comand \"conda deactivate\"\nconda activate mlu2026\nAlternatively, you can create an empty conda environment and then install the required packages/tools. You can search for bioconda packages here.\n# using mamba\nmamba create -n mlu2026 # This command will create an empty environment with the name \"mlu2026\". You can use any name for your environment, just avoid using the space character or any other special character!\n# similarly using conda\nconda create -n mlu2026\n# You can now activate your environment using one of the following commands.\nmamba activate mlu2026\n# or\nconda activate mlu2026\nHere you can find a conda-cheatsheet that you can use as a quick reference for managing your Conda environments.",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing bioinformatics tools and the conda package manager</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html",
    "href": "NGSanalysisPart2.html",
    "title": "11  NGS data quality control",
    "section": "",
    "text": "11.1 Obtain some high-throughput sequencing data\nThis practical will provide hands-on experience with quality control of high-throughput sequencing data. You will learn how to:\nFor this practical we will need to download some training sequencing data. We will use the following data (Bioproject PRJNA675888) associated with this article:\nIllumina: SRA paired-end dataset SRR13070681\nNanopore: SRA ONT dataset SRR13070731\nFirst you will need to create a data/ directory in your home folder. Within this directory create two sub-directories one for Illumian and one for the nanopore data.\nOption 1. Browser Navigate to the European Nucleotide Archive (ENA) and search for the sra accession numbers (). Download the files and move them to the respective directory.\nOption 2. You can use the command line to download the data.",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html#obtain-some-high-throughput-sequencing-data",
    "href": "NGSanalysisPart2.html#obtain-some-high-throughput-sequencing-data",
    "title": "11  NGS data quality control",
    "section": "",
    "text": "mkdir -p ~/data/illumina\nmkdir -p ~/data/nanopore\n\n\n# For the Illumina dataset\ncd ~/data/illumina\nwget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR130/081/SRR13070681/SRR13070681_1.fastq.gz\nwget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR130/081/SRR13070681/SRR13070681_2.fastq.gz\n# For the nanopore dataset\ncd ~/data/nanopore\nwget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR130/031/SRR13070731/SRR13070731_1.fastq.gz\n\n\n\n\n\n\nNoteNOTE\n\n\n\nWhat do the _1 and _2 in the Illumina dataset mean?\nMost Illumina sequencing is paired-end, meaning that after DNA fragmentation, both ends of each fragment are sequenced. This produces two reads for each DNA fragment: one read from one end of the fragment (_1, read 1) and a second read from the opposite end (_2, read 2).\n\n\n\npaired-end reads",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html#short-read-data-qc-illumina",
    "href": "NGSanalysisPart2.html#short-read-data-qc-illumina",
    "title": "11  NGS data quality control",
    "section": "11.2 Short-read data QC (Illumina)",
    "text": "11.2 Short-read data QC (Illumina)\nWe can inspect the fastq files\n# make sure you are directory ~/data/illumina\npwd\n# read the 10 first sequences\nzcat SRR13070681_1.fastq.gz | head -40\nWe can now generate some quality metrics for our data using the tool FastQC. For each file, FastQC will produced both a .zip archive containing all the plots, and a html report. You can run FastQC on the two files together or individually.\nmkdir fastqc_output\nfastqc -o fastqc_output *.fastq.gz\nQuestions:\n\nHow many total reads are in both files?\nWhat is the length of the read?\nWhich read files is of better quality?\n\n\n11.2.1 Quality control\nQuality control generally comes in two forms: 1. Trimming: involves removing poor quality bases from the reads (usually the ends) 2. Filtering: involves removing whole sequences either due to poor quality or they are too short\nTo carry this out we will use sickle. Let’s first have a look on the documentation page of sickle.\nsickle --help\n#  You can run now sickle with the Illumina reads using the following command.\nsickle pe -t sanger -f SRR13070681_1.fastq.gz -r SRR13070681_2.fastq.gz -o SRR13070681_1_Q28_MinL100.fastq.gz -p SRR13070681_2_Q28_MinL100.fastq.gz -s SRR13070681_unpaired.fastq.gz -q 28 -l 100\nWe can now re-run fastqc on the trimmed dataset.\nmkdir sickle_fastqc_output\nfastqc -o sickle_fastqc_output SRR13070681_*_Q28_MinL100.fastq.gz\nBonus. We can use the tool MultiQC to aggregate all the FastQC reports into one html report.\nmultiqc ~/data/illumina/fastqc_output ~/data/illumina/sickle_fastqc_output",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html#long-read-data-qc-oxford-nanopore",
    "href": "NGSanalysisPart2.html#long-read-data-qc-oxford-nanopore",
    "title": "11  NGS data quality control",
    "section": "11.3 Long-read data QC (Oxford Nanopore)",
    "text": "11.3 Long-read data QC (Oxford Nanopore)",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart2.html#exercise",
    "href": "NGSanalysisPart2.html#exercise",
    "title": "11  NGS data quality control",
    "section": "Exercise",
    "text": "Exercise\nCheck the quality of the Nanopore dataset. How does it differ from the Illumina reads?\n\n\n\n\n\n\nNoteNOTE\n\n\n\nSubsampling the sequencing data\nSometimes ngs data can be quite large. In this cases it is useful to downsample the data to a more manageable dataset.\nWe can use the tool seqkit to do this.\n# we can do this by number\nzcat file.fastq.gz | seqkit sample -n 100000 -o sample.fastq.gz\n# or by proportion e.g. 10%\nzcat file.fastq.gz | seqkit sample -p 0.1 -o sample.fastq.gz",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>NGS data quality control</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html",
    "href": "NGSanalysisPart3.html",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "",
    "text": "12.1 Prepare the materials for the practical\nThis tutorial provides hands-on experience with a typical metabarcoding workflow for analyzing microbial communities using full-length 16S rRNA ONT sequencing. We will begin with pre-basecalled data, and after performing initial quality control and demultiplexing of the reads, we will assign taxonomy directly to the reads and generate taxon abundance tables for subsequent diversity analysis. The dataset originates from 16S rRNA amplicon sequencing of multiple samples from two tick species: Dermacentor marginatus and Dermacentor reticulatus.\nDownload the materials (ONT_metabarcoding.tar.gz) using the link below.\nlink\nDdecompress the folder with the material using the command\nNavigate to the extracted follder using the cd command",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html#prepare-the-materials-for-the-practical",
    "href": "NGSanalysisPart3.html#prepare-the-materials-for-the-practical",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "",
    "text": "tar -xvzf ONT_metabarcoding.tar.gz  \n\ncd ONT_metabarcoding",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html#quality-filtering-and-inspection.",
    "href": "NGSanalysisPart3.html#quality-filtering-and-inspection.",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "12.2 Quality filtering and inspection.",
    "text": "12.2 Quality filtering and inspection.\nBefore beginning any analyses is a good practice to perform a QC assessment and quality/length filtering of the data.\nFirst create a directory for the filtered reads\nmkdir filtered_reads  \nPerform quality and length filtering using SeqKit. (Length filtering is optional, as it will also be performed during the demultiplexing step.)\nseqkit seq --min-qual 10 --max-len 1800 --min-len 1200 calls_sup_ticks_16S.fastq &gt; filtered_reads/calls_sup_ticks_16S_Q10_1200-1800.fastq\nInspect and compare the reads before and after filtering. How many reads were retained? What is the average read length? How do the quality scores look (mean quality)?\nseqkit stats calls_sup_ticks_16S.fastq filtered_reads/calls_sup_ticks_16S_Q10_1200-1800.fastq\nfastqc calls_sup_ticks_16S.fastq filtered_reads/calls_sup_ticks_16S_Q10_1200-1800.fastq",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html#demultiplexing",
    "href": "NGSanalysisPart3.html#demultiplexing",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "12.3 Demultiplexing",
    "text": "12.3 Demultiplexing\nNow we are ready to demultiplex your data (split the reads by sample). To do this we need some demuliplexing information that usually comes in a form of a table (depending on the specific tool). This table contains information on the sample IDs, the primers used for amplification and the unique dual indices used for each sample. We will use the tool speimux (https://github.com/joshuaowalker/specimux).\nInspect the demultiplexing files in the directory.\nless 16S_specimens.txt\n# press q to exit\nless 16S_primers.fasta\n# press q to exit\nRun specimux command for demultiplexing the ONT reads.\nspecimux 16S_primers.fasta 16S_specimens_sub.txt calls_sup_ticks_16S_Q10_1200-1800.fastq\\\n--output-to-files\\\n--trim primers\\\n--search-len 100\\\n--index-edit-distance 2\\\n--primer-edit-distance 10\\\n--min-length 1300\\\n--max-length 1700\nThe option meanings are:\n--output-to-files: Create individual sample files for sequences\n--trim primers: Remove the primer and the adapters from the read sequence\n--search-len: Length to search for index and primer at start and end of sequence\n--index-edit-distance: The maximum allowed edit distance when matching indices during the demultiplexing process. How many mismatches (or differences) are permitted\n--primer-edit-distance: The maximum allowed edit distance when matching primers. How many mismatches (or differences) are permitted\n--min-length: Minimum sequence length. Shorter sequences will be skipped\n--max-length: Maximum sequence length. Longer sequences will be skipped\nThe demultiplexed reads can be found in the directory \\~/ONT_metabarcoding/data/intermediate/tutorial_2/specimux_16S_demuliplexed/full. There should be X .fastq files corresponding to the X samples.\nInspect the demultiplexed files. How many reads are there per sample? What is the average read size? Why we removed the primers?\nseqkit stats ./16S/full/*.fastq\n\n# Or you can save the summary in a file\n\nseqkit stats ./16S/full/*.fastq | sed 's/,//g' &gt; summary.txt \n\n\n\n\n\n\nNoteNote\n\n\n\nAmong our samples, we have included negative controls (PN.fastq) to monitor for contamination during the PCR amplifications. As indicated in your summary, some of the samples did not generate enough reads, similar to the negative controls. It is always good practice to remove such samples from further analysis, as they may be affected by contamination. What would be an appropriate cutoff for the number of reads to use when identifying and removing failed samples?\n# Create a directory to store the files that have few reads\nmkdir failed\n# Identify the files with not enougth reads and create a list with their names\ncat summary.txt | sed 's/,//g' | awk '{ if ($4 &lt; 250) { print } }' | cut -f1 -d \" \" &gt; remove.txt\n# Now you can move the failed files to the faild/ directory using a for loop\nfor i in $(cat remove.txt); do mv $i failed/; done\n# Or using another unix command `xargs`\nxargs -a remove.txt -I {} mv {} failed/",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "NGSanalysisPart3.html#taxonomic-profiling",
    "href": "NGSanalysisPart3.html#taxonomic-profiling",
    "title": "12  Microbial community profiling of full-length 16S rRNA ONT sequencing reads.",
    "section": "12.4 Taxonomic profiling",
    "text": "12.4 Taxonomic profiling\nThe next goal is to generate taxonomic abundance profiles from our demultiplexed ONT data. This can be done using two different methodologies depending on the approach used for taxonomic assignment. The first methodology assigns taxonomy directly to the individual reads while the second performs clustering of the reads and builds consensus sequences before proceeding with taxonomic assignment.\nPerform taxonomic classification of the reads and estimate taxonomic abundance in each sample using the Emu software. For additional details on the method see the original article (Curry, K.D., Wang, Q., Nute, M.G. et al. Emu: species-level microbial community profiling of full-length 16S rRNA Oxford Nanopore sequencing data. Nat Methods 19, 845–853 (2022). https://doi.org/10.1038/s41592-022-01520-4).\nCreate a directory for the results\nmkdir emu_results\nAssign taxonomy and calculate relative abundances using the emu abundance command.\nfor i in 16S/full/*.fastq; do emu abundance ${i}\\\n--db ~/ONTworkshop_10_06_2025/db/gtdb_ssu_emu\\\n--min-abundance 0.01\\\n--output-dir emu_results; done\nThe option meanings are:\n--db: path to emu database; directory must include the following files, species_taxid.fasta, taxonomy.tsv\n\n\n\n\n\n\n\nFilename\nDescription\n\n\n\n\ntaxonomy.tsv\ntab separated datasheet of database taxonomy lineages containing at columns: ‘tax_id’ and any taxonomic ranks (i.e. species, genus, etc)\n\n\nspecies_taxid.fasta\ndatabase sequences where each sequence header starts with the respective species-level tax id (or lowest level above species-level if missing) preceeding a colon [:]\n\n\n\n--min-abundance: By default emu will report abundances equal or above 0.01% (0.0001). You can adjust this using this option. This will generates results with species relative abundance above this value in addition to full results; e.g. 0.01 = 1%.\nMove the full abundance reports in a separate directory.\nmkdir emu_results/full_abundance mv emu_results/\\*-abundance.tsv emu_results /full_abundance/\nCombine the outputs to create a single table containing all the emu output relative abundances using the combine-outputs function at the desired taxonomic rank (e.g. genus or species). Accepted ranks: [‘species’, ‘genus’, ‘family’, ‘order’, ‘class’, ‘phylum’, ‘superkingdom’]. Note this function will select all the .tsv files in the provided directory that contain ‘rel-abundance’ in the filename.\nemu combine-outputs emu_results/ genus\nNow we are ready to use the combined abundance table(s) for downstream diversity analysis and plotting in R using the phyloseq package. Follow the instructions in the emu-R-analysis.R file.",
    "crumbs": [
      "NGSanalysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Microbial community profiling of full-length 16S rRNA ONT sequencing reads.</span>"
    ]
  },
  {
    "objectID": "phylo1.html",
    "href": "phylo1.html",
    "title": "13  Retrieving and aligning sequences",
    "section": "",
    "text": "13.1 Compiling data\nThe following chapters are intended as a guide through the practical phylogenetics of the course, and as a starting point and reference for your future phylogenetics endeavours. It will cover the usage of the tools that will be introduced to you which together form a very basic phylogenetic workflow.\nYou usually already have a question in mind before you start you phylogenetic analysis. The choice of phylogenetic markers strongly depends on your question: in general you want to use fast evolving markers to resolve recent evolutionary events, and slow evolving markers for ancient evolutionary events. A good starting point for marker selection is the literature: likely someone has already worked on a similar type of question and has identified markers that are potentially useful for you. If your interest is protein evolution you would of course use the protein sequences of interest directly.\nPhylogenies can in principle be reconstructed from many different types of data. In this course, we will focus on DNA sequences. Once you have decided on a marker, you will need to compile your sequences. A good starting point for this is NCBI (National Center for Biotechnology Information) which hosts a massive database of protein and nucleotide sequences.. You will find almost any protein or nucleotide sequence ever published in these databases. The search function is powerful and supports boolean operators (Fig. 1).\nFig. 1 | NCBI landing page with search bar.\nLet’s download some sequences! Select the file accessions_coi.txt from the data folder and use the Batch Entrez feature of NCBI. This convenient tool allows us to simply upload a list of accession numbers and NCBI will retrieve the entries, which you can then download (Fig. 2).\nFig. 2 | Downloading sequences on NCBI. 1. Click Send to; 2. select File; 3. select FASTA; 4. click Create file to download.\nSome words about the sequence data we will be using in the course. We will look at two loci from 60 different species of bees, all occurring in Germany. The data set also includes three outgroup taxa from sphecid and crabronid wasps, the closest relatives of bees.\nOne locus is a part of the mitochondrial cytochrome subunit 1 (COI, also known as the “barcoding locus”). I will demonstrate all of the steps using the COI locus. The other locus is the partial long-wavelength rhodopsin gene (lwr), which contains both coding regions and non-coding, intronic regions.\nBefore we move on, we should check and fix the sequence names. Often one has little control over the identifiers other people put on databases, and it is good to check and correct names at this stage. We can use the UNIX tools we learned about earlier to do this\nWhat are the names of the sequences?\ngrep '&gt;' data/coi.fas | head\n\n&gt;JN262171.1 Andrena chrysosceles voucher BC ZSM HYM 05913 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ836763.1 Andrena proxima voucher BC ZSM HYM 00016 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ837959.1 Epeolus schummeli voucher BC ZSM HYM 08497 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838028.1 Nomada sexfasciata voucher BC ZSM HYM 06153 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838166.1 Colletes succinctus voucher BC ZSM HYM 02017 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838491.1 Epeoloides coecutiens voucher BC ZSM HYM 01997 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838499.1 Nomada leucophthalma voucher BC ZSM HYM 01713 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838914.1 Colletes nasutus voucher BC ZSM HYM 02013 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838921.1 Andrena clarkella voucher BC ZSM HYM 00208 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\n&gt;KJ838946.1 Nomada fabriciana voucher BC ZSM HYM 06144 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondrial\nThese are too long and contain white spaces, which will lead to problems downstream. Let’s fix the names! Before actually changing the file, make sure the script works as expected. Can you try to figure out what the three sed substitution patterns do? If not, run them one by one and look at the result.\nsed -e 's/ voucher.*//' -e 's/&gt;.*1 /&gt;/' -e 's/ /_/' data/coi.fas | head -n 20\n\n&gt;Andrena_chrysosceles\nATTGGGGCTTCACTAAGATTCATTATTCGCATAGAACTAAGAAACCCAGGAAACTGAATCAATAATGACC\nAAATCTATAACTCAATTGTAACTTCTCACGCCTTTATTATAATTTTCTTCATAGTTATGCCATTCATAAT\nCGGAGGTTTCGGAAACTGACTCACACCGTTAATATTAGGAGCGCCCGACATGGCCTTCCCACGAATAAAC\nAATATAAGATTCTGACTTCTACCACCTTCAATTCTTATTATTTTAATAAGAATAGTTATAAATTCAGGAT\nCCGGTACAGGATGAACAGTGTACCCCCCCCTATCTTCCTACGCATTTCACCCATCATCATCCGTAGACCT\nGACAATTTTTTCACTACACATCGCAGGAGTATCATCAATCATAGGAGCAATTAATTTTATTGTCACAATC\nTTAAATATAAAAAATATTTCAATAAATTATGATCAACTACCATTATTCCCATGATCAGTATTCATTACAA\nCAATTCTACTACTAATTTCACTGCCAGTACTAGCTGGGGCCATTACAATATTATTATCAGACCGAAACTT\nANATTCATCATTTTTTGACCCCATGGGGGGCGGAGATCC\n\n&gt;Andrena_proxima\nAATATTATACTTCATCTTCGCTATATGATCCGGAATAATTGGCGCCTCACTAAGATTTATCATCCGTATA\nGAATTAAGAAATCCAGGAAATTGAATCAACAATGATCAAATTTATAATTCTATCGTAACCTCACACGCTT\nTCATTATAATTTTTTTCATAGTAATACCATTTATAATCGGAGGATTCGGAAACTGACTTACACCACTAAT\nATTAGGAGCACCTGATATAGCCTTCCCACGAATAAATAATATAAGATTTTGATTACTCCCTCCATCCATT\nACAATACTTTTAATAAGAACAATCTTAAATTCAGGATCTGGAACAGGATGAACTGTTTATCCTCCTTTAT\nCCTCCTACTCATATCATCCATCATCATCTGTAGATTTAACAATTTTTTCACTTCACATTGCAGGTATCTC\nATCAATTATAGGAGCTATTAACTTCATTGTAACCATCTTAAATATAAAAAATATCTCAATAAATTATGAT\nCAAATACCCCTATTCCCATGATCTGTCTTTATTACAACAATCCTATTATTAATTTCATTACCAGTCCTCG\nLooking good? Let’s make the change!\nsed -i -e 's/ voucher.*//' -e 's/&gt;.*1 /&gt;/' -e 's/ /_/' data/coi.fas",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#compiling-data",
    "href": "phylo1.html#compiling-data",
    "title": "13  Retrieving and aligning sequences",
    "section": "",
    "text": "At this point you will need to decide in which format to store your sequence data in. Frustratingly, there is a plethora of file formats, and some pieces of software only support a particular file format. My recommendation is to use the fasta file format for AA and nucleotide sequences and alignments, and to convert this if needed (e.g., using the excellent online tool ALTER). The fasta file format has the big advantage of being very easily readable by human and machines and follows the following format:\n&gt;Sequence_A\nTAGTAGCGATCGACTAAGCTAGCT\n&gt;Sequence_B\nCGACTAAGCTAGCTTAGTAGCGAT\nDescriptions (usually your sequence names) always start with a &gt; and should provide a unique identifier for your sequences. The sequences may or may not contain line breaks:\n&gt;Sequence_A\nTAGTAGCGATCG\nACTAAGCTAGCT\n&gt;Sequence_B\nCGACTAAGCTAG\nCTTAGTAGCGAT\nIMPORTANT: Never use white spaces in fasta decription lines. This will inevitably lead to problems in downstream applications. You can use any text editor to replace white spaces e.g., with underscores by using the search & replace function.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#aligning-sequences",
    "href": "phylo1.html#aligning-sequences",
    "title": "13  Retrieving and aligning sequences",
    "section": "13.2 Aligning sequences",
    "text": "13.2 Aligning sequences\nWe will use the Mafft online server to align our sequences. Upload your fasta file, and let Mafft decide on the optimal alignment strategy (this is the default behaviour; Fig. 3). For more specific applications you may want to check out the other alignment options.\n\n\n\n\n\nFig. 3 | Using the Mafft web server 1. Select your fasta file ; 2. provide email address to retrieve your results later; 3. Click Submit to start alignment\n\n\n\n\nWhen the alignment is done, download the fasta file (Fig. 4) and give it a meaningful name.\n\n\n\n\n\nFig. 4 | Download the aligned sequences from Mafft\n\n\n\n\nAlternatively, you can use the command-line version of Mafft which should already be in your conda environment.\n\nmafft --auto data/coi.fas &gt; data/coi_ali.fas",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#using-an-alignment-editor",
    "href": "phylo1.html#using-an-alignment-editor",
    "title": "13  Retrieving and aligning sequences",
    "section": "13.3 Using an alignment editor",
    "text": "13.3 Using an alignment editor\nYou should always visually check your alignments to make sure that everything looks as expected. Remember, your hypothesis for any aligned position is that all of the characters are homologous. Alignment viewers make it easy to view and manipulate your alignment files. We will use Aliview, a fast and versatile alignment viewer (Fig. 5). It can be started by double-clicking the corresponding file.\n\n\n\n\n\nFig. 5 | Some functions available in aliview illustrated (opening a file, scrolling through the sequences, zooming in and out, aligning, removing alignment positions, renaming a sequence, saving an alignment file). Make sure to check out the entire range of operations of this versatile tool.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#automated-alignment-trimming",
    "href": "phylo1.html#automated-alignment-trimming",
    "title": "13  Retrieving and aligning sequences",
    "section": "13.4 Automated alignment trimming",
    "text": "13.4 Automated alignment trimming\nIn many cases, you will want to trim your alignment to exclude positions that you consider unreliably aligned or positions that contain mostly gaps. This can be done manually in Aliview as we have seen above. However, for a more reproducible workflow it is recommendable to automate this task using clearly defined criteria. We will use trimAl for automated alignment trimming. This programme (as most phylogenetics software) does not have a graphical user interface (GUI), so most be used via the command line. No worries, this is not as difficult as it may seem.\nInstall trimal via conda if you have not done so already, and run\n\ntrimal -in data/coi_ali.fas -out data/coi_trimmed.fas -htmlout data/coi_ali.html -gt .5\n\nLet’s go through this command step by step: trimal is our executable. -in specifies the input file. Note that we have to specify the entire path of the file unless the file is in the same directory as the executable. -out is the name of the output file. Again, we are using the entire path here. -htmlout specifies the path to a useful html file that displays the trimmed characters. -gt .5 specifies how trimming should be performed: we are asking to remove any position in the alignment that has a gap in more than 50% of all sequences. This is only one of multiple trimming criteria trimAl can employ, make sure to have a look at the other ones as well.\nLet’s have a look at the html output file trimAl has generated for us (Fig. 6). What can you observe? Check the alignment again in Aliview before moving on.\n\n\n\n\n\nFig. 6 | Html output of trimAl illustrating the results of the trimming. Positions that were retained are highlighted in grey and positions trimmed are shown with white background",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo1.html#exercise",
    "href": "phylo1.html#exercise",
    "title": "13  Retrieving and aligning sequences",
    "section": "Exercise",
    "text": "Exercise\nRepeat all of the steps for the second locus of our data set! Use the accession numbers of accessions_lwr.txt to start.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Retrieving and aligning sequences</span>"
    ]
  },
  {
    "objectID": "phylo2.html",
    "href": "phylo2.html",
    "title": "14  Phylogenetic reconstruction using distance methods.",
    "section": "",
    "text": "14.1 Reading in and checking the alignment using ape\nFor many reasons distance based methods in phylogenetics are very prone to systematic biases and generally regarded inferior to character based methods. Distance based methods are still used widely, e.g., as a quick & dirty explorative technique or to generate starting trees for other methods. We will use the the R package ape to create a neighbor joining tree.\nYou know your way around R, so most of this will be familiar.\nLet’s open R and load the ape package.\nlibrary(ape)\nWe can now load the alignment file and make some checks\n# read the fasta alignment\ncoi &lt;- read.dna(\"data/coi_ali.fas\", format = \"fasta\") \n\n# check if successful\ncheckAlignment(coi)\n\n\nNumber of sequences: 63 \nNumber of sites: 658 \n\nSome gap lengths are not multiple of 3: 1 22 25 28 34 37 40 44 46 47 52 59 232 233 251 343 350\n\nFrequencies of gap lengths:\n  1  22  25  28  34  36  37  39  40  44  46  47  52  59  69 117 232 233 234 251 \n  1   1   1   1   3   2   2   1   1   3   1   1   1   1   1   1   5   1   1   9 \n339 343 350 \n  1   1   2 \n   =&gt; length of gaps on the left border of the alignment: 37 0 59 52 232 233 251 36 47 28 44 34 25 234 46 1 \n   =&gt; length of gaps on the right border of the alignment: 350 350 343 339 117 69 44 40 39 37 34 22 \n\nNumber of unique contiguous base segments defined by gaps: 22 \nNumber of segment lengths not multiple of 3: 11 \n    =&gt; on the left border of the alignement: 3 \n    =&gt; on the right border                 : 4 \n    =&gt; positions of these segments inside the alignment: 38..636 60..621 252..624 37..308 \n\nNumber of segregating sites (including gaps): 392\nNumber of sites with at least one substitution: 392\nNumber of sites with 1, 2, 3 or 4 observed bases:\n  1   2   3   4 \n266 166 151  75\nThis looks all good and as expected.\nPlease note that the alignment is stored as a matrix. This means we can use the same tools we have learned about earlier in the course to access and modify the alignment.\nFor example, to only look at the first 100bp of the first 10 taxa, simply run\ncoi[1:10, 1:100]\n\n10 DNA sequences in binary format stored in a matrix.\n\nAll sequences of same length: 100 \n\nLabels:\nAndrena_chrysosceles\nAndrena_proxima\nEpeolus_schummeli\nNomada_sexfasciata\nColletes_succinctus\nEpeoloides_coecutiens\n...\n\nBase composition:\n    a     c     g     t \n0.359 0.120 0.147 0.375 \n(Total: 1 kb)\nWe can also determine the diversity of the alignment at the different codon positions. Consider this example\n# extract each codon position into a new alignment\nfirstpos &lt;- coi[, seq(from = 1, by = 3, to = ncol(coi))]\nsecondpos &lt;- coi[, seq(from = 2, by = 3, to = ncol(coi))]\nthirdpos &lt;- coi[, seq(from = 3, by = 3, to = ncol(coi))]\n\n# determine how many sites each are not conserved across all positions\ndiv1 &lt;- length(seg.sites(firstpos))\ndiv2 &lt;- length(seg.sites(secondpos))\ndiv3 &lt;- length(seg.sites(thirdpos))\n\n# generate named vector from results\nposwisedivs &lt;- c(div1, div2, div3)\nnames(poswisedivs) &lt;- c(\"1st\", \"2nd\", \"3rd\")\n\n# plot\nbarplot(poswisedivs)\nWhat does this tell you about the reading frame in your alignment? Check if your assumption is correct in Aliview!",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Phylogenetic reconstruction using distance methods.</span>"
    ]
  },
  {
    "objectID": "phylo2.html#calculating-the-neighbor-joining-tree",
    "href": "phylo2.html#calculating-the-neighbor-joining-tree",
    "title": "14  Phylogenetic reconstruction using distance methods.",
    "section": "14.2 Calculating the neighbor joining tree",
    "text": "14.2 Calculating the neighbor joining tree\nThe first step in any distance based method is the creation of a distance matrix. To this end, we need to calculate phylogenetic distances between all pairs of sequences in our alignment.\n\n# calculate distance matrix – we could specify a model of sequence evolution here, but will use the raw distances for now\ncoi.mat &lt;- dist.dna(coi, model = \"raw\", pairwise.deletion = TRUE)\n\n# have a look at the distance matrix (only display first 10 rows & 5 columns)\nas.matrix(coi.mat)[1:10, 1:5]\n\n                      Andrena_chrysosceles Andrena_proxima Epeolus_schummeli\nAndrena_chrysosceles             0.0000000       0.1739130         0.2793103\nAndrena_proxima                  0.1739130       0.0000000         0.2637540\nEpeolus_schummeli                0.2793103       0.2637540         0.0000000\nNomada_sexfasciata               0.2591973       0.2401216         0.1634304\nColletes_succinctus              0.2535971       0.2104317         0.1844485\nEpeoloides_coecutiens            0.2675815       0.2458746         0.1501767\nNomada_leucophthalma             0.2803970       0.2488263         0.1113990\nColletes_nasutus                 0.2493766       0.2334906         0.1922078\nAndrena_clarkella                0.1720430       0.1662198         0.2806540\nNomada_fabriciana                0.2625418       0.2325228         0.1504854\n                      Nomada_sexfasciata Colletes_succinctus\nAndrena_chrysosceles          0.25919732          0.25359712\nAndrena_proxima               0.24012158          0.21043165\nEpeolus_schummeli             0.16343042          0.18444846\nNomada_sexfasciata            0.00000000          0.18165468\nColletes_succinctus           0.18165468          0.00000000\nEpeoloides_coecutiens         0.12046205          0.17625899\nNomada_leucophthalma          0.06807512          0.19843342\nColletes_nasutus              0.17924528          0.08638743\nAndrena_clarkella             0.26809651          0.23351648\nNomada_fabriciana             0.08814590          0.18525180\n\n\nObserve that the size of distance matrix will always be determined by the number of taxa and is not influenced by the number of positions in your alignment!\nFrom this we can now reconstruct a phylogenetic tree. There are various options available, and you may already be familiar with hierarchical clustering algorithms. Neigbor Joining is the most commonly used method for tree reconstruction from DNA sequences based on clustering.\n\n# The output of the nj function is a phylogenetic tree\ncoi.nj.tree &lt;- nj(coi.mat)\n\ncoi.nj.tree\n\n\nPhylogenetic tree with 63 tips and 61 internal nodes.\n\nTip labels:\n  Andrena_chrysosceles, Andrena_proxima, Epeolus_schummeli, Nomada_sexfasciata, Colletes_succinctus, Epeoloides_coecutiens, ...\n\nUnrooted; includes branch length(s).\n\n# this tree can be plotted\nplot(coi.nj.tree, no.margin = TRUE)\n\n\n\n\n\n\n\n# but it is better to export it and take a look later\nwrite.tree(coi.nj.tree, \"data/COI_NJ.tre\")\n\n\nAs you may have guessed at this point, there are multiple different tree file formats. We will be using the newick file format, mainly because it is simple and can be read by humans and machines alike. The downside is that not much metadata can be stored using this format. Example of a simple tree (please note the semicolon at the end!):\n(((a,b),(c,d)),e); \nTo read the tree, follow the parentheses from inside out: a & b are closest relatives, as are c & d. a+b & c+d are united by the next set of parentheses, and e is the furthest outside. The resulting tree therefore looks like this:\n        /-- a\n    /---+\n    |   \\-- b\n/---+\n|   |   /-- c\n|   \\---+\n|       \\-- d\n|\n\\---------- e\nBranch lengths can also be stored using the newick format. The tree we have just saved to disk looks like this:\n\n\n\ncat data/COI_NJ.tre\n\n((((((((Andrena_clarkella:0.03793042308,Andrena_fulva:0.04002656617):0.006208554979,Andrena_praecox:0.03722621523):0.03291462202,Andrena_bicolor:0.04954187293):0.007225657745,(((((Isodontia_mexicana:0.1093010072,Podalonia_hirsuta:0.09648130568):0.02548389406,Philanthus_triangulum:0.1155825578):0.006695663236,(((((Hylaeus_signatus:0.04330756335,Hylaeus_variegatus:0.04098362438):0.01416345624,(((Colletes_nasutus:0.03494954881,Colletes_daviesanus:0.01929573421):0.01055673006,(Colletes_succinctus:0.003754770216,Colletes_hederae:0.003439474389):0.03336123395):0.002796295485,Colletes_cunicularius:0.03696443129):0.02985969807):0.0101427056,((((((Nomada_fabriciana:0.03002916815,Nomada_rufipes:0.02772159173):0.001368761399,(((((((Nomada_leucophthalma:0.006942628419,Nomada_panzeri:0.0118367143):0.003169805492,Nomada_flava:-0.0008223876521):0.002330886604,Nomada_ferruginata:0.001122457575):0.00381701872,Nomada_signata:0.02321217773):0.009814662959,Nomada_conjungens:0.04436483664):0.004295624941,(Nomada_succincta:-0.007614381512,Nomada_goodeniana:0.01007138397):0.03622586243):0.003452537977,((Nomada_alboguttata:0.01556458855,Nomada_obscura:0.0219940969):0.007506503678,Nomada_fulvicornis:0.02965262128):0.00423029554):0.008100450394):0.007212177037,Nomada_sexfasciata:0.03678430511):0.00911774246,Melecta_luctuosa:0.08976173859):0.008273234502,Epeoloides_coecutiens:0.05027692376):0.005602424146,((Anthophora_plumipes:0.07822533014,Ceratina_cyanea:0.07165181973):0.009971414203,Epeolus_schummeli:0.07111294455):0.01049365998):0.03769645362):0.005469236369,(Macropis_fulvipes:0.03583557437,Macropis_europaea:0.02712738859):0.06123857798):0.004000420205,(Melitta_haemorrhoidalis:0.05206633079,Melitta_leporina:0.05735616161):0.05882258335):0.008031882647):0.00585270542,((((Sphecodes_pellucidus:0.07099590708,Sphecodes_albilabris:0.09024187793):0.003993983912,((Sphecodes_gibbus:0.03477631432,Sphecodes_rufiventris:0.04761463883):0.01818620717,Sphecodes_monilicornis:0.06443370358):0.01263176106):0.005503990579,Halictus_quadricinctus:0.08194419011):0.004349517682,(((Lasioglossum_albipes:0.02397005069,Lasioglossum_calceatum:0.01674656168):0.03654527748,Lasioglossum_malachurum:0.06593930218):0.008299880436,Lasioglossum_pauxillum:0.06163616107):0.01815370924):0.01994318425):0.008363308522,Hylaeus_nigritus:0.09032225511):0.03675728699):0.004694871292,((Andrena_barbilabris:0.0584486813,Andrena_ventralis:0.06440144155):0.01245351575,Andrena_ruficrus:0.06674642282):0.007775616838):0.00054511563,Andrena_proxima:0.07491713868):0.01146065374,Andrena_fuscipes:0.0622328689):0.005542362885,((Andrena_nigroaenea:0.05334280321,Andrena_nitida:0.05000066183):0.006493494734,Andrena_gravida:0.0611356846):0.003032856579,((((Andrena_minutula:0.07182900939,Andrena_nasuta:0.05317099061):0.009780251298,Andrena_chrysosceles:0.05984836932):0.003518142377,Andrena_dorsata:0.07479486746):0.002907434525,((Andrena_vaga:0.03063052226,Andrena_cineraria:0.04629255467):0.01078510112,Andrena_tibialis:0.06346247033):0.00900205477):0.002678477105);",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Phylogenetic reconstruction using distance methods.</span>"
    ]
  },
  {
    "objectID": "phylo2.html#exercises",
    "href": "phylo2.html#exercises",
    "title": "14  Phylogenetic reconstruction using distance methods.",
    "section": "Exercises",
    "text": "Exercises\n\nChange the model in the dist.dna command. Determine if specifying a model increases or decreases the average nucleotide distances.\nRepeat the above steps for the LW Rhodopsin alignment. What does the analysis of the segragating sites tell you about this locus?",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Phylogenetic reconstruction using distance methods.</span>"
    ]
  },
  {
    "objectID": "phylo3.html",
    "href": "phylo3.html",
    "title": "15  Models and Maximum Likelihood",
    "section": "",
    "text": "15.1 Model selection\nAny phylogenetic reconstruction approach implicitly or explicitly uses a model of sequence evolution. You should employ a subjective measure for selecting the appropriate model. We will use ModelFinder, which is implemented in IQ-TREE. A model selection process consists of calculating maximum likelihood phylogenies under a number of considered models of sequence evolution and subsequently ranking the models via their log likelihoods. Because parameter rich models are usually expected to fit the data better, but overparameterisation can be problematic, we select the best model via an approach that penalises additional parameters: the BIC.\nTo perform model selection using IQ-TREE, run the following command:\niqtree -s COI_ali.fas -pre COI_MF -m TESTONLY -mset mrbayes -mtree\nLet’s dissect this command:\nIQ-TREE has created a number of output files, but we are really only interested in the file ending with .iqtree (Fig. 7).\nFig. 7 | Result of a ModelFinder analysis. GTR+F+I+G4 is the best model according to AIC and BIC",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models and Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "phylo3.html#model-selection",
    "href": "phylo3.html#model-selection",
    "title": "15  Models and Maximum Likelihood",
    "section": "",
    "text": "iqtree\nName of the executable\n\n\n-s\nPath to alignment file\n\n\n-pre\nPrefix for output files (important to choose a meaningful name here)\n\n\n-m TESTONLY\nspecifies that we want to do a model test only\n\n\n-mset mrbayes\nThis tells iqtree to only consider models that are also supported by MrBayes (we will be using this software later)\n\n\n-mtree\nDo a complete ML search for all models (this is the most accurate model search)",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models and Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "phylo3.html#maximum-likelihood-analysis",
    "href": "phylo3.html#maximum-likelihood-analysis",
    "title": "15  Models and Maximum Likelihood",
    "section": "15.2 Maximum Likelihood analysis",
    "text": "15.2 Maximum Likelihood analysis\nNow that we have determined the best model, we can run IQ-TREE to reconstruct a phylogeny:\n\niqtree -s COI_ali.fas -pre COI_ML -m GTR+F+I+G4 -bb 1000\n\nWe have used the following options:\n\n\n\n\n\n\n\n-s\nPath to alignment file\n\n\n-pre\nPrefix for output files\n\n\n-m GTR+F+I+G4\nThe ‘best’ model as determined by us in the previous step\n\n\n-bb 1000\nPerforms 1000 pseudoreplicates of IQ-TREE’s ultrafast bootstraps\n\n\n\nFurther options that may be important for your analyses:\n\n\n\n\n\n\n\n-nt\nHow many threads should be used for the analysis? This can considerably speed up you analysis\n\n\n-alrt 1000\nPerforms a likelihood ratio test for each branch with 1000 replicates. This is a good complement to the bootstrap!\n\n\n-b 100\nThis is the standard, non-parametric bootstrap – long computation times, but useful in some cases.\n\n\n-fast\nIf you are in a hurry, this is the option for you. Produces much faster results at the cost of accuracy. Good for quick first exploration and still more accurate than equally fast MP or NJ methods.\n\n\n--runs 10\nIQ-TREE uses heuristics to determine the best tree, and these may differ between runs. It is therefore important to have several independent runs of you ML analysis. IQ-TREE will automatically select the best of the runs.\n\n\n\nBtw, you can combine model selection and ML search in a single IQ-TREE run. IQ-TREE would then determine the best model, and use it to subsequently perform a ML search. The command would look like this:\n\niqtree -s COI_aligned.fas -pre COI_ML_combined -bb 1000\n\nNote how we don’t need to specify a model here - the default behavior is to determine it automatically. Very convenient indeed.\nIQ-TREE will have generated a .treefile, which can now be opened in FigTree or a different tree viewer.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models and Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "phylo3.html#exercises",
    "href": "phylo3.html#exercises",
    "title": "15  Models and Maximum Likelihood",
    "section": "Exercises",
    "text": "Exercises\n\nRepeat the model selection and ML analysis for the lwr alignment.\nRepeat the ML tree search for one alignment and model 10 times using the option -n 10 in IQ-TREE. Compare the log-likelihoods. Are they identical across the runs? Why/why not?\nRun an alignment under the worst possible model. Save the tree file for later!",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Models and Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "phylo4.html",
    "href": "phylo4.html",
    "title": "16  Displaying phylogenetic trees",
    "section": "",
    "text": "16.1 Figtree\nThe most convenient way of displaying phylogenetic trees is by using a software with a graphical user interface. However, R also has some convenient tools for displaying and altering trees.\nOpen FigTree by double clicking on the corresponding file in your file manager. Load your tree file by clicking File -&gt; Open and navigating to your tree file. If the file contains support values (e.g., from Bootstrap analysis), FigTree will ask you to provide a name for these. There are a couple of things you should do when you first view your tree (Fig. 8).\nFig. 8 | A simple workflow to display a phylogenetic tree using FigTree. The steps are: opening a tree file; rooting with an outgroup; ladderizing the tree; displaying bootstrap values; adjusting label size, aligning tip labels; exporting a pdf of the tree file.\nFigtree offers many more functions that you should explore in detail. Publication-ready figures can be generated, or further modified in e.g., a vector graphics programme such as Inkscape. A good alternative to Figtree is iTol (Interactive Tree of Life), an online platform that offers a similar set of tools as Figtree: https://itol.embl.de/.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Displaying phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "phylo4.html#plotting-trees-in-r",
    "href": "phylo4.html#plotting-trees-in-r",
    "title": "16  Displaying phylogenetic trees",
    "section": "16.2 Plotting trees in R",
    "text": "16.2 Plotting trees in R\nPlotting trees in R is straightforward but we need to understand the structure of the tree format first.\nRead in a tree and plot using ape\n\nlibrary(ape)\ncoi.tre &lt;- read.tree(\"data/COI_NJ.tre\")\nplot(coi.tre, no.margin = T)\n\n\n\n\n\n\n\n\nLet’s look at how tree data is stored in ape.\n\nstr(coi.tre)\n\nList of 4\n $ edge       : int [1:123, 1:2] 64 65 66 67 68 69 70 71 71 70 ...\n $ edge.length: num [1:123] 0.005542 0.011461 0.000545 0.004695 0.007226 ...\n $ Nnode      : int 61\n $ tip.label  : chr [1:63] \"Andrena_clarkella\" \"Andrena_fulva\" \"Andrena_praecox\" \"Andrena_bicolor\" ...\n - attr(*, \"class\")= chr \"phylo\"\n - attr(*, \"order\")= chr \"cladewise\"\n\nplot(coi.tre, no.margin = T)\nnodelabels()\n\n\n\n\n\n\n\n\nKnowing the nodelabels helps us to re-root the tree. Some more options to alter the display of the tree are shown\n\ncoi.rooted &lt;- root(coi.tre, node = 75, resolve.root = TRUE)\n\nplot(ladderize(coi.rooted, right = FALSE), \n     root.edge = TRUE, \n     no.margin = TRUE, \n     cex = 0.8,\n     align.tip.label = TRUE)\n\n\n\n\n\n\n\n# Let's add some colours!\n# First we need to create color vectors for the elements we may want to change, e.g., node labels or edges of the tree\ntipcolours &lt;- rep(\"black\", length(coi.rooted$tip.label))\n\n# Now let's alter the tip colours for each group we want to highlight\ntipcolours[grep(\"Andrena\", coi.rooted$tip.label)] &lt;- \"purple\"\ntipcolours[grep(\"Nomada\", coi.rooted$tip.label)] &lt;- \"turquoise\"\n\nplot(ladderize(coi.rooted, right = FALSE), \n     root.edge = TRUE, \n     no.margin = TRUE, \n     align.tip.label = TRUE,\n     tip.color = tipcolours)",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Displaying phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "phylo4.html#exercises",
    "href": "phylo4.html#exercises",
    "title": "16  Displaying phylogenetic trees",
    "section": "Exercises",
    "text": "Exercises\n\nUsing tree displaying software such as Figtree and iTol, compare our trees resulting from NJ and ML analysis. Also compare the trees resulting from analysing the different loci COI and lwr. What could explain differences in the topologies?\nRemember the tree created under the worst model in our previous analysis? How different is it from the tree reconstructed under the best model?\n\n\n\n\n\n\n\nTip\n\n\n\nTry out the function cophyloplot to compare different phylogenies with each other.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Displaying phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "phylo5.html",
    "href": "phylo5.html",
    "title": "17  Bayesian phylogenetic reconstruction",
    "section": "",
    "text": "Exercise\nWe will use MrBayes to reconstruct a phylogeny from our alignments. This requires a few more steps than our previous Maximum Likelihood analysis. First of all, we need to convert our alignment into the nexus format, as MrBayes won’t accept any other format. We can do this by using the online tool ALTER (Fig. 9).\nA nexus file always begins with #NEXUS followed by different blocks. Each block begins with begin &lt;BLOCK&gt; and ends with end;. The only block present in our file is the data block, but the nexus format allows many additional data to be stored here. Open the nexus file in a text editor and scroll to the very bottom. We will add the options and instructions for MrBayes directly into this file – this is different to e.g., IQ-TREE where we provide all options on the command line. It is a bit more tedious, but comes at the advantage that the settings for our run will be stored together with the data in the same nexus file – this is good in terms of reproducibility.\nAt the very bottom of the nexus file add\nThis starts the MrBayes block – Reading the file, MrBayes will now know that this is where its instructions are at. First of all, we need to specify the substitution model to be used. Using Modelfinder, we have determined GTR+F+G+I to be the best fitting. On a new line, enter:\nnst stands for number of substitution rates. GTR has 6 in total (1 for each exchange possibility between the 4 bases), so we need to specify 6 here. To include gamma distribution of rate heterogeneity across sites plus invariable sites, you need to specify invgamma. Other options are gamma (no invariable sites) or equal (for homogeneous rates across sites). For an overview of how to specify the most commonly used substitution models in MrBayes, please see below.\nWe now need to specify the priors for our analyses, i.e., our assumptions on what the data will look like. We want these to be uninformative, so will only use flat prior here. We can specify priors for all parameters (only 3 are considered here):\nNote that revmatprior and Shapeprior are the default values, so we wouldn’t need to specify these explicitly. We are using fixed state frequencies (the +F in our model), because ModelTest told us this would fit the data best.\nNow we need to tell MrBayes how to perform the analyses:\nWe are asking for 2 runs in parallel with 4 chains each. 100,000 generations will be performed, with samples taken and printed to screen avery 500 generations. This concludes our MrBayes block, so we add a\nAs the final line.\nThe complete MrBayes block now should look like this:\nMake sure to save your changes into the nexus file!\nWe can run MrBayes by executing the app and typing in its terminal:\nWhile MrBayes is running, it will print out information on the current status of the analysis (Fig 10.)\nIn regular intervals MrBayes tells us about the standard deviation of split frequencies – this is a measure for how different the topologies of the two different runs are. We want them to be very similar of course, and a low value here is indicative of that.\nOnce the initial 100,000 generations are done, MrBayes asks us if we want to continue. Type no and press &lt;ENTER&gt;. MrBayes will produce some run statistics. Type sump and press &lt;ENTER&gt;. Take some time to explore the run statistics here: did the runs converge? Should we let the program run a little longer?\nIf you are happy and want to produce a consensus tree from the posterior samples, type sumt and press &lt;ENTER&gt;. You can quit the programme now by typing quit and pressing &lt;ENTER&gt;. MrBayes has produced a consensus tree file (ending with .con.tre), which can now be looked at using a tree viewer such as FigTree. MrBayes has further sampled all trees and model parameters over the generations (stored in the .t and .p files, respectively). THese files can be further explored to determine how well the runs have converged, e.g., by using this online tool.\nDo a Bayesian phylogeny for the lwr alignment!",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian phylogenetic reconstruction</span>"
    ]
  },
  {
    "objectID": "phylo5.html#exercise",
    "href": "phylo5.html#exercise",
    "title": "17  Bayesian phylogenetic reconstruction",
    "section": "",
    "text": "Appendix: MrBayes Models\nA list of models supported by MrBayes and how to specify them\n\nRate heterogeneity\n\n\n\n\n\n\n\nI\nlset rates=propinv;\n\n\nG\nlset rates=gamma;\n\n\nF\nprset statefreqpr=fixed(empirical);\n\n\nI + G\nlset rates=invgamma;\n\n\nG + F\nprset statefreqpr=fixed(empirical); lset rates=gamma;\n\n\nI + F\nprset statefreqpr=fixed(empirical); lset rates=propinv;\n\n\nI + G + F\nprset statefreqpr=fixed(empirical); lset rates=invgamma;\n\n\n\n\n\nNucleotide substitution models\n\n\n\n\n\n\n\nGTR\nlset nst=6;\n\n\nSYM\nlset nst=6 prset statefreqpr=fixed(equal);\n\n\nHKY\nlset nst=2;\n\n\nK2P\nlset nst=2 prset statefreqpr=fixed(equal);\n\n\nF81\nlset nst=1;\n\n\nJC\nlset nst=1 prset statefreqpr=fixed(equal);\n\n\n\n\n\nFixed rate protein substitution models\n\n\n\nPoisson\nprset aamodelpr=fixed(poisson);\n\n\nDayhoff\nprset aamodelpr=fixed(dayhoff);\n\n\nmtREV\nprset aamodelpr=fixed(mtrev);\n\n\nmtMAM\nprset aamodelpr=fixed(mtmam);\n\n\nWAG\nprset aamodelpr=fixed(wag);\n\n\nrtREV\nprset aamodelpr=fixed(rtrev);\n\n\ncpREV\nprset aamodelpr=fixed(cprev);\n\n\nVT\nprset aamodelpr=fixed(vt);\n\n\nBlosum62\nprset aamodelpr=fixed(blosum);\n\n\nJTT\nprset aamodelpr=fixed(jtt);",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bayesian phylogenetic reconstruction</span>"
    ]
  },
  {
    "objectID": "phylo6.html",
    "href": "phylo6.html",
    "title": "18  Comparing trees",
    "section": "",
    "text": "18.1 Measuring differences\nYou will have noticed that different approaches may lead to different tree topologies. You can use a tree viewer and describe the differences, but for larger trees this may be cumbersome. Robinson-Foulds distances are an objective measurement of how much two trees differ. For two trees, the Robinson-Foulds distance is the number of splits in tree 1 that are missing in tree 2, PLUS the the number of splits in tree 2 that are missing in tree 1.\nYou will have noticed that different approaches may lead to different tree topologies. You can use a tree viewer and describe the differences, but for larger trees this may be cumbersome. Robinson-Foulds distances are an objective measurement of how much two trees differ. For two trees, the Robinson-Foulds distance is the number of splits in tree 1 that are missing in tree 2, PLUS the the number of splits in tree 2 that are missing in tree 1.\nLet’s compare the differences for our COI trees obtained by different methodologies: Neighbor Joining (NJ), Maximum Likelihood (ML), Bayesian Inference (BI).\nWe require all tree files in Newick format, however MrBayes provided a tree in Nexus format. Open the tree in FigTree, and save it as a Newick tree.\nNext, merge the 3 tree files together in a single file (but do keep track of the order you do this in!)\nUsing the terminal in MacOS, you can combine the trees like this:\ncat COI_NJ.tre COI_ML.treefile COI_BI.tre &gt; COI_combined.tre\nTo calculate Robinson-Foulds (RF) distances between these trees, run IQ-TREE using the -rf_all option:\niqtree -rf_all COI_combined.tre\nHave a look at the resulting file:\ndata/COI_combined.tre.rfdist: No such file or directory\nThe distances are displayed in a distance matrix which makes comparison straightforward.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Comparing trees</span>"
    ]
  },
  {
    "objectID": "phylo6.html#hypothesis-testing",
    "href": "phylo6.html#hypothesis-testing",
    "title": "18  Comparing trees",
    "section": "18.2 Hypothesis testing",
    "text": "18.2 Hypothesis testing\nIn some cases it may be sufficient to obtain a single reasonable phylogeny, but for many applications you may want to know how well your topology is supported compared with other topologies – for example, if you have 2 competing hypotheses on the evolutionary history of a protein, you not only want to obtain the most likely phylogeny, but also want to know if the alternative topology can be rejected with confidence. This can be done by hypothesis testing, and the most common tests used are the Shimodaira-Hasegawa (SH) test and the approximately unbiased (AU) test. IQ-TREE supports these tests.\nAs a very simple example, let us check if the COI trees tree recovered from our NJ, ML, and BI analyses differ significantly.\n\niqtree -z COI_combined.tre -m GTR+F+I+G4 -s COI_aligned.fas -au -zb 10000 -n 0 -pre COI.hypo.test\n\nWe are providing our file containing the 3 trees via -z and provide the best model determined earlier. We also need to supply a path to the alignment file via -s. -au specifies that we want an AU test, and -zb specifies the number of replicates for significance testing. -n 0 simply means that we do not want IQ-TREE to determine a maximum likelihood tree.\nThe output of the tests can be found in the file ending with .iqtree (Fig. 11)\n\n\n\n\n\nFig. 11 | Results of SH and AU tests (highlighted). IQ-TREE also does other topology tests, which we will ignore for now. As can be seen from the p-values, the SH test does not reject any of the three topologies, whereas the AU test rejects the NJ topology.\n\n\n\n\nThe little + and - symbols in the table help in the interpretation. In the example provided here, the AU test rejects the NJ topology with a barely significant p-value whereas the SH test does not reject any of the topologies. So, although our trees are quite different, we can not confidently reject any of the alternative topologies.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Comparing trees</span>"
    ]
  },
  {
    "objectID": "phylo6.html#exercise",
    "href": "phylo6.html#exercise",
    "title": "18  Comparing trees",
    "section": "Exercise",
    "text": "Exercise\nUsing the trees created in the earlier steps, try to compare the topologies between different loci, models, and phylogenetic approaches.",
    "crumbs": [
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Comparing trees</span>"
    ]
  }
]